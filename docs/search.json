[{"path":"https://business-science.github.io/modeltime/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2020 Business Science Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Extending Modeltime (Developer Tools)","text":"modeltime package extensibility mind. ’s impossible incorporate entire R time series ecosystem single package. likely going need something hasn’t included yet. , ’re developer, challenge extend modeltime suit needs. ’ve spent lot time thinking : systematic workflow can used time series analysis way integrate time series best--class modeling framework (.e. tidymodels) , want leverage built don’t recreate wheel.","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"before-we-get-started-where-are-we-coming-from","dir":"Articles","previous_headings":"Introduction","what":"Before we get started, where are we coming from?","title":"Extending Modeltime (Developer Tools)","text":"Beyond amazing ecosystem Machine Learning analysis R, tidymodels developer-friendly infrastructure enables parsnip-adjacent packages like modeltime. ecosystem provides basic building blocks extending machine learning models, tuning parameters, performance metrics, preprocessing (feature engineering) tools. start building, highly recommend reviewing resources: Parsnip Models (Algorithms): build parsnip model Dials Parameters (Tuning): create tuning parameter function Yardstick Metrics (Performance): Custom performance metrics Recipes Step Functions (preprocessing): Create recipe step function important first resource - “build parsnip model”.","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"where-we-are-going","dir":"Articles","previous_headings":"Introduction","what":"Where we are going!","title":"Extending Modeltime (Developer Tools)","text":"’re going momentarily modify “build parsnip model” article. learning “build modeltime model”. key nuances: Managing Date Date-time Features Processing Regressors Dates separately","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"example-integrating-multiple-seasonal-decomposition","dir":"Articles","previous_headings":"","what":"Example Integrating Multiple Seasonal Decomposition","title":"Extending Modeltime (Developer Tools)","text":"forecast package includes stlm() function creating models using time series decomposition. ’s great approach based concept can decompose complex seasonality multiple components: Trend One Multiple Seasonalities Remainder can forecast components using models like: auto.arima() ets(). Note - ’ve added seasonal_reg() function includes functionality create. Check function see seasonal decomposition methods.","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"libraries","dir":"Articles","previous_headings":"Example Integrating Multiple Seasonal Decomposition","what":"Libraries","title":"Extending Modeltime (Developer Tools)","text":"use code tutorial, ’ll need following packages installed.","code":"library(parsnip) library(forecast) library(rsample) library(modeltime) library(tidyverse) library(timetk) library(rlang)"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"data","dir":"Articles","previous_headings":"Example Integrating Multiple Seasonal Decomposition","what":"Data","title":"Extending Modeltime (Developer Tools)","text":"’ll use taylor_30_min data timetk package, electricity demand data 30-minute interval.","code":"taylor_30_min %>%   plot_time_series(date, value, .interactive = FALSE)"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"stlm-model---a-quick-overview","dir":"Articles","previous_headings":"Example Integrating Multiple Seasonal Decomposition","what":"STLM Model - A quick overview","title":"Extending Modeltime (Developer Tools)","text":"need use 2 functions create model: msts() - Tracks multiple-seasonality object stlm() - Creates model msts object. Note use seasonal periods (24*2 = 48) daily seasonality series aggregated 30-minute interval. second seasonality 7-days, just use 24*2*7.  forecast() function used generate predictions, much like predict() function.  Looks like ’s pretty decent job forecasting Taylor 30-Min Data.","code":"stlm_model <- taylor_30_min %>%   pull(value) %>%   msts(seasonal.periods = c(24*2, 24*2*7)) %>%   stlm()  stlm_model$stl %>% autoplot() stlm_model %>%   forecast(h = 24*2*7) %>%   autoplot()"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"how-to-build-a-modeltime-model","dir":"Articles","previous_headings":"","what":"How to build a modeltime model","title":"Extending Modeltime (Developer Tools)","text":"first step need think function. can create: msts(): Creates multiple time series object. Key arguments: seasonal.periods. method: Either ets arima (actually uses auto.arima) ’ll handle creating specific engine called “stlm_ets” ets version. add others, let’s keep simple now.","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"the-general-process","dir":"Articles","previous_headings":"","what":"The General Process","title":"Extending Modeltime (Developer Tools)","text":"follow similar process “build parsnip model” additional aspects: create “Modeltime Bridge” function bridge parsnip api functions forecast time series. handle feature preprocessing internally model bridge avoid issue losing dates date-time information.","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"step-1-register-the-model-modes-and-arguments","dir":"Articles","previous_headings":"The General Process","what":"Step 1: Register the Model, Modes, and Arguments","title":"Extending Modeltime (Developer Tools)","text":"Let’s add new model called decomposition_reg() Regression Decomposition. Time Series analysis generally regression, ’ll just register regression mode. can use show_model_info() see ’s added registered model. need add model arguments well. ’ll use set_model_args() add appropriate model arguments. Important note: going map “modeltime” arguments lower-level bridge function, don’t need original = seasonal.periods. fact, can’t need one--one relationship parsnip/modeltime model arguments lower-level modeling function.","code":"set_new_model(\"decomposition_reg\") set_model_mode(model = \"decomposition_reg\", mode = \"regression\") set_model_engine(model = \"decomposition_reg\", mode = \"regression\", eng = \"stlm_ets\")  # Here we set the dependency to forecast, though we can also use # your package if you import the lower level package set_dependency(model = \"decomposition_reg\", eng = \"stlm_ets\", pkg = \"forecast\") show_model_info(\"decomposition_reg\") #> Information for `decomposition_reg` #>  modes: unknown, regression  #>  #>  engines:  #>    regression: stlm_ets #>  #>  no registered arguments. #>  #>  no registered fit modules. #>  #>  no registered prediction modules. # 1st Frequency (period_seasonal_1 is a non-jargony term) set_model_arg(   model        = \"decomposition_reg\",    eng          = \"stlm_ets\",    parsnip      = \"period_seasonal_1\",    original     = \"period_seasonal_1\",    func         = list(pkg = \"foo\", fun = \"bar\"),   has_submodel = FALSE )  # 2nd Frequency (period_seasonal_2 is a non-jargony term) set_model_arg(   model        = \"decomposition_reg\",    eng          = \"stlm_ets\",    parsnip      = \"period_seasonal_2\",    original     = \"period_seasonal_2\",    func         = list(pkg = \"foo\", fun = \"bar\"),   has_submodel = FALSE )"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"step-2-create-the-model-function","dir":"Articles","previous_headings":"The General Process","what":"Step 2: Create the model function","title":"Extending Modeltime (Developer Tools)","text":"’ll create decomposition_reg() function core parsnip function.","code":"decomposition_reg <- function(mode = \"regression\",                                period_seasonal_1 = NULL,                                period_seasonal_2 = NULL) {      args <- list(         period_seasonal_1 = rlang::enquo(period_seasonal_1),         period_seasonal_2 = rlang::enquo(period_seasonal_2)     )      parsnip::new_model_spec(         \"decomposition_reg\",         args     = args,         eng_args = NULL,         mode     = mode,         method   = NULL,         engine   = NULL     )  }"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"step-3-add-a-fit-bridge-module","dir":"Articles","previous_headings":"The General Process","what":"Step 3: Add a Fit Bridge & Module","title":"Extending Modeltime (Developer Tools)","text":"Time Series models, need extra work get fit parsnip mold. easiest way create “Bridge” function helps process data connect arguments underlying time series package.","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"step-3a-create-the-fit-bridge","dir":"Articles","previous_headings":"The General Process > Step 3: Add a Fit Bridge & Module","what":"Step 3A: Create the Fit Bridge","title":"Extending Modeltime (Developer Tools)","text":"Time Series Developer Tools come included modeltime: Bridge Constructor: new_modeltime_bridge(), constructor creating modeltime bridges provide connection time series models parsnip infrastructure. Xreg Preprocessing Tools: create_xreg_recipe(), juice_xreg_recipe(), bake_xreg_recipe() Date Date-Time Parsing Tools: parse_index_from_data(), parse_period_from_index() ’s sample bridge function. : Handles data inputs, x (data.frame predictors) y (vector, target) Handles timestamp data (index) - column parsed x Handles Predictors - formatting necessary. ets() model univariate, predictors necessary transformed. Handles Fitting model Creates new model bridge using constructor, new_modeltime_bridge() ’ll make quick print method don’t get crazy output. Let’s make sure bridge works. bridge looks good.","code":"bridge_stlm_ets_fit_impl <- function(x, y, period_seasonal_1 = NULL, period_seasonal_2 = NULL, ...) {      outcome    <- y # Comes in as a vector   predictors <- x # Comes in as a data.frame (dates and possible xregs)      # 1. Create outcome msts object by mapping `period_seasonal` args to msts()   if (is.null(period_seasonal_1) || period_seasonal_1 <= 1) {     stop(\"'period_seasonal_1' must be greater than 1 to assess seasonality\")   } else if (is.null(period_seasonal_2) || period_seasonal_2 <= 1) {     seasonal.periods <- period_seasonal_1   } else {     seasonal.periods <- c(period_seasonal_1, period_seasonal_2)   }   outcome_msts <- forecast::msts(outcome, seasonal.periods = seasonal.periods)      # 2. Predictors - Handle Dates    index_tbl <- modeltime::parse_index_from_data(predictors)   idx_col   <- names(index_tbl)   idx       <- timetk::tk_index(index_tbl)      # 3. Predictors - Handle Xregs   # NOT REQUIRED - ETS is univariate   # REQUIRED FOR ARIMA - ARIMA can accept XRegs   # xreg_recipe <- create_xreg_recipe(predictor, prepare = TRUE)   # xreg_matrix <- juice_xreg_recipe(xreg_recipe, format = \"matrix\")      # 4. Fitting   model_1 <- forecast::stlm(y = outcome_msts, method = \"ets\", ...)      # 5. New Modeltime Bridge   new_modeltime_bridge(     class  = \"bridge_stlm_ets_fit_impl\",     models = list(model_1 = model_1),     data   = tibble::tibble(       idx_col   := idx,       .actual    = y,       .fitted    = model_1$fitted,       .residuals = model_1$residuals     ),     extras = list(NULL), # Can add xreg preprocessors here     desc   = stringr::str_c(\"STLM Model: \", model_1$model$method)   )    } print.bridge_stlm_ets_fit_impl <- function(x, ...) {      model <- x$models$model_1$model      cat(x$desc)   cat(\"\\n\")   print(model$call)   cat(\"\\n\")   print(     tibble(       aic    = model$aic,       bic    = model$bic,       aicc   = model$aicc,       loglik = model$loglik,       mse    = model$mse       )   )   invisible(x) } stlm_test <- bridge_stlm_ets_fit_impl(   x = taylor_30_min[,\"date\"],   y = taylor_30_min %>% pull(value),   period_seasonal_1 = 24*2,   period_seasonal_2 = 24*2*7 )  stlm_test #> STLM Model: ETS(M,N,N) #> ets(y = x, model = etsmodel, allow.multiplicative.trend = allow.multiplicative.trend) #>  #> # A tibble: 1 × 5 #>      aic    bic   aicc  loglik   mse #>    <dbl>  <dbl>  <dbl>   <dbl> <dbl> #> 1 70395. 70414. 70395. -35194. 9501."},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"step-3b-create-a-fit-module","dir":"Articles","previous_headings":"The General Process > Step 3: Add a Fit Bridge & Module","what":"Step 3B: Create a Fit Module","title":"Extending Modeltime (Developer Tools)","text":"Next, let’s hook parsnip fit module newly created bridge fit function.","code":"set_fit(   model  = \"decomposition_reg\",   eng    = \"stlm_ets\",   mode   = \"regression\",   value  = list(     interface = \"data.frame\",     protect   = c(\"x\", \"y\"),     func      = c(fun = \"bridge_stlm_ets_fit_impl\"),     defaults  = list()   ) )  show_model_info(\"decomposition_reg\") #> Information for `decomposition_reg` #>  modes: unknown, regression  #>  #>  engines:  #>    regression: stlm_ets #>  #>  arguments:  #>    stlm_ets:  #>       period_seasonal_1 --> period_seasonal_1 #>       period_seasonal_2 --> period_seasonal_2 #>  #>  fit modules: #>      engine       mode #>    stlm_ets regression #>  #>  no registered prediction modules."},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"step-4-add-a-prediction-bridge-module","dir":"Articles","previous_headings":"The General Process","what":"Step 4: Add a Prediction Bridge & Module","title":"Extending Modeltime (Developer Tools)","text":"Next, additional step bridge prediction. create predict method handles new_data, treating number rows forecasting horizon.","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"step-4a-create-a-prediction-bridge","dir":"Articles","previous_headings":"The General Process > Step 4: Add a Prediction Bridge & Module","what":"Step 4A: Create a Prediction Bridge","title":"Extending Modeltime (Developer Tools)","text":"Let’s test predict() method . help, ’ll leverage future_frame() predict next 3-hours worth forecast points.  Excellent. predictions!","code":"predict.bridge_stlm_ets_fit_impl <- function(object, new_data, ...) {      # PREPARE INPUTS     model       <- object$models$model_1     h_horizon   <- nrow(new_data)      # XREG     # NOT REQUIRED FOR ETS.      # xreg_recipe <- object$extras$xreg_recipe      # xreg_matrix <- bake_xreg_recipe(xreg_recipe, new_data, format = \"matrix\")      # PREDICTIONS     preds_forecast <- forecast::forecast(model, h = h_horizon)          # Return predictions as numeric vector     preds <- as.numeric(preds_forecast$mean)      return(preds)  } stlm_test %>%   predict(new_data = taylor_30_min %>% future_frame(.length_out = \"1 week\")) %>%    plot()"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"step-4b-add-modules-for-prediction","dir":"Articles","previous_headings":"The General Process > Step 4: Add a Prediction Bridge & Module","what":"Step 4B: Add Modules for Prediction","title":"Extending Modeltime (Developer Tools)","text":"’re now ready register prediction function ’ve created.","code":"set_pred(     model         = \"decomposition_reg\",     eng           = \"stlm_ets\",     mode          = \"regression\",     type          = \"numeric\",     value         = list(         pre       = NULL,         post      = NULL,         func      = c(fun = \"predict\"),         args      =             list(                 object   = rlang::expr(object$fit),                 new_data = rlang::expr(new_data)             )     ) )  show_model_info(\"decomposition_reg\") #> Information for `decomposition_reg` #>  modes: unknown, regression  #>  #>  engines:  #>    regression: stlm_ets #>  #>  arguments:  #>    stlm_ets:  #>       period_seasonal_1 --> period_seasonal_1 #>       period_seasonal_2 --> period_seasonal_2 #>  #>  fit modules: #>      engine       mode #>    stlm_ets regression #>  #>  prediction modules: #>          mode   engine methods #>    regression stlm_ets numeric"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"step-5-add-encoding","dir":"Articles","previous_headings":"The General Process","what":"Step 5: Add Encoding","title":"Extending Modeltime (Developer Tools)","text":"need modify parnsip handles features date date-time information processed model.frame() (default behavior). use set_encoding() ensure predictors processed without preprocessing. ’ll handle preprocessing fit model bridge, bridge_stlm_ets_fit_impl() (shown next).","code":"parsnip::set_encoding(   model   = \"decomposition_reg\",   eng     = \"stlm_ets\",   mode    = \"regression\",   options = list(     predictor_indicators = \"none\",     compute_intercept = FALSE,     remove_intercept = FALSE,     allow_sparse_x = FALSE   ) )"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"does-it-work","dir":"Articles","previous_headings":"","what":"Does it work?","title":"Extending Modeltime (Developer Tools)","text":"moment truth upon us! Let’s test new modeltime function .","code":""},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"make-the-fitted-stl-decomposition-model","dir":"Articles","previous_headings":"Does it work?","what":"Make the Fitted STL Decomposition Model","title":"Extending Modeltime (Developer Tools)","text":"’ll split data training test sets. create model training set.","code":"splits <- initial_time_split(taylor_30_min, prop = 0.9) model_fit <- decomposition_reg(     period_seasonal_1 = 24*2,      period_seasonal_2 = 24*2*7   ) %>%   set_engine(\"stlm_ets\") %>%   fit(value ~ date, data = training(splits))  model_fit #> parsnip model object #>  #> Fit time:  194ms  #> STLM Model: ETS(M,N,N) #> ets(y = x, model = etsmodel, allow.multiplicative.trend = allow.multiplicative.trend) #>  #> # A tibble: 1 × 5 #>      aic    bic   aicc  loglik   mse #>    <dbl>  <dbl>  <dbl>   <dbl> <dbl> #> 1 63052. 63071. 63052. -31523. 9747."},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"make-a-calibrated-modeltime-table","dir":"Articles","previous_headings":"Does it work?","what":"Make a Calibrated Modeltime Table","title":"Extending Modeltime (Developer Tools)","text":"Next, let’s calibrate fitted model.","code":"calibration_tbl <- model_fit %>%   modeltime_table() %>%   modeltime_calibrate(new_data = testing(splits))  calibration_tbl #> # Modeltime Table #> # A tibble: 1 × 5 #>   .model_id .model   .model_desc            .type .calibration_data  #>       <int> <list>   <chr>                  <chr> <list>             #> 1         1 <fit[+]> STLM MODEL: ETS(M,N,N) Test  <tibble [404 × 4]>"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"refit-on-the-full-dataset","dir":"Articles","previous_headings":"Does it work?","what":"Refit on the Full Dataset","title":"Extending Modeltime (Developer Tools)","text":"’ll refit full dataset. forecast forward 1 week.  now ’ve just created new model. Just wrap package ’ve extended parsnip modeltime. Congrats!","code":"refit_tbl <- calibration_tbl %>%   modeltime_refit(data = taylor_30_min) refit_tbl %>%   modeltime_forecast(h = \"1 week\", actual_data = taylor_30_min) %>%   plot_modeltime_forecast(.interactive = FALSE)"},{"path":"https://business-science.github.io/modeltime/articles/extending-modeltime.html","id":"further-resources","dir":"Articles","previous_headings":"","what":"Further Resources","title":"Extending Modeltime (Developer Tools)","text":"’d recommend looking code parsnip-arima_reg.R parsnip-arima_reg_data.R see process implemented inside package.","code":""},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"quickstart-video","dir":"Articles","previous_headings":"","what":"Quickstart Video","title":"Getting Started with Modeltime","text":"prefer video tutorials, 11-minute YouTube Video walks Modeltime Workflow.  (Click Watch YouTube)","code":""},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"the-modeltime-workflow","dir":"Articles","previous_headings":"","what":"The Modeltime Workflow","title":"Getting Started with Modeltime","text":"’s general process functions fit. Modeltime Workflow Just follow modeltime workflow, detailed 6 convenient steps: Collect data split training test sets Create & Fit Multiple Models Add fitted models Model Table Calibrate models testing set. Perform Testing Set Forecast & Accuracy Evaluation Refit models Full Dataset & Forecast Forward Let’s go guided tour kick tires modeltime.","code":""},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"time-series-forecasting-example","dir":"Articles","previous_headings":"","what":"Time Series Forecasting Example","title":"Getting Started with Modeltime","text":"Load libraries complete short tutorial.","code":"library(xgboost) library(tidymodels) library(modeltime) library(tidyverse) library(lubridate) library(timetk)  # This toggles plots from plotly (interactive) to ggplot (static) interactive <- FALSE"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"step-1---collect-data-and-split-into-training-and-test-sets-","dir":"Articles","previous_headings":"Time Series Forecasting Example","what":"Step 1 - Collect data and split into training and test sets.","title":"Getting Started with Modeltime","text":"can visualize dataset.  Let’s split data training test sets using initial_time_split()","code":"# Data m750 <- m4_monthly %>% filter(id == \"M750\") m750 %>%   plot_time_series(date, value, .interactive = interactive) # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"step-2---create-fit-multiple-models","dir":"Articles","previous_headings":"Time Series Forecasting Example","what":"Step 2 - Create & Fit Multiple Models","title":"Getting Started with Modeltime","text":"can easily create dozens forecasting models combining modeltime parsnip. can also use workflows interface adding preprocessing! forecasting possibilities endless. Let’s get basic models developed: ARIMA Exponential Smoothing Linear Regression MARS (Multivariate Adaptive Regression Splines) Important note: Handling Date Features Modeltime models (e.g. arima_reg()) created date date time feature model. see models include formula like fit(value ~ date, data). Parsnip models (e.g. linear_reg()) typically date features, may contain derivatives dates (e.g. month, year, etc). often see formulas like fit(value ~ .numeric(date) + month(date), data).","code":""},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"model-1-auto-arima-modeltime","dir":"Articles","previous_headings":"Time Series Forecasting Example > Step 2 - Create & Fit Multiple Models","what":"Model 1: Auto ARIMA (Modeltime)","title":"Getting Started with Modeltime","text":"First, create basic univariate ARIMA model using “Auto Arima” using arima_reg()","code":"# Model 1: auto_arima ---- model_fit_arima_no_boost <- arima_reg() %>%     set_engine(engine = \"auto_arima\") %>%     fit(value ~ date, data = training(splits)) #> frequency = 12 observations per 1 year"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"model-2-boosted-auto-arima-modeltime","dir":"Articles","previous_headings":"Time Series Forecasting Example > Step 2 - Create & Fit Multiple Models","what":"Model 2: Boosted Auto ARIMA (Modeltime)","title":"Getting Started with Modeltime","text":"Next, create boosted ARIMA using arima_boost(). Boosting uses XGBoost model ARIMA errors. Note model formula contains date feature derivatives date - ARIMA uses date - XGBoost uses derivatives date regressors Normally ’d use preprocessing workflow month features using function like step_timeseries_signature() timetk help reduce complexity parsnip formula interface.","code":"# Model 2: arima_boost ---- model_fit_arima_boosted <- arima_boost(     min_n = 2,     learn_rate = 0.015 ) %>%     set_engine(engine = \"auto_arima_xgboost\") %>%     fit(value ~ date + as.numeric(date) + factor(month(date, label = TRUE), ordered = F),         data = training(splits)) #> frequency = 12 observations per 1 year"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"model-3-exponential-smoothing-modeltime","dir":"Articles","previous_headings":"Time Series Forecasting Example > Step 2 - Create & Fit Multiple Models","what":"Model 3: Exponential Smoothing (Modeltime)","title":"Getting Started with Modeltime","text":"Next, create Error-Trend-Season (ETS) model using Exponential Smoothing State Space model. accomplished exp_smoothing().","code":"# Model 3: ets ---- model_fit_ets <- exp_smoothing() %>%     set_engine(engine = \"ets\") %>%     fit(value ~ date, data = training(splits)) #> frequency = 12 observations per 1 year"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"model-4-prophet-modeltime","dir":"Articles","previous_headings":"Time Series Forecasting Example > Step 2 - Create & Fit Multiple Models","what":"Model 4: Prophet (Modeltime)","title":"Getting Started with Modeltime","text":"’ll create prophet model using prophet_reg().","code":"# Model 4: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this."},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"model-5-linear-regression-parsnip","dir":"Articles","previous_headings":"Time Series Forecasting Example > Step 2 - Create & Fit Multiple Models","what":"Model 5: Linear Regression (Parsnip)","title":"Getting Started with Modeltime","text":"can model time series linear regression (TSLM) using linear_reg() algorithm parsnip. following derivatives date used: Trend: Modeled using .numeric(date) Seasonal: Modeled using month(date)","code":"# Model 5: lm ---- model_fit_lm <- linear_reg() %>%     set_engine(\"lm\") %>%     fit(value ~ as.numeric(date) + factor(month(date, label = TRUE), ordered = FALSE),         data = training(splits))"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"model-6-mars-workflow","dir":"Articles","previous_headings":"Time Series Forecasting Example > Step 2 - Create & Fit Multiple Models","what":"Model 6: MARS (Workflow)","title":"Getting Started with Modeltime","text":"can model Multivariate Adaptive Regression Spline model using mars(). ’ve modified process use workflow standardize preprocessing features provided machine learning model (mars). OK, 6 models, ’ll show easy forecast.","code":"# Model 6: earth ---- model_spec_mars <- mars(mode = \"regression\") %>%     set_engine(\"earth\")   recipe_spec <- recipe(value ~ date, data = training(splits)) %>%     step_date(date, features = \"month\", ordinal = FALSE) %>%     step_mutate(date_num = as.numeric(date)) %>%     step_normalize(date_num) %>%     step_rm(date)    wflw_fit_mars <- workflow() %>%     add_recipe(recipe_spec) %>%     add_model(model_spec_mars) %>%     fit(training(splits))"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"step-3---add-fitted-models-to-a-model-table-","dir":"Articles","previous_headings":"Time Series Forecasting Example","what":"Step 3 - Add fitted models to a Model Table.","title":"Getting Started with Modeltime","text":"next step add models Modeltime Table using modeltime_table(). step basic checking make sure models fitted organizes scalable structure called “Modeltime Table” used part forecasting workflow. 6 models add. couple notes moving : Note models tunable parameters. ’s expected tuning parameter selection performed prior incorporating Modeltime Table. try add unfitted model, modeltime_table() complain (throw informative error) saying need fit() model.","code":"models_tbl <- modeltime_table(     model_fit_arima_no_boost,     model_fit_arima_boosted,     model_fit_ets,     model_fit_prophet,     model_fit_lm,     wflw_fit_mars )  models_tbl #> # Modeltime Table #> # A tibble: 6 × 3 #>   .model_id .model     .model_desc                               #>       <int> <list>     <chr>                                     #> 1         1 <fit[+]>   ARIMA(0,1,1)(0,1,1)[12]                   #> 2         2 <fit[+]>   ARIMA(0,1,1)(0,1,1)[12] W/ XGBOOST ERRORS #> 3         3 <fit[+]>   ETS(M,A,A)                                #> 4         4 <fit[+]>   PROPHET                                   #> 5         5 <fit[+]>   LM                                        #> 6         6 <workflow> EARTH"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"step-4---calibrate-the-model-to-a-testing-set-","dir":"Articles","previous_headings":"Time Series Forecasting Example","what":"Step 4 - Calibrate the model to a testing set.","title":"Getting Started with Modeltime","text":"Calibrating adds new column, .calibration_data, test predictions residuals inside. notes Calibration: Calibration confidence intervals accuracy metrics determined Calibration Data simply forecasting predictions residuals calculated --sample data. calibrating, calibration data follows data forecasting workflow.","code":"calibration_tbl <- models_tbl %>%     modeltime_calibrate(new_data = testing(splits))  calibration_tbl #> # Modeltime Table #> # A tibble: 6 × 5 #>   .model_id .model     .model_desc                        .type .calibration_da… #>       <int> <list>     <chr>                              <chr> <list>           #> 1         1 <fit[+]>   ARIMA(0,1,1)(0,1,1)[12]            Test  <tibble [31 × 4… #> 2         2 <fit[+]>   ARIMA(0,1,1)(0,1,1)[12] W/ XGBOOS… Test  <tibble [31 × 4… #> 3         3 <fit[+]>   ETS(M,A,A)                         Test  <tibble [31 × 4… #> 4         4 <fit[+]>   PROPHET                            Test  <tibble [31 × 4… #> 5         5 <fit[+]>   LM                                 Test  <tibble [31 × 4… #> 6         6 <workflow> EARTH                              Test  <tibble [31 × 4…"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"step-5---testing-set-forecast-accuracy-evaluation","dir":"Articles","previous_headings":"Time Series Forecasting Example","what":"Step 5 - Testing Set Forecast & Accuracy Evaluation","title":"Getting Started with Modeltime","text":"2 critical parts evaluation. Visualizing Forecast vs Test Data Set Evaluating Test (Sample) Accuracy","code":""},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"a---visualizing-the-forecast-test","dir":"Articles","previous_headings":"Time Series Forecasting Example > Step 5 - Testing Set Forecast & Accuracy Evaluation","what":"5A - Visualizing the Forecast Test","title":"Getting Started with Modeltime","text":"Visualizing Test Error easy using interactive plotly visualization (just toggle visibility models using Legend).  visualizing test set forecast: Models 1&2: ARIMA & ARIMA Boost performing well. models “auto” components used Auto ARIMA. XGBoost component parameters specified. can possibly get better accuracy tuning, ARIMA component working well data, additional improvement may low. Model 3: ETS(M,,) performing best. 80% confidence interval narrow bunch, indicating hold set modeled well. Model 4: PROPHET comparable ARIMA models, slightly wider test error confidence interval. Model 5: LM -shooting local trend. trend component simple linear line, doesn’t account change points. Model 6: EARTH overfitting local trend. tune number change points, algorithm auto-calculating change points.","code":"calibration_tbl %>%     modeltime_forecast(         new_data    = testing(splits),         actual_data = m750     ) %>%     plot_modeltime_forecast(       .legend_max_width = 25, # For mobile screens       .interactive      = interactive     )"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"b---accuracy-metrics","dir":"Articles","previous_headings":"Time Series Forecasting Example > Step 5 - Testing Set Forecast & Accuracy Evaluation","what":"5B - Accuracy Metrics","title":"Getting Started with Modeltime","text":"can use modeltime_accuracy() collect common accuracy metrics. default reports following metrics using yardstick functions: MAE - Mean absolute error, mae() MAPE - Mean absolute percentage error, mape() MASE - Mean absolute scaled error, mase() SMAPE - Symmetric mean absolute percentage error, smape() RMSE - Root mean squared error, rmse() RSQ - R-squared, rsq() course can customized following rules creating new yardstick metrics, defaults useful. Refer default_forecast_accuracy_metrics() learn . make table-creation bit easier, ’ve included table_modeltime_accuracy() outputing results either interactive (reactable) static (gt) tables. accuracy metrics: Model 3: ETS clearly winner MAE 77 Model 6: MARS -fitting local trend. comes R-Squared 0.55.","code":"calibration_tbl %>%     modeltime_accuracy() %>%     table_modeltime_accuracy(         .interactive = interactive     )"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"step-6---refit-to-full-dataset-forecast-forward","dir":"Articles","previous_headings":"Time Series Forecasting Example","what":"Step 6 - Refit to Full Dataset & Forecast Forward","title":"Getting Started with Modeltime","text":"final step refit models full dataset using modeltime_refit() forecast forward.","code":"refit_tbl <- calibration_tbl %>%     modeltime_refit(data = m750)  refit_tbl %>%     modeltime_forecast(h = \"3 years\", actual_data = m750) %>%     plot_modeltime_forecast(       .legend_max_width = 25, # For mobile screens       .interactive      = interactive     )"},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"refitting---what-happened","dir":"Articles","previous_headings":"","what":"Refitting - What happened?","title":"Getting Started with Modeltime","text":"models changed! (Yes - point refitting) LM model looks much better now linear trend line now fit new data follows longer term trend. EARTH model trend representative near-term trend. PROPHET model trend similar EARTH model (modeling algorithms use changepoints model trend, prophet’s auto algorithm seems better job adapting). ETS model changed (M,,) (,,). ARIMA model updated better capture upswing. (potential) benefit refitting. often refitting good idea. Refitting: Retrieves model preprocessing steps Refits model new data Recalculating long-term trend Linear Model Recalculating changepoints Earth Model Recalculating ARIMA ETS parameters XGBoost Parameters Boosted ARIMA min_n = 2, learn_rate = 0.015. defaults automatic calculations used.","code":""},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Getting Started with Modeltime","text":"just showcased Modeltime Workflow. simple problem. , ’s lot learning time series. Many algorithms Ensembling Machine Learning Deep Learning Scalable Modeling: 10,000+ time series probably thinking ever going learn time series forecasting. ’s solution save years struggling.","code":""},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"take-the-high-performance-forecasting-course","dir":"Articles","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"Getting Started with Modeltime","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"time-series-is-changing","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"Getting Started with Modeltime","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"Getting Started with Modeltime","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"global-forecasting","dir":"Articles","previous_headings":"","what":"Global Forecasting","title":"Forecasting with Global Models","text":"general, two methods forecasting scale (slow accurate method fast good method): Iterative Forecasting (Slow Accurate): Best accuracy takes much longer global model (due -loop iteration), results many models (memory required), often needed run parallel scale thousands time series (otherwise can take days complete). , iterative forecasting can yield great results. See Nested Forecasting Iterative Forecasting implementation. Global Modeling (Fast Good): Best scalability thousands time series. Uses Panel Data structure. Can scale 10,000+ time series easily single model made generates predictions (forecasts) time series. Requires feature engineering parameter tuning get best performance, can challenging beginners.","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"important-concepts","dir":"Articles","previous_headings":"","what":"Important Concepts","title":"Forecasting with Global Models","text":"Students can struggle global modeling. help, terms cover help understand global modeling works (differs iterative modeling).","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"panel-data","dir":"Articles","previous_headings":"","what":"Panel Data","title":"Forecasting with Global Models","text":"panel data structure format used global modeling. ’s simplest form, Panel Data time series dataset one series. time series stacked row-wise (-top) . Panel Data Structure","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"traditional-modeling-iteration","dir":"Articles","previous_headings":"","what":"Traditional Modeling (Iteration)","title":"Forecasting with Global Models","text":"Traditional modeling techniques like ARIMA can used one time series time. widely accepted forecasting approach iterate time series producing unique model forecast time series identifier. downside approach ’s expensive many time series. Think number products database. number time series approaches range 1000-10,000, iterative approach becomes unscalable. Problem: 1000 ARIMA Models Needed 1000 Time Series","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"global-modeling","dir":"Articles","previous_headings":"","what":"Global Modeling","title":"Forecasting with Global Models","text":"Global Models alternatives iterative approach. Global Model single model forecasts time series . Global Models highly scalable, solves problem 1-10,000 time series. example XGBoost Model, can determine relationships 1000 time series panels single model. Solution: Single XGBOOST Model can Model 1000 Time Series","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"importance-of-feature-engineering","dir":"Articles","previous_headings":"","what":"Importance of Feature Engineering","title":"Forecasting with Global Models","text":"downside global approach can less accurate. improve accuracy, feature engineering localized model selection time series identifier become critical large-scale forecasting success. recommend learning feature engineering techniques proven get results Time Series Forecasting Course. Modeling Panel Data","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting Started","title":"Forecasting with Global Models","text":"’ll cover short tutorial global forecasting. first thing load following libraries:","code":"library(tidymodels) library(modeltime) library(tidyverse) library(timetk)"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"dataset","dir":"Articles","previous_headings":"","what":"Dataset","title":"Forecasting with Global Models","text":"Next, let’s use walmart_sales_weekly dataset comes timetk. dataset already set panel data modeling. important columns : “id”: separates time series groups (case represent sales departments walmart store) “date”: weekly sales period “value”: value sales week store/department can visualize time series group expose differences sales department.  can clearly see 7 time series groups different weekly sales patterns.","code":"data_tbl <- walmart_sales_weekly %>%     select(id, Date, Weekly_Sales) %>%     set_names(c(\"id\", \"date\", \"value\"))  data_tbl #> # A tibble: 1,001 × 3 #>    id    date        value #>    <fct> <date>      <dbl> #>  1 1_1   2010-02-05 24924. #>  2 1_1   2010-02-12 46039. #>  3 1_1   2010-02-19 41596. #>  4 1_1   2010-02-26 19404. #>  5 1_1   2010-03-05 21828. #>  6 1_1   2010-03-12 21043. #>  7 1_1   2010-03-19 22137. #>  8 1_1   2010-03-26 26229. #>  9 1_1   2010-04-02 57258. #> 10 1_1   2010-04-09 42961. #> # … with 991 more rows data_tbl %>%   group_by(id) %>%   plot_time_series(     date, value, .interactive = F, .facet_ncol = 2   )"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"traintest-splitting","dir":"Articles","previous_headings":"","what":"Train/Test Splitting","title":"Forecasting with Global Models","text":"can split data training testing sets using time_series_split(). ’ll investigate last 3-months year test global model 3-month forecast. message overlapping dates let us know multiple time series processed using last 3-month window testing.","code":"splits <- data_tbl %>%    time_series_split(     assess     = \"3 months\",      cumulative = TRUE   ) #> Using date_var: date #> Data is not ordered by the 'date_var'. Resamples will be arranged by `date`. #> Overlapping Timestamps Detected. Processing overlapping time series together using sliding windows.  splits #> <Analysis/Assess/Total> #> <917/84/1001>"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"recipe","dir":"Articles","previous_headings":"","what":"Recipe","title":"Forecasting with Global Models","text":"Next, need preprocess data. add good features imporve global model performance. use recipes workflow generating time series features. step_mutate_at(): Applies function one columns. Used remove extra unused levels categorical ID column. step_timeseries_signature(): Used add calendar features based date column step_rm(): Used remove date column. Machine Learning algorithms like XGBoost don’t handle date columns well. step_zv(): Used remove zero variance features. Features variance don’t add useful information model. step_dummy(): Used convert categorical features binary one-hot encoded features algorithms like XGBoost can model effectively. results 37 derived features modeling. can certainly include features lags rolling features known improve results. concepts covered High-Performance Time Series Course.","code":"rec_obj <- recipe(value ~ ., training(splits)) %>%     step_mutate_at(id, fn = droplevels) %>%     step_timeseries_signature(date) %>%     step_rm(date) %>%     step_zv(all_predictors()) %>%     step_dummy(all_nominal_predictors(), one_hot = TRUE)  summary(prep(rec_obj)) #> # A tibble: 38 × 4 #>    variable       type    role      source   #>    <chr>          <chr>   <chr>     <chr>    #>  1 value          numeric outcome   original #>  2 date_index.num numeric predictor derived  #>  3 date_year      numeric predictor derived  #>  4 date_year.iso  numeric predictor derived  #>  5 date_half      numeric predictor derived  #>  6 date_quarter   numeric predictor derived  #>  7 date_month     numeric predictor derived  #>  8 date_month.xts numeric predictor derived  #>  9 date_day       numeric predictor derived  #> 10 date_mday      numeric predictor derived  #> # … with 28 more rows"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"machine-learning","dir":"Articles","previous_headings":"","what":"Machine Learning","title":"Forecasting with Global Models","text":"’ll create xgboost workflow fitting default xgboost model derived features -sample training data set. Training global xgboost model takes approximately 50 milliseconds. Conversely, ARIMA model might take several minutes iterate possible parameter combinations 7 time series.","code":"# Workflow wflw_xgb <- workflow() %>%     add_model(         boost_tree() %>% set_engine(\"xgboost\")     ) %>%     add_recipe(rec_obj) %>%     fit(training(splits))  wflw_xgb #> ══ Workflow [trained] ══════════════════════════════════════════════════════════ #> Preprocessor: Recipe #> Model: boost_tree() #>  #> ── Preprocessor ──────────────────────────────────────────────────────────────── #> 5 Recipe Steps #>  #> • step_mutate_at() #> • step_timeseries_signature() #> • step_rm() #> • step_zv() #> • step_dummy() #>  #> ── Model ─────────────────────────────────────────────────────────────────────── #> ##### xgb.Booster #> raw: 58.2 Kb  #> call: #>   xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0,  #>     colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1,  #>     subsample = 1, objective = \"reg:squarederror\"), data = x$data,  #>     nrounds = 15, watchlist = x$watchlist, verbose = 0, nthread = 1) #> params (as set within xgb.train): #>   eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", objective = \"reg:squarederror\", nthread = \"1\", validate_parameters = \"TRUE\" #> xgb.attributes: #>   niter #> callbacks: #>   cb.evaluation.log() #> # of features: 37  #> niter: 15 #> nfeatures : 37  #> evaluation_log: #>     iter training_rmse #>        1     46315.141 #>        2     33001.734 #> ---                    #>       14      3676.542 #>       15      3373.945"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"modeltime-workflow","dir":"Articles","previous_headings":"","what":"Modeltime Workflow","title":"Forecasting with Global Models","text":"’ll step modeltime workflow, used test many different models time series organize entire process. Modeltime Workflow","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"step-1-create-a-modeltime-table","dir":"Articles","previous_headings":"Modeltime Workflow","what":"Step 1: Create a Modeltime Table","title":"Forecasting with Global Models","text":"first step create Modeltime Table modeltime_table().","code":"model_tbl <- modeltime_table(     wflw_xgb )  model_tbl #> # Modeltime Table #> # A tibble: 1 × 3 #>   .model_id .model     .model_desc #>       <int> <list>     <chr>       #> 1         1 <workflow> XGBOOST"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"step-2-calibrate-by-id","dir":"Articles","previous_headings":"Modeltime Workflow","what":"Step 2: Calibrate by ID","title":"Forecasting with Global Models","text":"Next, need calibrate, calculates forecast error test set. Use modeltime_calibrate() perform calibration. Note add id, able track --sample residual error “id” column.","code":"calib_tbl <- model_tbl %>%     modeltime_calibrate(       new_data = testing(splits),        id       = \"id\"     )  calib_tbl #> # Modeltime Table #> # A tibble: 1 × 5 #>   .model_id .model     .model_desc .type .calibration_data #>       <int> <list>     <chr>       <chr> <list>            #> 1         1 <workflow> XGBOOST     Test  <tibble [84 × 5]>"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"step-3-measure-test-accuracy","dir":"Articles","previous_headings":"Modeltime Workflow","what":"Step 3: Measure Test Accuracy","title":"Forecasting with Global Models","text":"Next, measure global local accuracy global model.","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"global-accuracy","dir":"Articles","previous_headings":"Modeltime Workflow > Step 3: Measure Test Accuracy","what":"Global Accuracy","title":"Forecasting with Global Models","text":"default modeltime_accuracy(acc_by_id = FALSE), returns global model accuracy.","code":"calib_tbl %>%      modeltime_accuracy(acc_by_id = FALSE) %>%      table_modeltime_accuracy(.interactive = FALSE)"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"local-accuracy","dir":"Articles","previous_headings":"Modeltime Workflow > Step 3: Measure Test Accuracy","what":"Local Accuracy","title":"Forecasting with Global Models","text":"toggling modeltime_accuracy(acc_by_id = TRUE), can obtain local model accuracy. can useful identifying specifically time series model well (poorly ). can apply model selection logic select specific global models specific IDs.","code":"calib_tbl %>%      modeltime_accuracy(acc_by_id = TRUE) %>%      table_modeltime_accuracy(.interactive = FALSE)"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"step-4-forecast-the-test-data","dir":"Articles","previous_headings":"Modeltime Workflow","what":"Step 4: Forecast the Test Data","title":"Forecasting with Global Models","text":"Next, need forecast test dataset. useful evaluate model using sampling time series within panel dataset. use modeltime_forecast(conf_by_id  = TRUE) allow confidence intervals (prediction intervals) calculated time series identifier. Note, modeltime_calibrate() must performed id specified.","code":"calib_tbl %>%     modeltime_forecast(         new_data    = testing(splits),         actual_data = data_tbl,         conf_by_id  = TRUE     ) %>%     group_by(id) %>%     plot_modeltime_forecast(         .facet_ncol  = 3,         .interactive = FALSE     )"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"step-5-refit-and-forecast-the-future","dir":"Articles","previous_headings":"Modeltime Workflow","what":"Step 5: Refit and Forecast the Future","title":"Forecasting with Global Models","text":"see global model performs well dataset. Now, forecast future.","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"refit-the-models","dir":"Articles","previous_headings":"Modeltime Workflow > Step 5: Refit and Forecast the Future","what":"Refit the Models","title":"Forecasting with Global Models","text":"refitting step needed incorporate recent information models. use modeltime_refit() update model(s) modeltime tables.","code":"refit_tbl <- calib_tbl %>%   modeltime_refit(data = data_tbl)  refit_tbl #> # Modeltime Table #> # A tibble: 1 × 5 #>   .model_id .model     .model_desc .type .calibration_data #>       <int> <list>     <chr>       <chr> <list>            #> 1         1 <workflow> XGBOOST     Test  <tibble [84 × 5]>"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"future-data","dir":"Articles","previous_headings":"Modeltime Workflow > Step 5: Refit and Forecast the Future","what":"Future Data","title":"Forecasting with Global Models","text":"Next, make future dataset using future_frame() timetk, creates future time stamps extend 52-weeks future ID panel structure needed global forecast model.","code":"future_tbl <- data_tbl %>%   group_by(id) %>%   future_frame(.length_out = 52, .bind_data = FALSE)  future_tbl #> # A tibble: 364 × 2 #> # Groups:   id [7] #>    id    date       #>    <fct> <date>     #>  1 1_1   2012-11-02 #>  2 1_1   2012-11-09 #>  3 1_1   2012-11-16 #>  4 1_1   2012-11-23 #>  5 1_1   2012-11-30 #>  6 1_1   2012-12-07 #>  7 1_1   2012-12-14 #>  8 1_1   2012-12-21 #>  9 1_1   2012-12-28 #> 10 1_1   2013-01-04 #> # … with 354 more rows"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"future-predictions","dir":"Articles","previous_headings":"Modeltime Workflow > Step 5: Refit and Forecast the Future","what":"Future Predictions","title":"Forecasting with Global Models","text":"Finally, can forecast future using future data.","code":"refit_tbl %>%   modeltime_forecast(     new_data    = future_tbl,     actual_data = data_tbl,      conf_by_id  = TRUE   ) %>%   group_by(id) %>%   plot_modeltime_forecast(     .interactive = F,     .facet_ncol  = 2   )"},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Forecasting with Global Models","text":"’ve now successfully completed Global Forecast. may find challenging, especially familiar Modeltime Workflow, terminology, tidymodeling R. case, solution. Take high-performance forecasting course.","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"take-the-high-performance-forecasting-course","dir":"Articles","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"Forecasting with Global Models","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"time-series-is-changing","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"Forecasting with Global Models","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeling-panel-data.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"Forecasting with Global Models","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeltime-model-list.html","id":"modeltime-roadmap","dir":"Articles","previous_headings":"","what":"Modeltime Roadmap","title":"Modeltime Algorithm Roadmap","text":"Modeltime growing ecosystem. Learn ’s incorporated ’s coming soon following GitHub Issue #5. (Click View GitHub)","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeltime-model-list.html","id":"learning-modeltime","dir":"Articles","previous_headings":"","what":"Learning Modeltime","title":"Modeltime Algorithm Roadmap","text":"Modeltime cutting-edge ecosystem forecasting using strategies best practices won placed highly major forecasting competitions. state---art Time Series Forecasting Course (DS4B 203-R) teaches modeltime ecosystem timetk (Machine Learning, Deep Learning, Feature Engineering Time Series). Take course become forecasting expert organization.","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"spark-for-iterative-nested-forecasting","dir":"Articles","previous_headings":"","what":"Spark for Iterative (Nested) Forecasting","title":"The Modeltime Spark Backend","text":"One common situations parallel computation required iterative forecasting data scientist needs experiment 10+ models across 10,000+ time series. ’s common large-scale, high-performance forecasting exercise take days. ’ll show can combine Modeltime Nested Forecasting ’s Parallel Spark Backend scale computation distributed parallel Spark execution. Let’s go!","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"system-requirements","dir":"Articles","previous_headings":"","what":"System Requirements","title":"The Modeltime Spark Backend","text":"tutorial requires: sparklyr: Used register spark foreach adapter via registerDoSpark(). Java: Spark installation depends Java installed. Spark Installation: Can accomplished via sparklyr::spark_install() provided user sparklyr Java.","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"libraries","dir":"Articles","previous_headings":"","what":"Libraries","title":"The Modeltime Spark Backend","text":"Load following libraries.","code":"library(sparklyr) library(tidymodels) library(modeltime) library(tidyverse) library(timetk)"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"spark-connection","dir":"Articles","previous_headings":"","what":"Spark Connection","title":"The Modeltime Spark Backend","text":"Next, set Spark connection via sparklyr. tutorial, use “local” connection. many users use Databricks scale forecasting workload. Ro run Spark locally: using Databricks, can use:","code":"sc <- spark_connect(master = \"local\") sc <- spark_connect(method = \"databricks\")"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"setup-the-spark-backend","dir":"Articles","previous_headings":"","what":"Setup the Spark Backend","title":"The Modeltime Spark Backend","text":"Next, register Spark Backend using parallel_start(sc, .method = \"spark\"). helper set registerDoSpark() foreach adaptor. layman’s terms, just means can now run parallel using Spark.","code":"parallel_start(sc, .method = \"spark\")"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"data-preparation-nested-forecasting","dir":"Articles","previous_headings":"","what":"Data Preparation (Nested Forecasting)","title":"The Modeltime Spark Backend","text":"’ll run minimal nested forecasting example. Refer Nested Forecasting Tutorial information perform iterative forecasting “nested” data structure modeltime. dataset ’ll forecasting walmart_sales_weekly, modify just include 3 columns: “id”, “date”, “value”. id feature grouping variable. date feature contains timestamps. value feature sales value Walmart store-department combination.  prepare nested data using Nested Forecasting preparation functions.","code":"walmart_sales_weekly %>%     select(id, Date, Weekly_Sales) %>%     set_names(c(\"id\", \"date\", \"value\")) %>%     group_by(id) %>%     plot_time_series(date, value, .facet_ncol = 2, .interactive = F) nested_data_tbl <- walmart_sales_weekly %>%     select(id, Date, Weekly_Sales) %>%     set_names(c(\"id\", \"date\", \"value\")) %>%     extend_timeseries(         .id_var        = id,         .date_var      = date,         .length_future = 52     ) %>%     nest_timeseries(         .id_var        = id,         .length_future = 52     ) %>%          split_nested_timeseries(         .length_test = 52     )  nested_data_tbl #> # A tibble: 7 × 4 #>   id    .actual_data       .future_data      .splits         #>   <fct> <list>             <list>            <list>          #> 1 1_1   <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 2 1_3   <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 3 1_8   <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 4 1_13  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 5 1_38  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 6 1_93  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 7 1_95  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]>"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"modeling","dir":"Articles","previous_headings":"","what":"Modeling","title":"The Modeltime Spark Backend","text":"’ll create two unfitted models: XGBoost Prophet. ’ll use modeltime_nested_fit() iteratively fit models time series using Spark Backend.","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"xgboost","dir":"Articles","previous_headings":"","what":"XGBoost","title":"The Modeltime Spark Backend","text":"create XGBoost model features derived date column.","code":"rec_xgb <- recipe(value ~ ., extract_nested_train_split(nested_data_tbl)) %>%     step_timeseries_signature(date) %>%     step_rm(date) %>%     step_zv(all_predictors()) %>%     step_dummy(all_nominal_predictors(), one_hot = TRUE)  wflw_xgb <- workflow() %>%     add_model(boost_tree(\"regression\") %>% set_engine(\"xgboost\")) %>%     add_recipe(rec_xgb)  wflw_xgb #> ══ Workflow ════════════════════════════════════════════════════════════════════ #> Preprocessor: Recipe #> Model: boost_tree() #>  #> ── Preprocessor ──────────────────────────────────────────────────────────────── #> 4 Recipe Steps #>  #> • step_timeseries_signature() #> • step_rm() #> • step_zv() #> • step_dummy() #>  #> ── Model ─────────────────────────────────────────────────────────────────────── #> Boosted Tree Model Specification (regression) #>  #> Computational engine: xgboost"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"prophet","dir":"Articles","previous_headings":"","what":"Prophet","title":"The Modeltime Spark Backend","text":"Next, create prophet workflow.","code":"rec_prophet <- recipe(value ~ date, extract_nested_train_split(nested_data_tbl))   wflw_prophet <- workflow() %>%     add_model(         prophet_reg(\"regression\", seasonality_yearly = TRUE) %>%              set_engine(\"prophet\")     ) %>%     add_recipe(rec_prophet)  wflw_prophet #> ══ Workflow ════════════════════════════════════════════════════════════════════ #> Preprocessor: Recipe #> Model: prophet_reg() #>  #> ── Preprocessor ──────────────────────────────────────────────────────────────── #> 0 Recipe Steps #>  #> ── Model ─────────────────────────────────────────────────────────────────────── #> PROPHET Regression Model Specification (regression) #>  #> Main Arguments: #>   seasonality_yearly = TRUE #>  #> Computational engine: prophet"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"nested-forecasting-with-spark","dir":"Articles","previous_headings":"","what":"Nested Forecasting with Spark","title":"The Modeltime Spark Backend","text":"Now, beauty everything set us perform nested forecasting Spark. simply use modeltime_nested_fit() make sure uses Spark Backend setting control_nested_fit(allow_par = TRUE). Note take 30-seconds one-time cost move data, libraries, environment variables Spark clusters. good news scale 10,000+ time series, one-time cost minimal compared speed distributed computation. nested modeltime object now fit models using Spark.","code":"nested_modeltime_tbl <- nested_data_tbl %>%     modeltime_nested_fit(         wflw_xgb,         wflw_prophet,                  control = control_nested_fit(allow_par = TRUE, verbose = TRUE)     ) #> Using existing parallel backend with 12 clusters (cores)... #>  Beginning Parallel Loop | 0.004 seconds #>  Finishing parallel backend. Clusters are remaining open. | 20.186 seconds #>  Close clusters by running: `parallel_stop()`. #>  #> Finished in: 20.18654 secs. nested_modeltime_tbl #> # Nested Modeltime Table #>   # A tibble: 7 × 5 #>   id    .actual_data       .future_data      .splits         .modeltime_tables   #>   <fct> <list>             <list>            <list>          <list>              #> 1 1_1   <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> <mdl_time_tbl [2 ×… #> 2 1_3   <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> <mdl_time_tbl [2 ×… #> 3 1_8   <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> <mdl_time_tbl [2 ×… #> 4 1_13  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> <mdl_time_tbl [2 ×… #> 5 1_38  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> <mdl_time_tbl [2 ×… #> 6 1_93  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> <mdl_time_tbl [2 ×… #> 7 1_95  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> <mdl_time_tbl [2 ×…"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"model-test-accuracy","dir":"Articles","previous_headings":"","what":"Model Test Accuracy","title":"The Modeltime Spark Backend","text":"can observe results. First, can check accuracy model","code":"nested_modeltime_tbl %>%   extract_nested_test_accuracy() %>%   table_modeltime_accuracy(.interactive = F)"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"test-forecast","dir":"Articles","previous_headings":"","what":"Test Forecast","title":"The Modeltime Spark Backend","text":"Next, can examine test forecast models.","code":"nested_modeltime_tbl %>%   extract_nested_test_forecast() %>%   group_by(id) %>%   plot_modeltime_forecast(.facet_ncol = 2, .interactive = F)"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"close-clusters-and-shutdown-spark","dir":"Articles","previous_headings":"","what":"Close Clusters and Shutdown Spark","title":"The Modeltime Spark Backend","text":"can close Spark adapter shut Spark session finished.","code":"# Unregisters the Spark Backend parallel_stop()  # Disconnects Spark spark_disconnect_all() #> [1] 1"},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"The Modeltime Spark Backend","text":"’ve now successfully completed Nested Forecast using Spark Backend. may find challenging, especially familiar Modeltime Workflow, terminology, tidy-modeling R. case, solution. Take high-performance forecasting course.","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"take-the-high-performance-forecasting-course","dir":"Articles","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"The Modeltime Spark Backend","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"time-series-is-changing","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"The Modeltime Spark Backend","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/modeltime/articles/modeltime-spark.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"The Modeltime Spark Backend","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"nested-forecasting","dir":"Articles","previous_headings":"","what":"Nested Forecasting","title":"Nested Forecasting","text":"core idea nested forecasting convert dataset containing many time series groups nested data set, fit many models nested datasets. result iterative forecasting process generates Nested Modeltime Tables forecast attributes needed make decisions.","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"important-concepts-logging-attributes","dir":"Articles","previous_headings":"","what":"Important Concepts: Logging & Attributes","title":"Nested Forecasting","text":"new feature nested forecasting workflow logged attributes, useful complex workflows loops (iteration) performed. Nested Modeltime Table, push many operations possible fitting refitting stages, logging important aspects including: Test Accuracy: extract_nested_test_accuracy() Test Forecast: extract_nested_test_forecast() Error Reports: extract_nested_error_report() Best Models: extract_nested_best_model_report() Future Forecasts: extract_nested_future_forecast() deviates traditional Modeltime Workflow, find logging vastly speeds experimentation information retrieval especially number time series increases.","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting Started","title":"Nested Forecasting","text":"’ll go short tutorial Nested Forecasting. first thing load following libraries:","code":"library(tidymodels) library(modeltime) library(tidyverse) library(timetk)"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"dataset","dir":"Articles","previous_headings":"Getting Started","what":"Dataset","title":"Nested Forecasting","text":"Next, let’s use walmart_sales_weekly dataset comes timetk. problem dataset ’s set panel data modeling. important columns : “id”: separates time series groups (case represent sales departments walmart store) “date”: weekly sales period “value”: value sales week store/department can visualize time series group expose differences sales department.  can clearly see 7 time series groups different weekly sales patterns.","code":"data_tbl <- walmart_sales_weekly %>%     select(id, Date, Weekly_Sales) %>%     set_names(c(\"id\", \"date\", \"value\"))  data_tbl #> # A tibble: 1,001 × 3 #>    id    date        value #>    <fct> <date>      <dbl> #>  1 1_1   2010-02-05 24924. #>  2 1_1   2010-02-12 46039. #>  3 1_1   2010-02-19 41596. #>  4 1_1   2010-02-26 19404. #>  5 1_1   2010-03-05 21828. #>  6 1_1   2010-03-12 21043. #>  7 1_1   2010-03-19 22137. #>  8 1_1   2010-03-26 26229. #>  9 1_1   2010-04-02 57258. #> 10 1_1   2010-04-09 42961. #> # … with 991 more rows data_tbl %>%   group_by(id) %>%   plot_time_series(     date, value, .interactive = F, .facet_ncol = 2   )"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"nested-forecasting-preparation","dir":"Articles","previous_headings":"Getting Started","what":"Nested Forecasting Preparation","title":"Nested Forecasting","text":"two key components need prepare : Nested Data Structure: critical ensure data prepared (covered next) Nested Modeltime Workflow: stage create many models, fit models data, generate forecasts scale Conceptually, workflow looks like combine nested data tidymodels workflows using upcoming function, modeltime_nested_fit().","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"Data Preparation","title":"Nested Forecasting","text":"critical stage “Nested Forecasting” data preparation, making sure input nested forecasting workflow appropriate structure. ’ve included several functions help involve bit forethought can broken 3 steps: Extending times series: far future need predict time series? See extend_timeseries(). Nesting grouping variable: create nested structure. ’ll identify ID column separates time series, number timestamps include “.future_data” optionally “.actual_data”. Typically, ’ll select .length_future extension previous step. See nest_timeseries(). Train/Test Set Splitting: Finally, ’ll take .actual_data convert train/test splits can used accuracy confidence interval estimation. See split_nested_timeseries(). 3-steps action: creates nested tibble “.actual_data”, “.future_data”, “.splits”. column help nested modeltime workflow.","code":"nested_data_tbl <- data_tbl %>%          # 1. Extending: We'll predict 52 weeks into the future.     extend_timeseries(         .id_var        = id,         .date_var      = date,         .length_future = 52     ) %>%          # 2. Nesting: We'll group by id, and create a future dataset     #    that forecasts 52 weeks of extended data and     #    an actual dataset that contains 104 weeks (2-years of data)     nest_timeseries(         .id_var        = id,         .length_future = 52,         .length_actual = 52*2     ) %>%         # 3. Splitting: We'll take the actual data and create splits    #    for accuracy and confidence interval estimation of 52 weeks (test)    #    and the rest is training data     split_nested_timeseries(         .length_test = 52     )  nested_data_tbl #> # A tibble: 7 × 4 #>   id    .actual_data       .future_data      .splits         #>   <fct> <list>             <list>            <list>          #> 1 1_1   <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> #> 2 1_3   <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> #> 3 1_8   <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> #> 4 1_13  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> #> 5 1_38  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> #> 6 1_93  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> #> 7 1_95  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]>"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"nested-modeltime-workflow","dir":"Articles","previous_headings":"","what":"Nested Modeltime Workflow","title":"Nested Forecasting","text":"Next, move Nested Modeltime Workflow now nested data created. Nested Modeltime Workflow includes 3 core functions: Modeling Fitting: training stage fit training data. test forecast generated step. See modeltime_nested_fit(). Model Evaluation Selection: review model performance select best model minimizing maximizing error metric. See modeltime_nested_select_best(). Model Refitting: final fitting stage fit actual data. future forecast generated step. See modeltime_nested_refit().","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"step-1-create-tidymodels-workflows","dir":"Articles","previous_headings":"Nested Modeltime Workflow","what":"Step 1: Create Tidymodels Workflows","title":"Nested Forecasting","text":"First, create tidymodels workflows various models intend create.","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"prophet","dir":"Articles","previous_headings":"Nested Modeltime Workflow > Step 1: Create Tidymodels Workflows","what":"Prophet","title":"Nested Forecasting","text":"common modeling method prophet, can created using prophet_reg(). ’ll create workflow. Note use extract_nested_train_split(nested_data_tbl) help us build preprocessing features.","code":"rec_prophet <- recipe(value ~ date, extract_nested_train_split(nested_data_tbl))   wflw_prophet <- workflow() %>%     add_model(       prophet_reg(\"regression\", seasonality_yearly = TRUE) %>%          set_engine(\"prophet\")     ) %>%     add_recipe(rec_prophet)"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"xgboost","dir":"Articles","previous_headings":"Nested Modeltime Workflow > Step 1: Create Tidymodels Workflows","what":"XGBoost","title":"Nested Forecasting","text":"Next, can use machine learning method can get good results: XGBoost. add extra features recipe feature engineering step generate features tend get better modeling results. Note use extract_nested_train_split(nested_data_tbl) help us build preprocessing features.","code":"rec_xgb <- recipe(value ~ ., extract_nested_train_split(nested_data_tbl)) %>%     step_timeseries_signature(date) %>%     step_rm(date) %>%     step_zv(all_predictors()) %>%     step_dummy(all_nominal_predictors(), one_hot = TRUE)  wflw_xgb <- workflow() %>%     add_model(boost_tree(\"regression\") %>% set_engine(\"xgboost\")) %>%     add_recipe(rec_xgb)"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"step-2-nested-modeltime-tables","dir":"Articles","previous_headings":"Nested Modeltime Workflow","what":"Step 2: Nested Modeltime Tables","title":"Nested Forecasting","text":"couple modeling workflows hand, now ready test time series. start using modeltime_nested_fit() function, iteratively fits model nested time series train/test “.splits” column. adds new column .modeltime_tables data sets created several logged attributes part “Nested Modeltime Table”. also can see models trained “.splits” none models errors.","code":"nested_modeltime_tbl <- modeltime_nested_fit(   # Nested data    nested_data = nested_data_tbl,      # Add workflows   wflw_prophet,   wflw_xgb ) #> Fitting models on training data... ■■■■■                             14% | ETA:… #> Fitting models on training data... ■■■■■■■■■■■■■■                    43% | ETA:… #> Fitting models on training data... ■■■■■■■■■■■■■■■■■■■■■■            71% | ETA:… #> Fitting models on training data... ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% | ETA:…  nested_modeltime_tbl #> # Nested Modeltime Table #>  #> Trained on: .splits | Model Errors: [0] #> # A tibble: 7 × 5 #>   id    .actual_data       .future_data      .splits         .modeltime_tables   #>   <fct> <list>             <list>            <list>          <list>              #> 1 1_1   <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [2 ×… #> 2 1_3   <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [2 ×… #> 3 1_8   <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [2 ×… #> 4 1_13  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [2 ×… #> 5 1_38  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [2 ×… #> 6 1_93  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [2 ×… #> 7 1_95  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [2 ×…"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"step-3-logged-attributes","dir":"Articles","previous_headings":"Nested Modeltime Workflow","what":"Step 3: Logged Attributes","title":"Nested Forecasting","text":"forecasting, iterative modeltime fitting process logs important information enable us evaluate models. logged attributes accessable “extract” functions.","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"extract-nested-test-accuracy","dir":"Articles","previous_headings":"Nested Modeltime Workflow > Step 3: Logged Attributes","what":"Extract Nested Test Accuracy","title":"Nested Forecasting","text":"Using extract_nested_test_accuracy(), can get accuracy measures time series model. allows us see model performs best time series.","code":"nested_modeltime_tbl %>%    extract_nested_test_accuracy() %>%   table_modeltime_accuracy(.interactive = F)"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"extract-nested-test-forecast","dir":"Articles","previous_headings":"Nested Modeltime Workflow > Step 3: Logged Attributes","what":"Extract Nested Test Forecast","title":"Nested Forecasting","text":"Next, can visualize test forecast extract_nested_test_forecast().","code":"nested_modeltime_tbl %>%    extract_nested_test_forecast() %>%   group_by(id) %>%   plot_modeltime_forecast(     .facet_ncol  = 2,     .interactive = FALSE   )"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"extract-nested-error-logs","dir":"Articles","previous_headings":"Nested Modeltime Workflow > Step 3: Logged Attributes","what":"Extract Nested Error Logs","title":"Nested Forecasting","text":"models errors, can investigate error logs extract_nested_error_report(). Fortunately, don’t errors, investigate .","code":"nested_modeltime_tbl %>%    extract_nested_error_report() #> # A tibble: 0 × 4 #> # … with 4 variables: id <fct>, .model_id <int>, .model_desc <chr>, #> #   .error_desc <chr>"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"step-4-select-the-best","dir":"Articles","previous_headings":"Nested Modeltime Workflow","what":"Step 4: Select the Best","title":"Nested Forecasting","text":"Using accuracy data, can pick metric select best model based metric. available metrics default_forecast_accuracy_metric_set(). Make sure select minimize based metric. filter_test_forecasts parameter tells function filter logged test forecasts just best. identifies models used final forecast. models selected, can make future forecast.","code":"best_nested_modeltime_tbl <- nested_modeltime_tbl %>%     modeltime_nested_select_best(       metric                = \"rmse\",        minimize              = TRUE,        filter_test_forecasts = TRUE     )"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"extract-nested-best-model-report","dir":"Articles","previous_headings":"Nested Modeltime Workflow > Step 4: Select the Best","what":"Extract Nested Best Model Report","title":"Nested Forecasting","text":"best model selections can accessed extract_nested_best_model_report().","code":"best_nested_modeltime_tbl %>%   extract_nested_best_model_report() #> # Nested Modeltime Table #>   # A tibble: 7 × 10 #>   id    .model_id .model_desc .type    mae  mape  mase smape   rmse   rsq #>   <fct>     <int> <chr>       <chr>  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> #> 1 1_1           2 XGBOOST     Test   6237. 25.3  1.23  24.6   9017. 0.191 #> 2 1_3           1 PROPHET     Test   3540. 29.9  1.37  25.5   4708. 0.796 #> 3 1_8           2 XGBOOST     Test   3586.  9.33 1.53   9.89  4009. 0.297 #> 4 1_13          2 XGBOOST     Test   2338.  5.83 0.861  6.02  2721. 0.536 #> 5 1_38          2 XGBOOST     Test   6847.  8.47 0.585  8.75  8825. 0.402 #> 6 1_93          2 XGBOOST     Test   7234.  9.11 0.728  9.66  8879. 0.488 #> 7 1_95          2 XGBOOST     Test  10784.  8.54 1.30   8.96 12843. 0.139"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"extract-nested-best-test-forecasts","dir":"Articles","previous_headings":"Nested Modeltime Workflow > Step 4: Select the Best","what":"Extract Nested Best Test Forecasts","title":"Nested Forecasting","text":"’ve selected best models, can easily visualize best forecasts time series. Note nested test forecast logs modified isolate best models.","code":"best_nested_modeltime_tbl %>%   extract_nested_test_forecast() %>%   group_by(id) %>%   plot_modeltime_forecast(     .facet_ncol  = 2,     .interactive = FALSE   )"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"step-5-refitting-and-future-forecast","dir":"Articles","previous_headings":"Nested Modeltime Workflow","what":"Step 5: Refitting and Future Forecast","title":"Nested Forecasting","text":"best models hand, can make future forecasts refitting models full dataset. best models selected, best models refit. best models selected, models refit. ’ve selected best models, move forward refitting future forecast logging using modeltime_nested_refit() function. Note used control_nested_refit(verbose = TRUE) display modeling results model refit. necessary, can useful follow nested model fitting process. can see nested modeltime table appears , now trained .actual_data.","code":"nested_modeltime_refit_tbl <- best_nested_modeltime_tbl %>%     modeltime_nested_refit(         control = control_nested_refit(verbose = TRUE)     ) #> ℹ [1/7] Starting Modeltime Table: ID 1_1... #> ✓ Model 2 Passed XGBOOST. #> ✓ [1/7] Finished Modeltime Table: ID 1_1 #> ℹ [2/7] Starting Modeltime Table: ID 1_3... #> ✓ Model 1 Passed PROPHET. #> ✓ [2/7] Finished Modeltime Table: ID 1_3 #> ℹ [3/7] Starting Modeltime Table: ID 1_8... #> ✓ Model 2 Passed XGBOOST. #> ✓ [3/7] Finished Modeltime Table: ID 1_8 #> ℹ [4/7] Starting Modeltime Table: ID 1_13... #> ✓ Model 2 Passed XGBOOST. #> ✓ [4/7] Finished Modeltime Table: ID 1_13 #> ℹ [5/7] Starting Modeltime Table: ID 1_38... #> ✓ Model 2 Passed XGBOOST. #> ✓ [5/7] Finished Modeltime Table: ID 1_38 #> ℹ [6/7] Starting Modeltime Table: ID 1_93... #> ✓ Model 2 Passed XGBOOST. #> ✓ [6/7] Finished Modeltime Table: ID 1_93 #> ℹ [7/7] Starting Modeltime Table: ID 1_95... #> ✓ Model 2 Passed XGBOOST. #> ✓ [7/7] Finished Modeltime Table: ID 1_95 #>  #> Finished in: 3.590017 secs. nested_modeltime_refit_tbl #> # Nested Modeltime Table #>  #> Trained on: .actual_data | Model Errors: [0] #> # A tibble: 7 × 5 #>   id    .actual_data       .future_data      .splits         .modeltime_tables   #>   <fct> <list>             <list>            <list>          <list>              #> 1 1_1   <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [1 ×… #> 2 1_3   <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [1 ×… #> 3 1_8   <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [1 ×… #> 4 1_13  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [1 ×… #> 5 1_38  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [1 ×… #> 6 1_93  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [1 ×… #> 7 1_95  <tibble [104 × 2]> <tibble [52 × 2]> <split [52|52]> <mdl_time_tbl [1 ×…"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"extract-nested-future-forecast","dir":"Articles","previous_headings":"Nested Modeltime Workflow > Step 5: Refitting and Future Forecast","what":"Extract Nested Future Forecast","title":"Nested Forecasting","text":"refitting process completes, can now access future forecast, logged.","code":"nested_modeltime_refit_tbl %>%   extract_nested_future_forecast() %>%   group_by(id) %>%   plot_modeltime_forecast(     .interactive = FALSE,     .facet_ncol  = 2   )"},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Nested Forecasting","text":"’ve now successfully completed Nested Forecast. may find challenging, especially familiar Modeltime Workflow, terminology, tidymodeling R. case, solution. Take high-performance forecasting course.","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"take-the-high-performance-forecasting-course","dir":"Articles","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"Nested Forecasting","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"time-series-is-changing","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"Nested Forecasting","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/modeltime/articles/nested-forecasting.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"Nested Forecasting","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"out-of-the-box-parallel-processing-functionality-included","dir":"Articles","previous_headings":"","what":"Out-of-the-BoxParallel Processing Functionality Included","title":"Hyperparameter Tuning with Parallel Processing","text":"modeltime package (>= 0.6.1) comes parallel processing functionality. Use parallel_start() parallel_stop() simplify parallel processing setup. Use create_model_grid() help generating parsnip model specs dials parameter grids. Use modeltime_fit_workflowset() initial fitting many models parallel using workflowsets tidymodels ecosystem. Use modeltime_refit() refit models parallel. Use control_fit_workflowset() control_refit() controlling fitting refitting many models.","code":""},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"how-to-use-parallel-processing","dir":"Articles","previous_headings":"","what":"How to Use Parallel Processing","title":"Hyperparameter Tuning with Parallel Processing","text":"Let’s go common Hyperparameter Tuning workflow shows modeltime parallel processing integration support workflowsets tidymodels ecosystem.","code":""},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"libraries","dir":"Articles","previous_headings":"","what":"Libraries","title":"Hyperparameter Tuning with Parallel Processing","text":"Load following libraries.","code":"# Machine Learning library(modeltime) library(tidymodels) library(workflowsets)  # Core library(tidyverse) library(timetk)"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"setup-parallel-backend","dir":"Articles","previous_headings":"","what":"Setup Parallel Backend","title":"Hyperparameter Tuning with Parallel Processing","text":"modeltime package uses parallel_start() simplify setup, integrates multiple backend options parallel processing including: .method = \"parallel\" (default): Uses parallel doParallel packages. .method = \"spark\": Uses sparklyr. See tutorial, Modeltime Spark Backend.","code":""},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"parallel-setup","dir":"Articles","previous_headings":"Setup Parallel Backend","what":"Parallel Setup","title":"Hyperparameter Tuning with Parallel Processing","text":"’ll set tutorial use two (2) cores using default parallel package. simplify creating clusters, modeltime includes parallel_start(). can simply supply number cores ’d like use. detect many physical cores , can run parallel::detectCores(logical = FALSE). .method argument specifies want parallel backend.","code":"parallel_start(2, .method = \"parallel\")"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"spark-setup","dir":"Articles","previous_headings":"Setup Parallel Backend","what":"Spark Setup","title":"Hyperparameter Tuning with Parallel Processing","text":"optionally run tutorial Spark Backend. information, refer tutorial Modeltime Spark Backend.","code":"# OPTIONAL - Run using spark backend library(sparklyr)  sc <- spark_connect(master = \"local\")  parallel_start(sc, .method = \"spark\")"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"load-data","dir":"Articles","previous_headings":"","what":"Load Data","title":"Hyperparameter Tuning with Parallel Processing","text":"’ll use walmart_sales_weeekly dataset timetk. seven (7) time series represent weekly sales demand department.","code":"dataset_tbl <- walmart_sales_weekly %>%   select(id, Date, Weekly_Sales)  dataset_tbl %>%    group_by(id) %>%   plot_time_series(     .date_var    = Date,      .value       = Weekly_Sales,      .facet_ncol  = 2,      .interactive = FALSE   )"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"train-test-splits","dir":"Articles","previous_headings":"","what":"Train / Test Splits","title":"Hyperparameter Tuning with Parallel Processing","text":"Use time_series_split() make temporal split seven time series.","code":"splits <- time_series_split(   dataset_tbl,    assess     = \"6 months\",    cumulative = TRUE )  splits %>%    tk_time_series_cv_plan() %>%    plot_time_series_cv_plan(Date, Weekly_Sales, .interactive = F)"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"recipe","dir":"Articles","previous_headings":"","what":"Recipe","title":"Hyperparameter Tuning with Parallel Processing","text":"Make preprocessing recipe generates time series features.","code":"recipe_spec_1 <- recipe(Weekly_Sales ~ ., data = training(splits)) %>%   step_timeseries_signature(Date) %>%   step_rm(Date) %>%   step_normalize(Date_index.num) %>%   step_zv(all_predictors()) %>%   step_dummy(all_nominal_predictors(), one_hot = TRUE)"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"model-specifications","dir":"Articles","previous_headings":"","what":"Model Specifications","title":"Hyperparameter Tuning with Parallel Processing","text":"’ll make 6 xgboost model specifications using boost_tree() “xgboost” engine. combined recipe previous step using workflow_set() next section.","code":""},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"the-general-idea","dir":"Articles","previous_headings":"Model Specifications","what":"The general idea","title":"Hyperparameter Tuning with Parallel Processing","text":"can vary learn_rate parameter see ’s effect forecast error.","code":"# XGBOOST MODELS model_spec_xgb_1 <- boost_tree(learn_rate = 0.001) %>%   set_engine(\"xgboost\")  model_spec_xgb_2 <- boost_tree(learn_rate = 0.010) %>%   set_engine(\"xgboost\")  model_spec_xgb_3 <- boost_tree(learn_rate = 0.100) %>%   set_engine(\"xgboost\")  model_spec_xgb_4 <- boost_tree(learn_rate = 0.350) %>%   set_engine(\"xgboost\")  model_spec_xgb_5 <- boost_tree(learn_rate = 0.500) %>%   set_engine(\"xgboost\")  model_spec_xgb_6 <- boost_tree(learn_rate = 0.650) %>%   set_engine(\"xgboost\")"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"a-faster-way","dir":"Articles","previous_headings":"Model Specifications","what":"A faster way","title":"Hyperparameter Tuning with Parallel Processing","text":"may notice lot repeated code adjust learn_rate. simplify process, can use create_model_grid().","code":"model_tbl <- tibble(   learn_rate = c(0.001, 0.010, 0.100, 0.350, 0.500, 0.650) ) %>%   create_model_grid(     f_model_spec = boost_tree,     engine_name  = \"xgboost\",     mode         = \"regression\"   )  model_tbl #> # A tibble: 6 × 2 #>   learn_rate .models   #>        <dbl> <list>    #> 1      0.001 <spec[+]> #> 2      0.01  <spec[+]> #> 3      0.1   <spec[+]> #> 4      0.35  <spec[+]> #> 5      0.5   <spec[+]> #> 6      0.65  <spec[+]>"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"extracting-the-model-list","dir":"Articles","previous_headings":"Model Specifications","what":"Extracting the model list","title":"Hyperparameter Tuning with Parallel Processing","text":"can extract model list use workflowset next. result placed manually generated 6 model specs list().","code":"model_list <- model_tbl$.models  model_list #> [[1]] #> Boosted Tree Model Specification (regression) #>  #> Main Arguments: #>   learn_rate = 0.001 #>  #> Computational engine: xgboost  #>  #>  #> [[2]] #> Boosted Tree Model Specification (regression) #>  #> Main Arguments: #>   learn_rate = 0.01 #>  #> Computational engine: xgboost  #>  #>  #> [[3]] #> Boosted Tree Model Specification (regression) #>  #> Main Arguments: #>   learn_rate = 0.1 #>  #> Computational engine: xgboost  #>  #>  #> [[4]] #> Boosted Tree Model Specification (regression) #>  #> Main Arguments: #>   learn_rate = 0.35 #>  #> Computational engine: xgboost  #>  #>  #> [[5]] #> Boosted Tree Model Specification (regression) #>  #> Main Arguments: #>   learn_rate = 0.5 #>  #> Computational engine: xgboost  #>  #>  #> [[6]] #> Boosted Tree Model Specification (regression) #>  #> Main Arguments: #>   learn_rate = 0.65 #>  #> Computational engine: xgboost"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"workflowsets","dir":"Articles","previous_headings":"","what":"Workflowsets","title":"Hyperparameter Tuning with Parallel Processing","text":"workflow_set() function, can combine 6 xgboost models 1 recipe return six (6) combinations recipe model specifications. currently untrained (unfitted).","code":"model_wfset <- workflow_set(   preproc = list(     recipe_spec_1   ),   models = model_list,    cross = TRUE )  model_wfset #> # A workflow set/tibble: 6 × 4 #>   wflow_id            info             option    result     #>   <chr>               <list>           <list>    <list>     #> 1 recipe_boost_tree_1 <tibble [1 × 4]> <opts[0]> <list [0]> #> 2 recipe_boost_tree_2 <tibble [1 × 4]> <opts[0]> <list [0]> #> 3 recipe_boost_tree_3 <tibble [1 × 4]> <opts[0]> <list [0]> #> 4 recipe_boost_tree_4 <tibble [1 × 4]> <opts[0]> <list [0]> #> 5 recipe_boost_tree_5 <tibble [1 × 4]> <opts[0]> <list [0]> #> 6 recipe_boost_tree_6 <tibble [1 × 4]> <opts[0]> <list [0]>"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"parallel-training-fitting","dir":"Articles","previous_headings":"","what":"Parallel Training (Fitting)","title":"Hyperparameter Tuning with Parallel Processing","text":"can train combinations parallel.","code":""},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"controlling-the-fitting-proces","dir":"Articles","previous_headings":"Parallel Training (Fitting)","what":"Controlling the Fitting Proces","title":"Hyperparameter Tuning with Parallel Processing","text":"fitting function modeltime “control” function: control_fit_workflowset() modeltime_fit_workflowset() control_refit() modeltime_refit() control functions help user control verbosity (adding remarks training) set parallel processing. can see output verbose = TRUE allow_par = TRUE. allow_par: Whether user indicated parallel processing used. user set parallel processing externally, clusters reused. user set parallel processing, fitting (training) process set parallel processing internally shutdown. Note expensive, usually costs around 10-15 seconds set . verbose: return important messages showing progress fitting operation. cores: cores user set . Since ’ve already set doParallel use 2 cores, control recognizes . packages: packages packages sent workers.","code":"control_fit_workflowset(   verbose   = TRUE,   allow_par = TRUE ) #> workflowset control object #> -------------------------- #> allow_par : TRUE  #> cores     : 2  #> verbose   : TRUE  #> packages  : modeltime parsnip workflows dplyr stats lubridate tidymodels timetk rsample recipes yardstick dials tune forcats stringr readr tidyverse workflowsets tidyr tibble purrr modeldata infer ggplot2 scales broom graphics grDevices utils datasets methods base"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"fitting-using-parallel-backend","dir":"Articles","previous_headings":"Parallel Training (Fitting)","what":"Fitting Using Parallel Backend","title":"Hyperparameter Tuning with Parallel Processing","text":"use modeltime_fit_workflowset() control_fit_workflowset() together train unfitted workflowset parallel. returns modeltime table.","code":"model_parallel_tbl <- model_wfset %>%   modeltime_fit_workflowset(     data    = training(splits),     control = control_fit_workflowset(       verbose   = TRUE,       allow_par = TRUE     )   ) #> Using existing parallel backend with 2 clusters (cores)... #>  Beginning Parallel Loop | 0.007 seconds #>  Finishing parallel backend. Clusters are remaining open. | 13.383 seconds #>  Close clusters by running: `parallel_stop()`. #>  Total time | 13.384 seconds model_parallel_tbl #> # Modeltime Table #> # A tibble: 6 × 3 #>   .model_id .model     .model_desc         #>       <int> <list>     <chr>               #> 1         1 <workflow> RECIPE_BOOST_TREE_1 #> 2         2 <workflow> RECIPE_BOOST_TREE_2 #> 3         3 <workflow> RECIPE_BOOST_TREE_3 #> 4         4 <workflow> RECIPE_BOOST_TREE_4 #> 5         5 <workflow> RECIPE_BOOST_TREE_5 #> 6         6 <workflow> RECIPE_BOOST_TREE_6"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"comparison-to-sequential-backend","dir":"Articles","previous_headings":"Parallel Training (Fitting)","what":"Comparison to Sequential Backend","title":"Hyperparameter Tuning with Parallel Processing","text":"can compare sequential backend. slight perfomance boost. Note performance benefit increases size training task.","code":"model_sequential_tbl <- model_wfset %>%   modeltime_fit_workflowset(     data    = training(splits),     control = control_fit_workflowset(       verbose   = TRUE,       allow_par = FALSE     )   ) #> ℹ Fitting Model: 1 #> ✓ Model Successfully Fitted: 1 #> ℹ Fitting Model: 2 #> ✓ Model Successfully Fitted: 2 #> ℹ Fitting Model: 3 #> ✓ Model Successfully Fitted: 3 #> ℹ Fitting Model: 4 #> ✓ Model Successfully Fitted: 4 #> ℹ Fitting Model: 5 #> ✓ Model Successfully Fitted: 5 #> ℹ Fitting Model: 6 #> ✓ Model Successfully Fitted: 6 #> Total time | 16.519 seconds"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"accuracy-assessment","dir":"Articles","previous_headings":"","what":"Accuracy Assessment","title":"Hyperparameter Tuning with Parallel Processing","text":"can review forecast accuracy. can see Model 5 lowest MAE.","code":"model_parallel_tbl %>%   modeltime_calibrate(testing(splits)) %>%   modeltime_accuracy() %>%   table_modeltime_accuracy(.interactive = FALSE)"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"forecast-assessment","dir":"Articles","previous_headings":"","what":"Forecast Assessment","title":"Hyperparameter Tuning with Parallel Processing","text":"can visualize forecast.","code":"model_parallel_tbl %>%   modeltime_forecast(     new_data    = testing(splits),     actual_data = dataset_tbl,     keep_data   = TRUE   ) %>%   group_by(id) %>%   plot_modeltime_forecast(     .facet_ncol  = 3,     .interactive = FALSE   )"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"closing-clusters","dir":"Articles","previous_headings":"","what":"Closing Clusters","title":"Hyperparameter Tuning with Parallel Processing","text":"can close parallel clusters using parallel_stop().","code":"parallel_stop()"},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Hyperparameter Tuning with Parallel Processing","text":"just showcased simple Hyperparameter Tuning example using Parallel Processing. simple problem. , ’s lot learning time series. Many algorithms Ensembling Machine Learning Deep Learning Scalable Modeling: 10,000+ time series probably thinking ever going learn time series forecasting. ’s solution save years struggling.","code":""},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"take-the-high-performance-forecasting-course","dir":"Articles","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"Hyperparameter Tuning with Parallel Processing","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"time-series-is-changing","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"Hyperparameter Tuning with Parallel Processing","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/modeltime/articles/parallel-processing.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"Hyperparameter Tuning with Parallel Processing","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"what-is-a-recursive-model","dir":"Articles","previous_headings":"","what":"What is a Recursive Model?","title":"Autoregressive Forecasting with Recursive","text":"recursive model uses predictions generate new values independent features. features typically lags used autoregressive models.","code":""},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"why-is-recursive-needed-for-autoregressive-models","dir":"Articles","previous_headings":"","what":"Why is Recursive needed for Autoregressive Models?","title":"Autoregressive Forecasting with Recursive","text":"’s important understand recursive model needed using lagged features Lag Size < Forecast Horizon. lag length less forecast horizon, problem exists missing values (NA) generated future data. solution recursive() implements iteratively fill missing values values generated predictions. technique can used : Single time series predictions - Effectively turning tidymodels model Autoregressive (AR) model Panel time series predictions - many situations need forecast one time series. can batch-process 1 model processing time series groups panels. technique can extended recursive forecasting scalable models (1 model predicts many time series).","code":""},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"libraries","dir":"Articles","previous_headings":"","what":"Libraries","title":"Autoregressive Forecasting with Recursive","text":"Load following libraries.","code":"library(modeltime) library(tidymodels) library(tidyverse) library(lubridate) library(timetk)"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"make-a-recursive-forecast-model","dir":"Articles","previous_headings":"","what":"Make a Recursive Forecast Model","title":"Autoregressive Forecasting with Recursive","text":"’ll start simplest example, turning Linear Regresion Autoregressive model.","code":""},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"data-visualization","dir":"Articles","previous_headings":"","what":"Data Visualization","title":"Autoregressive Forecasting with Recursive","text":"Let’s start m750 dataset. can visualize data plot_time_series().","code":"m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows m750 %>%    plot_time_series(     .date_var    = date,      .value       = value,      .facet_var   = id,      .smooth      = F,      .interactive = F   )"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"Data Preparation","title":"Autoregressive Forecasting with Recursive","text":"Let’s establish forecast horizon extend dataset create forecast region.","code":"FORECAST_HORIZON <- 24  m750_extended <- m750 %>%     group_by(id) %>%     future_frame(         .length_out = FORECAST_HORIZON,         .bind_data  = TRUE     ) %>%     ungroup()"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"transform-function","dir":"Articles","previous_headings":"","what":"Transform Function","title":"Autoregressive Forecasting with Recursive","text":"’ll use short-term lags, lags size smaller forecast horizon. create custom function, lag_roll_transformer() takes dataset adds lags 1 12 rolling mean using lag 12. features function use lags less forecast horizon 24 months, means need use recursive().","code":"lag_roll_transformer <- function(data){     data %>%         tk_augment_lags(value, .lags = 1:FORECAST_HORIZON) %>%         tk_augment_slidify(           contains(\"lag12\"),           .f = ~mean(.x, na.rm = T),           .period  = 12,           .partial = TRUE         )  }"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"apply-the-transform-function","dir":"Articles","previous_headings":"","what":"Apply the Transform Function","title":"Autoregressive Forecasting with Recursive","text":"apply lag roll transformation extended data set, can see effect.","code":"m750_rolling <- m750_extended %>%     lag_roll_transformer() %>%     select(-id)  m750_rolling #> # A tibble: 330 × 27 #>    date       value value_lag1 value_lag2 value_lag3 value_lag4 value_lag5 #>    <date>     <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl> #>  1 1990-01-01  6370         NA         NA         NA         NA         NA #>  2 1990-02-01  6430       6370         NA         NA         NA         NA #>  3 1990-03-01  6520       6430       6370         NA         NA         NA #>  4 1990-04-01  6580       6520       6430       6370         NA         NA #>  5 1990-05-01  6620       6580       6520       6430       6370         NA #>  6 1990-06-01  6690       6620       6580       6520       6430       6370 #>  7 1990-07-01  6000       6690       6620       6580       6520       6430 #>  8 1990-08-01  5450       6000       6690       6620       6580       6520 #>  9 1990-09-01  6480       5450       6000       6690       6620       6580 #> 10 1990-10-01  6820       6480       5450       6000       6690       6620 #> # … with 320 more rows, and 20 more variables: value_lag6 <dbl>, #> #   value_lag7 <dbl>, value_lag8 <dbl>, value_lag9 <dbl>, value_lag10 <dbl>, #> #   value_lag11 <dbl>, value_lag12 <dbl>, value_lag13 <dbl>, value_lag14 <dbl>, #> #   value_lag15 <dbl>, value_lag16 <dbl>, value_lag17 <dbl>, value_lag18 <dbl>, #> #   value_lag19 <dbl>, value_lag20 <dbl>, value_lag21 <dbl>, value_lag22 <dbl>, #> #   value_lag23 <dbl>, value_lag24 <dbl>, value_lag12_roll_12 <dbl>"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"split-into-training-and-future-data","dir":"Articles","previous_headings":"","what":"Split into Training and Future Data","title":"Autoregressive Forecasting with Recursive","text":"training data needs completely filled . remove rows NA. future data missing values “value” column. isolate . autoregressive algorithm predict . Notice lags missing data, OK - going use recursive() fill missing values predictions.","code":"train_data <- m750_rolling %>%     drop_na()  train_data #> # A tibble: 282 × 27 #>    date       value value_lag1 value_lag2 value_lag3 value_lag4 value_lag5 #>    <date>     <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl> #>  1 1992-01-01  7030       7040       7000       6980       6550       5780 #>  2 1992-02-01  7170       7030       7040       7000       6980       6550 #>  3 1992-03-01  7150       7170       7030       7040       7000       6980 #>  4 1992-04-01  7180       7150       7170       7030       7040       7000 #>  5 1992-05-01  7140       7180       7150       7170       7030       7040 #>  6 1992-06-01  7100       7140       7180       7150       7170       7030 #>  7 1992-07-01  6490       7100       7140       7180       7150       7170 #>  8 1992-08-01  6060       6490       7100       7140       7180       7150 #>  9 1992-09-01  6870       6060       6490       7100       7140       7180 #> 10 1992-10-01  6880       6870       6060       6490       7100       7140 #> # … with 272 more rows, and 20 more variables: value_lag6 <dbl>, #> #   value_lag7 <dbl>, value_lag8 <dbl>, value_lag9 <dbl>, value_lag10 <dbl>, #> #   value_lag11 <dbl>, value_lag12 <dbl>, value_lag13 <dbl>, value_lag14 <dbl>, #> #   value_lag15 <dbl>, value_lag16 <dbl>, value_lag17 <dbl>, value_lag18 <dbl>, #> #   value_lag19 <dbl>, value_lag20 <dbl>, value_lag21 <dbl>, value_lag22 <dbl>, #> #   value_lag23 <dbl>, value_lag24 <dbl>, value_lag12_roll_12 <dbl> future_data <- m750_rolling %>%     filter(is.na(value))  future_data #> # A tibble: 24 × 27 #>    date       value value_lag1 value_lag2 value_lag3 value_lag4 value_lag5 #>    <date>     <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl> #>  1 2015-07-01    NA      11000      11310      11290      11250      11010 #>  2 2015-08-01    NA         NA      11000      11310      11290      11250 #>  3 2015-09-01    NA         NA         NA      11000      11310      11290 #>  4 2015-10-01    NA         NA         NA         NA      11000      11310 #>  5 2015-11-01    NA         NA         NA         NA         NA      11000 #>  6 2015-12-01    NA         NA         NA         NA         NA         NA #>  7 2016-01-01    NA         NA         NA         NA         NA         NA #>  8 2016-02-01    NA         NA         NA         NA         NA         NA #>  9 2016-03-01    NA         NA         NA         NA         NA         NA #> 10 2016-04-01    NA         NA         NA         NA         NA         NA #> # … with 14 more rows, and 20 more variables: value_lag6 <dbl>, #> #   value_lag7 <dbl>, value_lag8 <dbl>, value_lag9 <dbl>, value_lag10 <dbl>, #> #   value_lag11 <dbl>, value_lag12 <dbl>, value_lag13 <dbl>, value_lag14 <dbl>, #> #   value_lag15 <dbl>, value_lag16 <dbl>, value_lag17 <dbl>, value_lag18 <dbl>, #> #   value_lag19 <dbl>, value_lag20 <dbl>, value_lag21 <dbl>, value_lag22 <dbl>, #> #   value_lag23 <dbl>, value_lag24 <dbl>, value_lag12_roll_12 <dbl>"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"modeling","dir":"Articles","previous_headings":"","what":"Modeling","title":"Autoregressive Forecasting with Recursive","text":"’ll make 2 models comparison purposes: Straight-Line Forecast Model using Linear Regression Date feature Autoregressive Forecast Model using Linear Regression Date feature, Lags 1-12, Rolling Mean Lag 12","code":""},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"model-1-baseline-straight-line-forecast-model","dir":"Articles","previous_headings":"Modeling","what":"Model 1 (Baseline): Straight-Line Forecast Model","title":"Autoregressive Forecasting with Recursive","text":"straight-line forecast just illustrate effect autoregressive features. Consider NAIVE modeling approach. feature used dependent variable “date” column.","code":"model_fit_lm <- linear_reg() %>%     set_engine(\"lm\") %>%     fit(value ~ date, data = train_data)  model_fit_lm #> parsnip model object #>  #> Fit time:  3ms  #>  #> Call: #> stats::lm(formula = value ~ date, data = data) #>  #> Coefficients: #> (Intercept)         date   #>   3356.7208       0.4712"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"model-2-autoregressive-forecast-model","dir":"Articles","previous_headings":"Modeling","what":"Model 2: Autoregressive Forecast Model","title":"Autoregressive Forecasting with Recursive","text":"autoregressive forecast model simply parsnip model one additional step: using recursive(). key components : transform: transformation function. use function previously made generated Lags 1 12 Rolling Mean Lag 12 features. train_tail: tail training data, must large lags used transform function (.e. lag 12). Train tail can larger lag size used. Notice use Forecast Horizon, size 24. Panel Data, need include tail group. provided convenient panel_tail() function. id (Optional): used identify groups Recursive Panel Data.","code":"# Autoregressive Forecast model_fit_lm_recursive <- linear_reg() %>%     set_engine(\"lm\") %>%     fit(value ~ ., data = train_data) %>%     # One additional step - use recursive()     recursive(         transform  = lag_roll_transformer,         train_tail = tail(train_data, FORECAST_HORIZON)     )  model_fit_lm_recursive #> Recursive [parsnip model] #>  #> parsnip model object #>  #> Fit time:  4ms  #>  #> Call: #> stats::lm(formula = value ~ ., data = data) #>  #> Coefficients: #>         (Intercept)                 date           value_lag1   #>           164.14732              0.00677              0.61244   #>          value_lag2           value_lag3           value_lag4   #>             0.18402             -0.07128              0.12089   #>          value_lag5           value_lag6           value_lag7   #>            -0.01750              0.07095              0.09785   #>          value_lag8           value_lag9          value_lag10   #>            -0.08053              0.04887              0.03030   #>         value_lag11          value_lag12          value_lag13   #>            -0.01755              0.73318             -0.52958   #>         value_lag14          value_lag15          value_lag16   #>            -0.21410              0.07734             -0.13879   #>         value_lag17          value_lag18          value_lag19   #>             0.04351             -0.08894             -0.08732   #>         value_lag20          value_lag21          value_lag22   #>             0.06641             -0.05737             -0.02331   #>         value_lag23          value_lag24  value_lag12_roll_12   #>             0.05754              0.15960                   NA"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"modeltime-forecasting-workflow","dir":"Articles","previous_headings":"","what":"Modeltime Forecasting Workflow","title":"Autoregressive Forecasting with Recursive","text":"fitted model, can follow Modeltime Workflow (note skipping calibration refitting, can performed get confidence intervals): First, add fitted models Model Table using modeltime_table(). (Note - model description says “LM”, install development version modeltime, improved model descriptions recursive models). Next, perform Forecast Evaluation using modeltime_forecast() plot_modeltime_forecast().  can see benefit autoregressive features.","code":"model_tbl <- modeltime_table(     model_fit_lm,     model_fit_lm_recursive )   model_tbl #> # Modeltime Table #> # A tibble: 2 × 3 #>   .model_id .model   .model_desc  #>       <int> <list>   <chr>        #> 1         1 <fit[+]> LM           #> 2         2 <fit[+]> RECURSIVE LM model_tbl %>%         # Forecast using future data     modeltime_forecast(         new_data    = future_data,         actual_data = m750     ) %>%        # Visualize the forecast     plot_modeltime_forecast(         .interactive        = FALSE,         .conf_interval_show = FALSE     )"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"recursive-forecasting-with-panel-models","dir":"Articles","previous_headings":"","what":"Recursive Forecasting with Panel Models","title":"Autoregressive Forecasting with Recursive","text":"can take extending ’ve learned panel data: Panel Data: Grouped transformation functions: lag_roll_transformer_grouped() recursive(): Using id panel_tail() function sophisticated algorithms: Instead using simple Linear Regression use xgboost forecast multiple time series","code":""},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"data-visualization-1","dir":"Articles","previous_headings":"","what":"Data Visualization","title":"Autoregressive Forecasting with Recursive","text":"Now 4 time series forecast.","code":"m4_monthly %>%     plot_time_series(     .date_var    = date,      .value       = value,      .facet_var   = id,      .facet_ncol  = 2,     .smooth      = F,      .interactive = F )"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"data-preparation-1","dir":"Articles","previous_headings":"","what":"Data Preparation","title":"Autoregressive Forecasting with Recursive","text":"use timetk::future_frame() project series forward forecast horizon. sets extended data set series extended 24 time stamps.","code":"FORECAST_HORIZON <- 24  m4_extended <- m4_monthly %>%     group_by(id) %>%     future_frame(         .length_out = FORECAST_HORIZON,         .bind_data  = TRUE     ) %>%     ungroup()"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"transform-function-1","dir":"Articles","previous_headings":"","what":"Transform Function","title":"Autoregressive Forecasting with Recursive","text":"difference applying lags group.","code":"lag_roll_transformer_grouped <- function(data){     data %>%         group_by(id) %>%         tk_augment_lags(value, .lags = 1:FORECAST_HORIZON) %>%         tk_augment_slidify(           .value   = contains(\"lag12\"),           .f       = ~mean(.x, na.rm = T),           .period  = c(12),           .partial = TRUE         ) %>%         ungroup() }"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"apply-the-transform-function-1","dir":"Articles","previous_headings":"","what":"Apply the Transform Function","title":"Autoregressive Forecasting with Recursive","text":"apply groupwise lag transformation extended data set. adds autoregressive features.","code":"m4_lags <- m4_extended %>%     lag_roll_transformer_grouped()  m4_lags #> # A tibble: 1,670 × 28 #>    id    date       value value_lag1 value_lag2 value_lag3 value_lag4 value_lag5 #>    <fct> <date>     <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl> #>  1 M1    1976-06-01  8000         NA         NA         NA         NA         NA #>  2 M1    1976-07-01  8350       8000         NA         NA         NA         NA #>  3 M1    1976-08-01  8570       8350       8000         NA         NA         NA #>  4 M1    1976-09-01  7700       8570       8350       8000         NA         NA #>  5 M1    1976-10-01  7080       7700       8570       8350       8000         NA #>  6 M1    1976-11-01  6520       7080       7700       8570       8350       8000 #>  7 M1    1976-12-01  6070       6520       7080       7700       8570       8350 #>  8 M1    1977-01-01  6650       6070       6520       7080       7700       8570 #>  9 M1    1977-02-01  6830       6650       6070       6520       7080       7700 #> 10 M1    1977-03-01  5710       6830       6650       6070       6520       7080 #> # … with 1,660 more rows, and 20 more variables: value_lag6 <dbl>, #> #   value_lag7 <dbl>, value_lag8 <dbl>, value_lag9 <dbl>, value_lag10 <dbl>, #> #   value_lag11 <dbl>, value_lag12 <dbl>, value_lag13 <dbl>, value_lag14 <dbl>, #> #   value_lag15 <dbl>, value_lag16 <dbl>, value_lag17 <dbl>, value_lag18 <dbl>, #> #   value_lag19 <dbl>, value_lag20 <dbl>, value_lag21 <dbl>, value_lag22 <dbl>, #> #   value_lag23 <dbl>, value_lag24 <dbl>, value_lag12_roll_12 <dbl>"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"split-into-training-and-future-data-1","dir":"Articles","previous_headings":"","what":"Split into Training and Future Data","title":"Autoregressive Forecasting with Recursive","text":"Just like single case, split future training data.","code":"train_data <- m4_lags %>%     drop_na()  future_data <- m4_lags %>%     filter(is.na(value))"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"modeling-1","dir":"Articles","previous_headings":"","what":"Modeling","title":"Autoregressive Forecasting with Recursive","text":"’ll use sophisticated algorithm xgboost develop autoregressive model.","code":"# Modeling Autoregressive Panel Data set.seed(123) model_fit_xgb_recursive <- boost_tree(         mode = \"regression\",         learn_rate = 0.35     ) %>%     set_engine(\"xgboost\") %>%     fit(         value ~ .          + month(date, label = TRUE)          + as.numeric(date)          - date,          data = train_data     ) %>%     recursive(         id         = \"id\", # We add an id = \"id\" to specify the groups         transform  = lag_roll_transformer_grouped,         # We use panel_tail() to grab tail by groups         train_tail = panel_tail(train_data, id, FORECAST_HORIZON)     )  model_fit_xgb_recursive #> Recursive [parsnip model] #>  #> parsnip model object #>  #> Fit time:  116ms  #> ##### xgb.Booster #> raw: 69.1 Kb  #> call: #>   xgboost::xgb.train(params = list(eta = 0.35, max_depth = 6, gamma = 0,  #>     colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1,  #>     subsample = 1, objective = \"reg:squarederror\"), data = x$data,  #>     nrounds = 15, watchlist = x$watchlist, verbose = 0, nthread = 1) #> params (as set within xgb.train): #>   eta = \"0.35\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", objective = \"reg:squarederror\", nthread = \"1\", validate_parameters = \"TRUE\" #> xgb.attributes: #>   niter #> callbacks: #>   cb.evaluation.log() #> # of features: 41  #> niter: 15 #> nfeatures : 41  #> evaluation_log: #>     iter training_rmse #>        1     3767.4570 #>        2     2494.2036 #> ---                    #>       14      210.7798 #>       15      200.4421"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"modeltime-forecasting-workflow-1","dir":"Articles","previous_headings":"","what":"Modeltime Forecasting Workflow","title":"Autoregressive Forecasting with Recursive","text":"First, create Modeltime Table. Note - model description says “XGBOOST”, install development version modeltime, improved model descriptions recursive models). Next, can forecast results.","code":"model_tbl <- modeltime_table(     model_fit_xgb_recursive )  model_tbl #> # Modeltime Table #> # A tibble: 1 × 3 #>   .model_id .model   .model_desc       #>       <int> <list>   <chr>             #> 1         1 <fit[+]> RECURSIVE XGBOOST model_tbl %>%     modeltime_forecast(         new_data    = future_data,         actual_data = m4_monthly,         keep_data   = TRUE     ) %>%     group_by(id) %>%     plot_modeltime_forecast(         .interactive        = FALSE,         .conf_interval_show = FALSE,         .facet_ncol         = 2     )"},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Autoregressive Forecasting with Recursive","text":"just showcased Recursive Forecasting. simple problem. , ’s lot learning time series. Many algorithms Ensembling Machine Learning Deep Learning Scalable Modeling: 10,000+ time series probably thinking ever going learn time series forecasting. ’s solution save years struggling.","code":""},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"take-the-high-performance-forecasting-course","dir":"Articles","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"Autoregressive Forecasting with Recursive","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"time-series-is-changing","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"Autoregressive Forecasting with Recursive","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/modeltime/articles/recursive-forecasting.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"Articles","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"Autoregressive Forecasting with Recursive","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/modeltime/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Matt Dancho. Author, maintainer. Business Science. Copyright holder.","code":""},{"path":"https://business-science.github.io/modeltime/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Dancho M (2022). modeltime: Tidymodels Extension Time Series Modeling. https://github.com/business-science/modeltime, https://business-science.github.io/modeltime/.","code":"@Manual{,   title = {modeltime: The Tidymodels Extension for Time Series Modeling},   author = {Matt Dancho},   year = {2022},   note = {https://github.com/business-science/modeltime, https://business-science.github.io/modeltime/}, }"},{"path":"https://business-science.github.io/modeltime/index.html","id":"modeltime","dir":"","previous_headings":"","what":"The Tidymodels Extension for Time Series Modeling","title":"The Tidymodels Extension for Time Series Modeling","text":"Tidy time series forecasting tidymodels.","code":""},{"path":"https://business-science.github.io/modeltime/index.html","id":"quickstart-video","dir":"","previous_headings":"","what":"Quickstart Video","title":"The Tidymodels Extension for Time Series Modeling","text":"prefer video tutorials, 11-minute YouTube Video walks Modeltime Workflow.  (Click Watch YouTube)","code":""},{"path":"https://business-science.github.io/modeltime/index.html","id":"tutorials","dir":"","previous_headings":"","what":"Tutorials","title":"The Tidymodels Extension for Time Series Modeling","text":"Getting Started Modeltime: walkthrough 6-Step Process using modeltime forecast Modeltime Documentation: Learn use modeltime, find Modeltime Models, extend modeltime can use new algorithms inside Modeltime Workflow.","code":""},{"path":"https://business-science.github.io/modeltime/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"The Tidymodels Extension for Time Series Modeling","text":"CRAN version: Development version:","code":"install.packages(\"modeltime\", dependencies = TRUE) remotes::install_github(\"business-science/modeltime\", dependencies = TRUE)"},{"path":"https://business-science.github.io/modeltime/index.html","id":"why-modeltime","dir":"","previous_headings":"","what":"Why modeltime?","title":"The Tidymodels Extension for Time Series Modeling","text":"Modeltime unlocks time series models machine learning one framework  need switch back forth various frameworks. modeltime unlocks machine learning & classical time series analysis. forecast: Use ARIMA, ETS, models coming (arima_reg(), arima_boost(), & exp_smoothing()). prophet: Use Facebook’s Prophet algorithm (prophet_reg() & prophet_boost()) tidymodels: Use parsnip model: rand_forest(), boost_tree(), linear_reg(), mars(), svm_rbf() forecast","code":""},{"path":"https://business-science.github.io/modeltime/index.html","id":"forecast-faster","dir":"","previous_headings":"","what":"Forecast faster","title":"The Tidymodels Extension for Time Series Modeling","text":"streamlined workflow forecasting Modeltime incorporates streamlined workflow (see Getting Started Modeltime) using best practices forecast. streamlined workflow forecasting","code":""},{"path":"https://business-science.github.io/modeltime/index.html","id":"meet-the-modeltime-ecosystem","dir":"","previous_headings":"","what":"Meet the modeltime ecosystem","title":"The Tidymodels Extension for Time Series Modeling","text":"Learn growing ecosystem forecasting packages modeltime ecosystem growing Modeltime part growing ecosystem Modeltime forecasting packages. Modeltime (Machine Learning) Modeltime H2O (AutoML) Modeltime GluonTS (Deep Learning) Modeltime Ensemble (Blending Forecasts) Modeltime Resample (Backtesting) Timetk (Feature Engineering, Data Wrangling, Time Series Visualization)","code":""},{"path":"https://business-science.github.io/modeltime/index.html","id":"summary","dir":"","previous_headings":"","what":"Summary","title":"The Tidymodels Extension for Time Series Modeling","text":"Modeltime amazing ecosystem time series forecasting. can take long time learn: Many algorithms Ensembling Resampling Machine Learning Deep Learning Scalable Modeling: 10,000+ time series probably thinking ever going learn time series forecasting. ’s solution save years struggling.","code":""},{"path":"https://business-science.github.io/modeltime/index.html","id":"take-the-high-performance-forecasting-course","dir":"","previous_headings":"","what":"Take the High-Performance Forecasting Course","title":"The Tidymodels Extension for Time Series Modeling","text":"Become forecasting expert organization  High-Performance Time Series Course","code":""},{"path":"https://business-science.github.io/modeltime/index.html","id":"time-series-is-changing","dir":"","previous_headings":"Take the High-Performance Forecasting Course","what":"Time Series is Changing","title":"The Tidymodels Extension for Time Series Modeling","text":"Time series changing. Businesses now need 10,000+ time series forecasts every day. call High-Performance Time Series Forecasting System (HPTSF) - Accurate, Robust, Scalable Forecasting. High-Performance Forecasting Systems save companies improving accuracy scalability. Imagine happen career can provide organization “High-Performance Time Series Forecasting System” (HPTSF System).","code":""},{"path":"https://business-science.github.io/modeltime/index.html","id":"how-to-learn-high-performance-time-series-forecasting","dir":"","previous_headings":"Take the High-Performance Forecasting Course","what":"How to Learn High-Performance Time Series Forecasting","title":"The Tidymodels Extension for Time Series Modeling","text":"teach build HPTFS System High-Performance Time Series Forecasting Course. learn: Time Series Machine Learning (cutting-edge) Modeltime - 30+ Models (Prophet, ARIMA, XGBoost, Random Forest, & many ) Deep Learning GluonTS (Competition Winners) Time Series Preprocessing, Noise Reduction, & Anomaly Detection Feature engineering using lagged variables & external regressors Hyperparameter Tuning Time series cross-validation Ensembling Multiple Machine Learning & Univariate Modeling Techniques (Competition Winner) Scalable Forecasting - Forecast 1000+ time series parallel . Become Time Series Expert organization. Take High-Performance Time Series Forecasting Course","code":""},{"path":"https://business-science.github.io/modeltime/reference/Adam_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for ADAM models — Adam_predict_impl","title":"Bridge prediction function for ADAM models — Adam_predict_impl","text":"Bridge prediction function ADAM models","code":""},{"path":"https://business-science.github.io/modeltime/reference/Adam_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for ADAM models — Adam_predict_impl","text":"","code":"Adam_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/Adam_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for ADAM models — Adam_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed smooth::adam()","code":""},{"path":"https://business-science.github.io/modeltime/reference/Arima_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level ARIMA function for translating modeltime to forecast — Arima_fit_impl","title":"Low-Level ARIMA function for translating modeltime to forecast — Arima_fit_impl","text":"Low-Level ARIMA function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/Arima_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level ARIMA function for translating modeltime to forecast — Arima_fit_impl","text":"","code":"Arima_fit_impl(   x,   y,   period = \"auto\",   p = 0,   d = 0,   q = 0,   P = 0,   D = 0,   Q = 0,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/Arima_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level ARIMA function for translating modeltime to forecast — Arima_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. p order non-seasonal auto-regressive (AR) terms. Often denoted \"p\" pdq-notation. d order integration non-seasonal differencing. Often denoted \"d\" pdq-notation. q order non-seasonal moving average (MA) terms. Often denoted \"q\" pdq-notation. P order seasonal auto-regressive (SAR) terms. Often denoted \"P\" PDQ-notation. D order integration seasonal differencing. Often denoted \"D\" PDQ-notation. Q order seasonal moving average (SMA) terms. Often denoted \"Q\" PDQ-notation. ... Additional arguments passed forecast::Arima","code":""},{"path":"https://business-science.github.io/modeltime/reference/Arima_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for ARIMA models — Arima_predict_impl","title":"Bridge prediction function for ARIMA models — Arima_predict_impl","text":"Bridge prediction function ARIMA models","code":""},{"path":"https://business-science.github.io/modeltime/reference/Arima_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for ARIMA models — Arima_predict_impl","text":"","code":"Arima_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/Arima_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for ARIMA models — Arima_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed forecast::Arima()","code":""},{"path":"https://business-science.github.io/modeltime/reference/Auto_adam_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for AUTO ADAM models — Auto_adam_predict_impl","title":"Bridge prediction function for AUTO ADAM models — Auto_adam_predict_impl","text":"Bridge prediction function AUTO ADAM models","code":""},{"path":"https://business-science.github.io/modeltime/reference/Auto_adam_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for AUTO ADAM models — Auto_adam_predict_impl","text":"","code":"Auto_adam_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/Auto_adam_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for AUTO ADAM models — Auto_adam_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed smooth::auto.adam()","code":""},{"path":"https://business-science.github.io/modeltime/reference/adam_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level ADAM function for translating modeltime to forecast — adam_fit_impl","title":"Low-Level ADAM function for translating modeltime to forecast — adam_fit_impl","text":"Low-Level ADAM function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/adam_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level ADAM function for translating modeltime to forecast — adam_fit_impl","text":"","code":"adam_fit_impl(   x,   y,   period = \"auto\",   p = 0,   d = 0,   q = 0,   P = 0,   D = 0,   Q = 0,   model = \"ZXZ\",   constant = FALSE,   regressors = c(\"use\", \"select\", \"adapt\"),   outliers = c(\"ignore\", \"use\", \"select\"),   level = 0.99,   occurrence = c(\"none\", \"auto\", \"fixed\", \"general\", \"odds-ratio\",     \"inverse-odds-ratio\", \"direct\"),   distribution = c(\"default\", \"dnorm\", \"dlaplace\", \"ds\", \"dgnorm\", \"dlnorm\",     \"dinvgauss\", \"dgamma\"),   loss = c(\"likelihood\", \"MSE\", \"MAE\", \"HAM\", \"LASSO\", \"RIDGE\", \"MSEh\", \"TMSE\",     \"GTMSE\", \"MSCE\"),   ic = c(\"AICc\", \"AIC\", \"BIC\", \"BICc\"),   select_order = FALSE,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/adam_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level ADAM function for translating modeltime to forecast — adam_fit_impl","text":"x data.frame predictors y vector outcome period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. p order non-seasonal auto-regressive (AR) terms. Often denoted \"p\" pdq-notation. d order integration non-seasonal differencing. Often denoted \"d\" pdq-notation. q order non-seasonal moving average (MA) terms. Often denoted \"q\" pdq-notation. P order seasonal auto-regressive (SAR) terms. Often denoted \"P\" PDQ-notation. D order integration seasonal differencing. Often denoted \"D\" PDQ-notation. Q order seasonal moving average (SMA) terms. Often denoted \"Q\" PDQ-notation. model type ETS model. constant Logical, determining, whether constant needed model . regressors variable defines provided explanatory variables. outliers Defines outliers. level confidence level use detection outliers. occurrence type model used probability estimation. distribution density function assume error term. loss type Loss Function used optimization. ic information criterion use model selection / combination procedure. select_order TRUE, function select appropriate order using mechanism similar auto.msarima(), implemented auto.adam(). values list(ar=...,=...,ma=...) specify maximum orders check case ... Additional arguments passed smooth::adam","code":""},{"path":"https://business-science.github.io/modeltime/reference/adam_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuning Parameters for ADAM Models — adam_params","title":"Tuning Parameters for ADAM Models — adam_params","text":"Tuning Parameters ADAM Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/adam_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning Parameters for ADAM Models — adam_params","text":"","code":"use_constant(values = c(FALSE, TRUE))  regressors_treatment(values = c(\"use\", \"select\", \"adapt\"))  outliers_treatment(values = c(\"ignore\", \"use\", \"select\"))  probability_model(   values = c(\"none\", \"auto\", \"fixed\", \"general\", \"odds-ratio\", \"inverse-odds-ratio\",     \"direct\") )  distribution(   values = c(\"default\", \"dnorm\", \"dlaplace\", \"ds\", \"dgnorm\", \"dlnorm\", \"dinvgauss\",     \"dgamma\") )  information_criteria(values = c(\"AICc\", \"AIC\", \"BICc\", \"BIC\"))  select_order(values = c(FALSE, TRUE))"},{"path":"https://business-science.github.io/modeltime/reference/adam_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuning Parameters for ADAM Models — adam_params","text":"values character string possible values.","code":""},{"path":"https://business-science.github.io/modeltime/reference/adam_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tuning Parameters for ADAM Models — adam_params","text":"dials parameter parameter parameter parameter parameter parameter parameter parameter","code":""},{"path":"https://business-science.github.io/modeltime/reference/adam_params.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tuning Parameters for ADAM Models — adam_params","text":"main parameters ADAM models : non_seasonal_ar: order non-seasonal auto-regressive (AR) terms. non_seasonal_differences: order integration non-seasonal differencing. non_seasonal_ma: order non-seasonal moving average (MA) terms. seasonal_ar: order seasonal auto-regressive (SAR) terms. seasonal_differences: order integration seasonal differencing. seasonal_ma: order seasonal moving average (SMA) terms. use_constant: Logical, determining, whether constant needed model . regressors_treatment: variable defines provided explanatory variables. outliers_treatment: Defines outliers. probability_model: type model used probability estimation. distribution: density function assume error term. information_criteria: information criterion use model selection / combination procedure. select_order: TRUE, function select appropriate order.","code":""},{"path":"https://business-science.github.io/modeltime/reference/adam_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tuning Parameters for ADAM Models — adam_params","text":"","code":"use_constant() #> Logical, determining, whether the constant is needed in the model or not  (qualitative) #> 2 possible value include: #> FALSE and TRUE   regressors_treatment() #> The variable defines what to do with the provided explanatory variables.  (qualitative) #> 3 possible value include: #> 'use', 'select' and 'adapt'   distribution() #> What density function to assume for the error term.  (qualitative) #> 8 possible value include: #> 'default', 'dnorm', 'dlaplace', 'ds', 'dgnorm', 'dlnorm', 'dinvgauss' and 'dg..."},{"path":"https://business-science.github.io/modeltime/reference/adam_reg.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for ADAM Regression Models — adam_reg","title":"General Interface for ADAM Regression Models — adam_reg","text":"adam_reg() way generate specification ADAM model fitting allows model created using different packages. Currently package smooth.","code":""},{"path":"https://business-science.github.io/modeltime/reference/adam_reg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for ADAM Regression Models — adam_reg","text":"","code":"adam_reg(   mode = \"regression\",   ets_model = NULL,   non_seasonal_ar = NULL,   non_seasonal_differences = NULL,   non_seasonal_ma = NULL,   seasonal_ar = NULL,   seasonal_differences = NULL,   seasonal_ma = NULL,   use_constant = NULL,   regressors_treatment = NULL,   outliers_treatment = NULL,   outliers_ci = NULL,   probability_model = NULL,   distribution = NULL,   loss = NULL,   information_criteria = NULL,   seasonal_period = NULL,   select_order = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/adam_reg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for ADAM Regression Models — adam_reg","text":"mode single character string type model. possible value model \"regression\". ets_model type ETS model. first letter stands type error term (\"\" \"M\"), second (sometimes third well) trend (\"N\", \"\", \"Ad\", \"M\" \"Md\"), last one type seasonality (\"N\", \"\" \"M\"). non_seasonal_ar order non-seasonal auto-regressive (AR) terms. Often denoted \"p\" pdq-notation. non_seasonal_differences order integration non-seasonal differencing. Often denoted \"d\" pdq-notation. non_seasonal_ma order non-seasonal moving average (MA) terms. Often denoted \"q\" pdq-notation. seasonal_ar order seasonal auto-regressive (SAR) terms. Often denoted \"P\" PDQ-notation. seasonal_differences order integration seasonal differencing. Often denoted \"D\" PDQ-notation. seasonal_ma order seasonal moving average (SMA) terms. Often denoted \"Q\" PDQ-notation. use_constant Logical, determining, whether constant needed model . mainly needed ARIMA part model, can used ETS well. regressors_treatment variable defines provided explanatory variables: \"use\" means data used, \"select\" means selection using ic done, \"adapt\" trigger mechanism time varying parameters explanatory variables. outliers_treatment Defines outliers: \"ignore\", just returning model, \"detect\" outliers based specified level include dummies model, detect \"select\" reduce ic value. outliers_ci confidence level use detection outliers. Default 99%. probability_model type model used probability estimation. Can \"none\" - none, \"fixed\" - constant probability, \"general\" - general Beta model two parameters, \"odds-ratio\" - Odds-ratio model b=1 Beta distribution, \"inverse-odds-ratio\" - model =1 Beta distribution, \"direct\" - TSB-like (Teunter et al., 2011) probability update mechanism +b=1, \"auto\" - automatically selected type occurrence model. distribution density function assume error term. full name distribution provided, starting letter \"d\" - \"density\". loss type Loss Function used optimization. information_criteria information criterion use model selection / combination procedure. seasonal_period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details . select_order TRUE, function select appropriate order. values list(ar=...,=...,ma=...) specify maximum orders check case.","code":""},{"path":"https://business-science.github.io/modeltime/reference/adam_reg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for ADAM Regression Models — adam_reg","text":"data given function saved used determine mode model. adam_reg(), mode always \"regression\". model can created using fit() function using following engines: \"auto_adam\" (default) - Connects smooth::auto.adam() \"adam\" - Connects smooth::adam() Main Arguments main arguments (tuning parameters) model : seasonal_period: periodic nature seasonality. Uses \"auto\" default. non_seasonal_ar: order non-seasonal auto-regressive (AR) terms. non_seasonal_differences: order integration non-seasonal differencing. non_seasonal_ma: order non-seasonal moving average (MA) terms. seasonal_ar: order seasonal auto-regressive (SAR) terms. seasonal_differences: order integration seasonal differencing. seasonal_ma: order seasonal moving average (SMA) terms. ets_model: type ETS model. use_constant: Logical, determining, whether constant needed model . regressors_treatment: variable defines provided explanatory variables. outliers_treatment: Defines outliers. probability_model: type model used probability estimation. distribution: density function assume error term. loss: type Loss Function used optimization. information_criteria: information criterion use model selection / combination procedure. arguments converted specific names time model fit. options argument can set using set_engine() (See Engine Details ). parameters need modified, update() can used lieu recreating object scratch. auto_adam (default engine) engine uses smooth::auto.adam(). Function Parameters: MAXIMUM nonseasonal ARIMA terms (max.p, max.d, max.q) seasonal ARIMA terms (max.P, max.D, max.Q) provided forecast::auto.arima() via arima_reg() parameters. options argument can set using set_engine(). Parameter Notes: values nonseasonal pdq seasonal PDQ maximums. smooth::auto.adam() model select value using upper limit. xreg - supplied via parsnip / modeltime fit() interface (provide manually). See Fit Details (). adam engine uses smooth::adam(). Function Parameters: nonseasonal ARIMA terms (orders) seasonal ARIMA terms (orders) provided smooth::adam() via adam_reg() parameters. options argument can set using set_engine(). Parameter Notes: xreg - supplied via parsnip / modeltime fit() interface (provide manually). See Fit Details ().","code":"## Registered S3 method overwritten by 'greybox': ##   method     from ##   print.pcor lava ## function (data, model = \"ZXZ\", lags = c(frequency(data)), orders = list(ar = c(0),  ##     i = c(0), ma = c(0), select = FALSE), formula = NULL, outliers = c(\"ignore\",  ##     \"use\", \"select\"), level = 0.99, distribution = c(\"dnorm\", \"dlaplace\",  ##     \"ds\", \"dgnorm\", \"dlnorm\", \"dinvgauss\", \"dgamma\"), h = 0, holdout = FALSE,  ##     persistence = NULL, phi = NULL, initial = c(\"optimal\", \"backcasting\"),  ##     arma = NULL, occurrence = c(\"none\", \"auto\", \"fixed\", \"general\", \"odds-ratio\",  ##         \"inverse-odds-ratio\", \"direct\"), ic = c(\"AICc\", \"AIC\", \"BIC\", \"BICc\"),  ##     bounds = c(\"usual\", \"admissible\", \"none\"), regressors = c(\"use\", \"select\",  ##         \"adapt\"), silent = TRUE, parallel = FALSE, ...) ## function (data, model = \"ZXZ\", lags = c(frequency(data)), orders = list(ar = c(0),  ##     i = c(0), ma = c(0), select = FALSE), constant = FALSE, formula = NULL,  ##     regressors = c(\"use\", \"select\", \"adapt\"), outliers = c(\"ignore\", \"use\",  ##         \"select\"), level = 0.99, occurrence = c(\"none\", \"auto\", \"fixed\",  ##         \"general\", \"odds-ratio\", \"inverse-odds-ratio\", \"direct\"), distribution = c(\"default\",  ##         \"dnorm\", \"dlaplace\", \"ds\", \"dgnorm\", \"dlnorm\", \"dinvgauss\", \"dgamma\"),  ##     loss = c(\"likelihood\", \"MSE\", \"MAE\", \"HAM\", \"LASSO\", \"RIDGE\", \"MSEh\",  ##         \"TMSE\", \"GTMSE\", \"MSCE\"), h = 0, holdout = FALSE, persistence = NULL,  ##     phi = NULL, initial = c(\"optimal\", \"backcasting\"), arma = NULL, ic = c(\"AICc\",  ##         \"AIC\", \"BIC\", \"BICc\"), bounds = c(\"usual\", \"admissible\", \"none\"),  ##     silent = TRUE, ...)"},{"path":"https://business-science.github.io/modeltime/reference/adam_reg.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for ADAM Regression Models — adam_reg","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) Seasonal Period Specification period can non-seasonal (seasonal_period = 1 \"none\") yearly seasonal (e.g. monthly time stamps, seasonal_period = 12, seasonal_period = \"12 months\", seasonal_period = \"yearly\"). 3 ways specify: seasonal_period = \"auto\": seasonal period selected based periodicity data (e.g. 12 monthly) seasonal_period = 12: numeric frequency. example, 12 common monthly data seasonal_period = \"1 year\": time-based phrase. example, \"1 year\" convert 12 monthly data. Univariate (xregs, Exogenous Regressors): univariate analysis, must include date date-time feature. Simply use: Formula Interface (recommended): fit(y ~ date) ignore xreg's. Multivariate (xregs, Exogenous Regressors) xreg parameter populated using fit() function: factor, ordered factor, numeric data used xregs. Date Date-time variables used xregs character data converted factor. Xreg Example: Suppose 3 features: y (target) date (time stamp), month.lbl (labeled month ordered factor). month.lbl exogenous regressor can passed arima_reg() using fit(): fit(y ~ date + month.lbl) pass month.lbl exogenous regressor. Note date date-time class values excluded xreg.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/adam_reg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for ADAM Regression Models — adam_reg","text":"","code":"if (FALSE) { library(dplyr) library(parsnip) library(rsample) library(timetk) library(modeltime) library(smooth)  # Data m750 <- m4_monthly %>% filter(id == \"M750\") m750  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # ---- AUTO ADAM ----  # Model Spec model_spec <- adam_reg() %>%     set_engine(\"auto_adam\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) model_fit   # ---- STANDARD ADAM ----  # Model Spec model_spec <- adam_reg(         seasonal_period          = 12,         non_seasonal_ar          = 3,         non_seasonal_differences = 1,         non_seasonal_ma          = 3,         seasonal_ar              = 1,         seasonal_differences     = 0,         seasonal_ma              = 1     ) %>%     set_engine(\"adam\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) model_fit }"},{"path":"https://business-science.github.io/modeltime/reference/add_modeltime_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Add a Model into a Modeltime Table — add_modeltime_model","title":"Add a Model into a Modeltime Table — add_modeltime_model","text":"Add Model Modeltime Table","code":""},{"path":"https://business-science.github.io/modeltime/reference/add_modeltime_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add a Model into a Modeltime Table — add_modeltime_model","text":"","code":"add_modeltime_model(object, model, location = \"bottom\")"},{"path":"https://business-science.github.io/modeltime/reference/add_modeltime_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add a Model into a Modeltime Table — add_modeltime_model","text":"object Multiple Modeltime Tables (class mdl_time_tbl) model model class model_fit fitted workflow object location add model. Either \"top\" \"bottom\". Default: \"bottom\".","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/add_modeltime_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add a Model into a Modeltime Table — add_modeltime_model","text":"","code":"# \\donttest{ library(tidymodels) #> ── Attaching packages ────────────────────────────────────── tidymodels 0.1.4 ── #> ✔ broom        0.7.10     ✔ recipes      0.1.17 #> ✔ dials        0.0.10     ✔ rsample      0.1.1  #> ✔ dplyr        1.0.7      ✔ tibble       3.1.6  #> ✔ ggplot2      3.3.5      ✔ tidyr        1.1.4  #> ✔ infer        1.0.0      ✔ tune         0.1.6  #> ✔ modeldata    0.1.1      ✔ workflows    0.2.4  #> ✔ parsnip      0.1.7      ✔ workflowsets 0.1.0  #> ✔ purrr        0.3.4      ✔ yardstick    0.0.9  #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() #> • Dig deeper into tidy modeling with R at https://www.tmwr.org  model_fit_ets <- exp_smoothing() %>%     set_engine(\"ets\") %>%     fit(value ~ date, training(m750_splits)) #> frequency = 12 observations per 1 year  m750_models %>%     add_modeltime_model(model_fit_ets) #> # Modeltime Table #> # A tibble: 4 × 3 #>   .model_id .model     .model_desc             #>       <int> <list>     <chr>                   #> 1         1 <workflow> ARIMA(0,1,1)(0,1,1)[12] #> 2         2 <workflow> PROPHET                 #> 3         3 <workflow> GLMNET                  #> 4         4 <fit[+]>   ETS(A,A,A)              # }"},{"path":"https://business-science.github.io/modeltime/reference/arima_boost.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for ","title":"General Interface for ","text":"arima_boost() way generate specification time series model uses boosting improve modeling errors (residuals) Exogenous Regressors. works \"automated\" ARIMA (auto.arima) standard ARIMA (arima). main algorithms : Auto ARIMA + XGBoost Errors (engine = auto_arima_xgboost, default) ARIMA + XGBoost Errors (engine = arima_xgboost)","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_boost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for ","text":"","code":"arima_boost(   mode = \"regression\",   seasonal_period = NULL,   non_seasonal_ar = NULL,   non_seasonal_differences = NULL,   non_seasonal_ma = NULL,   seasonal_ar = NULL,   seasonal_differences = NULL,   seasonal_ma = NULL,   mtry = NULL,   trees = NULL,   min_n = NULL,   tree_depth = NULL,   learn_rate = NULL,   loss_reduction = NULL,   sample_size = NULL,   stop_iter = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/arima_boost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for ","text":"mode single character string type model. possible value model \"regression\". seasonal_period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details . non_seasonal_ar order non-seasonal auto-regressive (AR) terms. Often denoted \"p\" pdq-notation. non_seasonal_differences order integration non-seasonal differencing. Often denoted \"d\" pdq-notation. non_seasonal_ma order non-seasonal moving average (MA) terms. Often denoted \"q\" pdq-notation. seasonal_ar order seasonal auto-regressive (SAR) terms. Often denoted \"P\" PDQ-notation. seasonal_differences order integration seasonal differencing. Often denoted \"D\" PDQ-notation. seasonal_ma order seasonal moving average (SMA) terms. Often denoted \"Q\" PDQ-notation. mtry number number (proportion) predictors randomly sampled split creating tree models (specific engines ) trees integer number trees contained ensemble. min_n integer minimum number data points node required node split . tree_depth integer maximum depth tree (.e. number splits) (specific engines ). learn_rate number rate boosting algorithm adapts iteration--iteration (specific engines ). loss_reduction number reduction loss function required split (specific engines ). sample_size number number (proportion) data exposed fitting routine. stop_iter number iterations without improvement stopping  (xgboost ).","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_boost.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for ","text":"data given function saved used determine mode model. arima_boost(), mode always \"regression\". model can created using fit() function using following engines: \"auto_arima_xgboost\" (default) - Connects forecast::auto.arima() xgboost::xgb.train \"arima_xgboost\" - Connects forecast::Arima() xgboost::xgb.train Main Arguments main arguments (tuning parameters) ARIMA model : seasonal_period: periodic nature seasonality. Uses \"auto\" default. non_seasonal_ar: order non-seasonal auto-regressive (AR) terms. non_seasonal_differences: order integration non-seasonal differencing. non_seasonal_ma: order non-seasonal moving average (MA) terms. seasonal_ar: order seasonal auto-regressive (SAR) terms. seasonal_differences: order integration seasonal differencing. seasonal_ma: order seasonal moving average (SMA) terms. main arguments (tuning parameters) model XGBoost model : mtry: number predictors randomly sampled split creating tree models. trees: number trees contained ensemble. min_n: minimum number data points node required node split . tree_depth: maximum depth tree (.e. number splits). learn_rate: rate boosting algorithm adapts iteration--iteration. loss_reduction: reduction loss function required split . sample_size: amount data exposed fitting routine. stop_iter: number iterations without improvement stopping. arguments converted specific names time model fit. options argument can set using set_engine() (See Engine Details ). parameters need modified, update() can used lieu recreating object scratch.","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_boost.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for ","text":"standardized parameter names modeltime can mapped original names engine: Model 1: ARIMA: Model 2: XGBoost: options can set using set_engine(). auto_arima_xgboost (default engine) Model 1: Auto ARIMA (forecast::auto.arima): Parameter Notes: values nonseasonal pdq seasonal PDQ maximums. auto.arima select value using upper limit. xreg - used since XGBoost regression Model 2: XGBoost (xgboost::xgb.train): Parameter Notes: XGBoost uses params = list() capture. Parsnip / Modeltime automatically sends args provided ... inside set_engine() params = list(...).","code":"## function (y, d = NA, D = NA, max.p = 5, max.q = 5, max.P = 2, max.Q = 2,  ##     max.order = 5, max.d = 2, max.D = 1, start.p = 2, start.q = 2, start.P = 1,  ##     start.Q = 1, stationary = FALSE, seasonal = TRUE, ic = c(\"aicc\", \"aic\",  ##         \"bic\"), stepwise = TRUE, nmodels = 94, trace = FALSE, approximation = (length(x) >  ##         150 | frequency(x) > 12), method = NULL, truncate = NULL, xreg = NULL,  ##     test = c(\"kpss\", \"adf\", \"pp\"), test.args = list(), seasonal.test = c(\"seas\",  ##         \"ocsb\", \"hegy\", \"ch\"), seasonal.test.args = list(), allowdrift = TRUE,  ##     allowmean = TRUE, lambda = NULL, biasadj = FALSE, parallel = FALSE,  ##     num.cores = 2, x = y, ...) ## function (params = list(), data, nrounds, watchlist = list(), obj = NULL,  ##     feval = NULL, verbose = 1, print_every_n = 1L, early_stopping_rounds = NULL,  ##     maximize = NULL, save_period = NULL, save_name = \"xgboost.model\", xgb_model = NULL,  ##     callbacks = list(), ...)"},{"path":"https://business-science.github.io/modeltime/reference/arima_boost.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for ","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) Seasonal Period Specification period can non-seasonal (seasonal_period = 1) seasonal (e.g. seasonal_period = 12 seasonal_period = \"12 months\"). 3 ways specify: seasonal_period = \"auto\": period selected based periodicity data (e.g. 12 monthly) seasonal_period = 12: numeric frequency. example, 12 common monthly data seasonal_period = \"1 year\": time-based phrase. example, \"1 year\" convert 12 monthly data. Univariate (xregs, Exogenous Regressors): univariate analysis, must include date date-time feature. Simply use: Formula Interface (recommended): fit(y ~ date) ignore xreg's. XY Interface: fit_xy(x = data[,\"date\"], y = data$y) ignore xreg's. Multivariate (xregs, Exogenous Regressors) xreg parameter populated using fit() fit_xy() function: factor, ordered factor, numeric data used xregs. Date Date-time variables used xregs character data converted factor. Xreg Example: Suppose 3 features: y (target) date (time stamp), month.lbl (labeled month ordered factor). month.lbl exogenous regressor can passed arima_boost() using fit(): fit(y ~ date + month.lbl) pass month.lbl exogenous regressor. fit_xy(data[,c(\"date\", \"month.lbl\")], y = data$y) pass x, x data frame containing month.lbl date feature. month.lbl used exogenous regressor. Note date date-time class values excluded xreg.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/arima_boost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for ","text":"","code":"library(tidyverse) #> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── #> ✔ readr   2.1.1     ✔ forcats 0.5.1 #> ✔ stringr 1.4.0      #> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #> ✖ readr::col_factor() masks scales::col_factor() #> ✖ purrr::discard()    masks scales::discard() #> ✖ dplyr::filter()     masks stats::filter() #> ✖ stringr::fixed()    masks recipes::fixed() #> ✖ dplyr::lag()        masks stats::lag() #> ✖ readr::spec()       masks yardstick::spec() library(lubridate) #>  #> Attaching package: ‘lubridate’ #> The following objects are masked from ‘package:base’: #>  #>     date, intersect, setdiff, union library(parsnip) library(rsample) library(timetk) library(modeltime)   # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)  # MODEL SPEC ----  # Set engine and boosting parameters model_spec <- arima_boost(      # ARIMA args     seasonal_period = 12,     non_seasonal_ar = 0,     non_seasonal_differences = 1,     non_seasonal_ma = 1,     seasonal_ar     = 0,     seasonal_differences = 1,     seasonal_ma     = 1,      # XGBoost Args     tree_depth = 6,     learn_rate = 0.1 ) %>%     set_engine(engine = \"arima_xgboost\")  # FIT ----  if (FALSE) { # Boosting - Happens by adding numeric date and month features model_fit_boosted <- model_spec %>%     fit(value ~ date + as.numeric(date) + month(date, label = TRUE),         data = training(splits))  model_fit_boosted }"},{"path":"https://business-science.github.io/modeltime/reference/arima_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuning Parameters for ARIMA Models — arima_params","title":"Tuning Parameters for ARIMA Models — arima_params","text":"Tuning Parameters ARIMA Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning Parameters for ARIMA Models — arima_params","text":"","code":"non_seasonal_ar(range = c(0L, 5L), trans = NULL)  non_seasonal_differences(range = c(0L, 2L), trans = NULL)  non_seasonal_ma(range = c(0L, 5L), trans = NULL)  seasonal_ar(range = c(0L, 2L), trans = NULL)  seasonal_differences(range = c(0L, 1L), trans = NULL)  seasonal_ma(range = c(0L, 2L), trans = NULL)"},{"path":"https://business-science.github.io/modeltime/reference/arima_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuning Parameters for ARIMA Models — arima_params","text":"range two-element vector holding defaults smallest largest possible values, respectively. trans trans object scales package, scales::log10_trans() scales::reciprocal_trans(). provided, default used matches units used range. transformation, NULL.","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_params.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tuning Parameters for ARIMA Models — arima_params","text":"main parameters ARIMA models : non_seasonal_ar: order non-seasonal auto-regressive (AR) terms. non_seasonal_differences: order integration non-seasonal differencing. non_seasonal_ma: order non-seasonal moving average (MA) terms. seasonal_ar: order seasonal auto-regressive (SAR) terms. seasonal_differences: order integration seasonal differencing. seasonal_ma: order seasonal moving average (SMA) terms.","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tuning Parameters for ARIMA Models — arima_params","text":"","code":"non_seasonal_ar() #> Non-seasonal AR Term (quantitative) #> Range: [0, 5]  non_seasonal_differences() #> Non-seasonal Differencing Term (quantitative) #> Range: [0, 2]  non_seasonal_ma() #> Non-seasonal MA Term (quantitative) #> Range: [0, 5]"},{"path":"https://business-science.github.io/modeltime/reference/arima_reg.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for ARIMA Regression Models — arima_reg","title":"General Interface for ARIMA Regression Models — arima_reg","text":"arima_reg() way generate specification ARIMA model fitting allows model created using different packages. Currently package forecast.","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_reg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for ARIMA Regression Models — arima_reg","text":"","code":"arima_reg(   mode = \"regression\",   seasonal_period = NULL,   non_seasonal_ar = NULL,   non_seasonal_differences = NULL,   non_seasonal_ma = NULL,   seasonal_ar = NULL,   seasonal_differences = NULL,   seasonal_ma = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/arima_reg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for ARIMA Regression Models — arima_reg","text":"mode single character string type model. possible value model \"regression\". seasonal_period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details . non_seasonal_ar order non-seasonal auto-regressive (AR) terms. Often denoted \"p\" pdq-notation. non_seasonal_differences order integration non-seasonal differencing. Often denoted \"d\" pdq-notation. non_seasonal_ma order non-seasonal moving average (MA) terms. Often denoted \"q\" pdq-notation. seasonal_ar order seasonal auto-regressive (SAR) terms. Often denoted \"P\" PDQ-notation. seasonal_differences order integration seasonal differencing. Often denoted \"D\" PDQ-notation. seasonal_ma order seasonal moving average (SMA) terms. Often denoted \"Q\" PDQ-notation.","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_reg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for ARIMA Regression Models — arima_reg","text":"data given function saved used determine mode model. arima_reg(), mode always \"regression\". model can created using fit() function using following engines: \"auto_arima\" (default) - Connects forecast::auto.arima() \"arima\" - Connects forecast::Arima() Main Arguments main arguments (tuning parameters) model : seasonal_period: periodic nature seasonality. Uses \"auto\" default. non_seasonal_ar: order non-seasonal auto-regressive (AR) terms. non_seasonal_differences: order integration non-seasonal differencing. non_seasonal_ma: order non-seasonal moving average (MA) terms. seasonal_ar: order seasonal auto-regressive (SAR) terms. seasonal_differences: order integration seasonal differencing. seasonal_ma: order seasonal moving average (SMA) terms. arguments converted specific names time model fit. options argument can set using set_engine() (See Engine Details ). parameters need modified, update() can used lieu recreating object scratch.","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_reg.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for ARIMA Regression Models — arima_reg","text":"standardized parameter names modeltime can mapped original names engine: options can set using set_engine(). auto_arima (default engine) engine uses forecast::auto.arima(). Function Parameters: MAXIMUM nonseasonal ARIMA terms (max.p, max.d, max.q) seasonal ARIMA terms (max.P, max.D, max.Q) provided forecast::auto.arima() via arima_reg() parameters. options argument can set using set_engine(). Parameter Notes: values nonseasonal pdq seasonal PDQ maximums. forecast::auto.arima() model select value using upper limit. xreg - supplied via parsnip / modeltime fit() interface (provide manually). See Fit Details (). arima engine uses forecast::Arima(). Function Parameters: nonseasonal ARIMA terms (order) seasonal ARIMA terms (seasonal) provided forecast::Arima() via arima_reg() parameters. options argument can set using set_engine(). Parameter Notes: xreg - supplied via parsnip / modeltime fit() interface (provide manually). See Fit Details (). method - default set \"ML\" (Maximum Likelihood). method robust expense speed possible selections may fail unit root inversion testing. Alternatively, can add method = \"CSS-ML\" evaluate Conditional Sum Squares starting values, Maximium Likelihood.","code":"## function (y, d = NA, D = NA, max.p = 5, max.q = 5, max.P = 2, max.Q = 2,  ##     max.order = 5, max.d = 2, max.D = 1, start.p = 2, start.q = 2, start.P = 1,  ##     start.Q = 1, stationary = FALSE, seasonal = TRUE, ic = c(\"aicc\", \"aic\",  ##         \"bic\"), stepwise = TRUE, nmodels = 94, trace = FALSE, approximation = (length(x) >  ##         150 | frequency(x) > 12), method = NULL, truncate = NULL, xreg = NULL,  ##     test = c(\"kpss\", \"adf\", \"pp\"), test.args = list(), seasonal.test = c(\"seas\",  ##         \"ocsb\", \"hegy\", \"ch\"), seasonal.test.args = list(), allowdrift = TRUE,  ##     allowmean = TRUE, lambda = NULL, biasadj = FALSE, parallel = FALSE,  ##     num.cores = 2, x = y, ...) ## function (y, order = c(0, 0, 0), seasonal = c(0, 0, 0), xreg = NULL, include.mean = TRUE,  ##     include.drift = FALSE, include.constant, lambda = model$lambda, biasadj = FALSE,  ##     method = c(\"CSS-ML\", \"ML\", \"CSS\"), model = NULL, x = y, ...)"},{"path":"https://business-science.github.io/modeltime/reference/arima_reg.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for ARIMA Regression Models — arima_reg","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) Seasonal Period Specification period can non-seasonal (seasonal_period = 1 \"none\") yearly seasonal (e.g. monthly time stamps, seasonal_period = 12, seasonal_period = \"12 months\", seasonal_period = \"yearly\"). 3 ways specify: seasonal_period = \"auto\": seasonal period selected based periodicity data (e.g. 12 monthly) seasonal_period = 12: numeric frequency. example, 12 common monthly data seasonal_period = \"1 year\": time-based phrase. example, \"1 year\" convert 12 monthly data. Univariate (xregs, Exogenous Regressors): univariate analysis, must include date date-time feature. Simply use: Formula Interface (recommended): fit(y ~ date) ignore xreg's. XY Interface: fit_xy(x = data[,\"date\"], y = data$y) ignore xreg's. Multivariate (xregs, Exogenous Regressors) xreg parameter populated using fit() fit_xy() function: factor, ordered factor, numeric data used xregs. Date Date-time variables used xregs character data converted factor. Xreg Example: Suppose 3 features: y (target) date (time stamp), month.lbl (labeled month ordered factor). month.lbl exogenous regressor can passed arima_reg() using fit(): fit(y ~ date + month.lbl) pass month.lbl exogenous regressor. fit_xy(data[,c(\"date\", \"month.lbl\")], y = data$y) pass x, x data frame containing month.lbl date feature. month.lbl used exogenous regressor. Note date date-time class values excluded xreg.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/arima_reg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for ARIMA Regression Models — arima_reg","text":"","code":"library(dplyr) library(parsnip) library(rsample) library(timetk) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\") m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # ---- AUTO ARIMA ----  # Model Spec model_spec <- arima_reg() %>%     set_engine(\"auto_arima\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) #> frequency = 12 observations per 1 year model_fit #> parsnip model object #>  #> Fit time:  320ms  #> Series: outcome  #> ARIMA(0,1,1)(1,1,1)[12]  #>  #> Coefficients: #>           ma1    sar1     sma1 #>       -0.3591  0.2034  -0.7114 #> s.e.   0.0702  0.1166   0.0970 #>  #> sigma^2 estimated as 0.0003485:  log likelihood=592.09 #> AIC=-1176.17   AICc=-1176   BIC=-1162.4   # ---- STANDARD ARIMA ----  # Model Spec model_spec <- arima_reg(         seasonal_period          = 12,         non_seasonal_ar          = 3,         non_seasonal_differences = 1,         non_seasonal_ma          = 3,         seasonal_ar              = 1,         seasonal_differences     = 0,         seasonal_ma              = 1     ) %>%     set_engine(\"arima\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  524ms  #> Series: outcome  #> ARIMA(3,1,3)(1,0,1)[12]  #>  #> Coefficients: #>          ar1     ar2      ar3      ma1      ma2     ma3    sar1     sma1 #>       0.2258  0.2542  -0.2801  -0.5205  -0.2663  0.2491  0.9846  -0.5381 #> s.e.  0.6883  0.4581   0.2378   0.6994   0.4192  0.3959  0.0077   0.0751 #>  #> sigma^2 estimated as 0.0003465:  log likelihood=613.46 #> AIC=-1208.91   AICc=-1208.14   BIC=-1177.48"},{"path":"https://business-science.github.io/modeltime/reference/arima_xgboost_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge ARIMA-XGBoost Modeling function — arima_xgboost_fit_impl","title":"Bridge ARIMA-XGBoost Modeling function — arima_xgboost_fit_impl","text":"Bridge ARIMA-XGBoost Modeling function","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_xgboost_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge ARIMA-XGBoost Modeling function — arima_xgboost_fit_impl","text":"","code":"arima_xgboost_fit_impl(   x,   y,   period = \"auto\",   p = 0,   d = 0,   q = 0,   P = 0,   D = 0,   Q = 0,   include.mean = TRUE,   include.drift = FALSE,   include.constant,   lambda = model$lambda,   biasadj = FALSE,   method = c(\"CSS-ML\", \"ML\", \"CSS\"),   model = NULL,   max_depth = 6,   nrounds = 15,   eta = 0.3,   colsample_bytree = NULL,   colsample_bynode = NULL,   min_child_weight = 1,   gamma = 0,   subsample = 1,   validation = 0,   early_stop = NULL,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/arima_xgboost_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge ARIMA-XGBoost Modeling function — arima_xgboost_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. p order non-seasonal auto-regressive (AR) terms. d order integration non-seasonal differencing. q order non-seasonal moving average (MA) terms. P order seasonal auto-regressive (SAR) terms. D order integration seasonal differencing. Q order seasonal moving average (SMA) terms. include.mean ARIMA model include mean term? default TRUE undifferenced series, FALSE differenced ones (mean affect fit predictions). include.drift ARIMA model include linear drift term? (.e., linear regression ARIMA errors fitted.)  default FALSE. include.constant TRUE, include.mean set TRUE undifferenced series include.drift set TRUE differenced series. Note one difference taken, constant included regardless value argument. deliberate otherwise quadratic higher order polynomial trends induced. lambda Box-Cox transformation parameter. lambda=\"auto\", transformation automatically selected using BoxCox.lambda. transformation ignored NULL. Otherwise, data transformed model estimated. biasadj Use adjusted back-transformed mean Box-Cox transformations. transformed data used produce forecasts fitted values, regular back transformation result median forecasts. biasadj TRUE, adjustment made produce mean forecasts fitted values. method Fitting method: maximum likelihood minimize conditional sum--squares. default (unless missing values) use conditional-sum--squares find starting values, maximum likelihood. model Output previous call Arima. model passed, model fitted y without re-estimating parameters. max_depth integer maximum depth tree. nrounds integer number boosting iterations. eta numeric value zero one control learning rate. colsample_bytree Subsampling proportion columns. colsample_bynode Subsampling proportion columns node within tree. See counts argument . default uses columns. min_child_weight numeric value minimum sum instance weights needed child continue split. gamma number minimum loss reduction required make partition leaf node tree subsample Subsampling proportion rows. validation positive number. [0, 1) value, validation random proportion data x y used performance assessment potential early stopping. 1 greater, number training set samples use purposes. early_stop integer NULL. NULL, number training iterations without improvement stopping. validation used, performance base validation set; otherwise training set used. ... Additional arguments passed xgboost::xgb.train","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_xgboost_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction Function for ARIMA-XGBoost Models — arima_xgboost_predict_impl","title":"Bridge prediction Function for ARIMA-XGBoost Models — arima_xgboost_predict_impl","text":"Bridge prediction Function ARIMA-XGBoost Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/arima_xgboost_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction Function for ARIMA-XGBoost Models — arima_xgboost_predict_impl","text":"","code":"arima_xgboost_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/arima_xgboost_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction Function for ARIMA-XGBoost Models — arima_xgboost_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed predict.xgb.Booster()","code":""},{"path":"https://business-science.github.io/modeltime/reference/auto_adam_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level ADAM function for translating modeltime to forecast — auto_adam_fit_impl","title":"Low-Level ADAM function for translating modeltime to forecast — auto_adam_fit_impl","text":"Low-Level ADAM function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/auto_adam_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level ADAM function for translating modeltime to forecast — auto_adam_fit_impl","text":"","code":"auto_adam_fit_impl(   x,   y,   period = \"auto\",   p = 0,   d = 0,   q = 0,   P = 0,   D = 0,   Q = 0,   model = \"ZXZ\",   constant = FALSE,   regressors = c(\"use\", \"select\", \"adapt\"),   outliers = c(\"ignore\", \"use\", \"select\"),   level = 0.99,   occurrence = c(\"none\", \"auto\", \"fixed\", \"general\", \"odds-ratio\",     \"inverse-odds-ratio\", \"direct\"),   distribution = c(\"default\", \"dnorm\", \"dlaplace\", \"ds\", \"dgnorm\", \"dlnorm\",     \"dinvgauss\", \"dgamma\"),   loss = c(\"likelihood\", \"MSE\", \"MAE\", \"HAM\", \"LASSO\", \"RIDGE\", \"MSEh\", \"TMSE\",     \"GTMSE\", \"MSCE\"),   ic = c(\"AICc\", \"AIC\", \"BIC\", \"BICc\"),   select_order = FALSE,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/auto_adam_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level ADAM function for translating modeltime to forecast — auto_adam_fit_impl","text":"x data.frame predictors y vector outcome period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. p order non-seasonal auto-regressive (AR) terms. Often denoted \"p\" pdq-notation. d order integration non-seasonal differencing. Often denoted \"d\" pdq-notation. q order non-seasonal moving average (MA) terms. Often denoted \"q\" pdq-notation. P order seasonal auto-regressive (SAR) terms. Often denoted \"P\" PDQ-notation. D order integration seasonal differencing. Often denoted \"D\" PDQ-notation. Q order seasonal moving average (SMA) terms. Often denoted \"Q\" PDQ-notation. model type ETS model. constant Logical, determining, whether constant needed model . regressors variable defines provided explanatory variables. outliers Defines outliers. level confidence level use detection outliers. occurrence type model used probability estimation. distribution density function assume error term. loss type Loss Function used optimization. ic information criterion use model selection / combination procedure. select_order TRUE, function select appropriate order using mechanism similar auto.msarima(), implemented auto.adam(). values list(ar=...,=...,ma=...) specify maximum orders check case. ... Additional arguments passed smooth::auto.adam","code":""},{"path":"https://business-science.github.io/modeltime/reference/auto_arima_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level ARIMA function for translating modeltime to forecast — auto_arima_fit_impl","title":"Low-Level ARIMA function for translating modeltime to forecast — auto_arima_fit_impl","text":"Low-Level ARIMA function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/auto_arima_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level ARIMA function for translating modeltime to forecast — auto_arima_fit_impl","text":"","code":"auto_arima_fit_impl(   x,   y,   period = \"auto\",   max.p = 5,   max.d = 2,   max.q = 5,   max.P = 2,   max.D = 1,   max.Q = 2,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/auto_arima_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level ARIMA function for translating modeltime to forecast — auto_arima_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. max.p maximum order non-seasonal auto-regressive (AR) terms. max.d maximum order integration non-seasonal differencing. max.q maximum order non-seasonal moving average (MA) terms. max.P maximum order seasonal auto-regressive (SAR) terms. max.D maximum order integration seasonal differencing. max.Q maximum order seasonal moving average (SMA) terms. ... Additional arguments passed forecast::auto.arima","code":""},{"path":"https://business-science.github.io/modeltime/reference/auto_arima_xgboost_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge ARIMA-XGBoost Modeling function — auto_arima_xgboost_fit_impl","title":"Bridge ARIMA-XGBoost Modeling function — auto_arima_xgboost_fit_impl","text":"Bridge ARIMA-XGBoost Modeling function","code":""},{"path":"https://business-science.github.io/modeltime/reference/auto_arima_xgboost_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge ARIMA-XGBoost Modeling function — auto_arima_xgboost_fit_impl","text":"","code":"auto_arima_xgboost_fit_impl(   x,   y,   period = \"auto\",   max.p = 5,   max.d = 2,   max.q = 5,   max.P = 2,   max.D = 1,   max.Q = 2,   max.order = 5,   d = NA,   D = NA,   start.p = 2,   start.q = 2,   start.P = 1,   start.Q = 1,   stationary = FALSE,   seasonal = TRUE,   ic = c(\"aicc\", \"aic\", \"bic\"),   stepwise = TRUE,   nmodels = 94,   trace = FALSE,   approximation = (length(x) > 150 | frequency(x) > 12),   method = NULL,   truncate = NULL,   test = c(\"kpss\", \"adf\", \"pp\"),   test.args = list(),   seasonal.test = c(\"seas\", \"ocsb\", \"hegy\", \"ch\"),   seasonal.test.args = list(),   allowdrift = TRUE,   allowmean = TRUE,   lambda = NULL,   biasadj = FALSE,   max_depth = 6,   nrounds = 15,   eta = 0.3,   colsample_bytree = NULL,   colsample_bynode = NULL,   min_child_weight = 1,   gamma = 0,   subsample = 1,   validation = 0,   early_stop = NULL,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/auto_arima_xgboost_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge ARIMA-XGBoost Modeling function — auto_arima_xgboost_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. max.p maximum order non-seasonal auto-regressive (AR) terms. max.d maximum order integration non-seasonal differencing. max.q maximum order non-seasonal moving average (MA) terms. max.P maximum order seasonal auto-regressive (SAR) terms. max.D maximum order integration seasonal differencing. max.Q maximum order seasonal moving average (SMA) terms. max.order Maximum value p+q+P+Q model selection stepwise. d Order first-differencing. missing, choose value based test. D Order seasonal-differencing. missing, choose value based season.test. start.p Starting value p stepwise procedure. start.q Starting value q stepwise procedure. start.P Starting value P stepwise procedure. start.Q Starting value Q stepwise procedure. stationary TRUE, restricts search stationary models. seasonal FALSE, restricts search non-seasonal models. ic Information criterion used model selection. stepwise TRUE, stepwise selection (faster). Otherwise, searches models. Non-stepwise selection can slow, especially seasonal models. nmodels Maximum number models considered stepwise search. trace TRUE, list ARIMA models considered reported. approximation TRUE, estimation via conditional sums squares information criteria used model selection approximated. final model still computed using maximum likelihood estimation. Approximation used long time series high seasonal period avoid excessive computation times. method fitting method: maximum likelihood minimize     conditional sum--squares.  default (unless missing     values) use conditional-sum--squares find starting     values, maximum likelihood.  Can abbreviated. truncate integer value indicating many observations use model selection. last truncate values series used select model truncate NULL approximation=TRUE. observations used either truncate=NULL approximation=FALSE. test Type unit root test use. See ndiffs details. test.args Additional arguments passed unit root test. seasonal.test determines method used select number seasonal differences. default method use measure seasonal strength computed STL decomposition. possibilities involve seasonal unit root tests. seasonal.test.args Additional arguments passed seasonal unit root test. See nsdiffs details. allowdrift TRUE, models drift terms considered. allowmean TRUE, models non-zero mean considered. lambda Box-Cox transformation parameter. lambda=\"auto\", transformation automatically selected using BoxCox.lambda. transformation ignored NULL. Otherwise, data transformed model estimated. biasadj Use adjusted back-transformed mean Box-Cox transformations. transformed data used produce forecasts fitted values, regular back transformation result median forecasts. biasadj TRUE, adjustment made produce mean forecasts fitted values. max_depth integer maximum depth tree. nrounds integer number boosting iterations. eta numeric value zero one control learning rate. colsample_bytree Subsampling proportion columns. colsample_bynode Subsampling proportion columns node within tree. See counts argument . default uses columns. min_child_weight numeric value minimum sum instance weights needed child continue split. gamma number minimum loss reduction required make partition leaf node tree subsample Subsampling proportion rows. validation positive number. [0, 1) value, validation random proportion data x y used performance assessment potential early stopping. 1 greater, number training set samples use purposes. early_stop integer NULL. NULL, number training iterations without improvement stopping. validation used, performance base validation set; otherwise training set used. ... Additional arguments passed xgboost::xgb.train","code":""},{"path":"https://business-science.github.io/modeltime/reference/combine_modeltime_tables.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine multiple Modeltime Tables into a single Modeltime Table — combine_modeltime_tables","title":"Combine multiple Modeltime Tables into a single Modeltime Table — combine_modeltime_tables","text":"Combine multiple Modeltime Tables single Modeltime Table","code":""},{"path":"https://business-science.github.io/modeltime/reference/combine_modeltime_tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine multiple Modeltime Tables into a single Modeltime Table — combine_modeltime_tables","text":"","code":"combine_modeltime_tables(...)"},{"path":"https://business-science.github.io/modeltime/reference/combine_modeltime_tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine multiple Modeltime Tables into a single Modeltime Table — combine_modeltime_tables","text":"... Multiple Modeltime Tables (class mdl_time_tbl)","code":""},{"path":"https://business-science.github.io/modeltime/reference/combine_modeltime_tables.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Combine multiple Modeltime Tables into a single Modeltime Table — combine_modeltime_tables","text":"function combines multiple Modeltime Tables. .model_id automatically renumbered ensure model unique ID. .model_id, .model, .model_desc columns returned. Re-Training Models Datasets One issue can arise models trained different datasets. models trained different datasets, can run modeltime_refit() train models data. Re-Calibrating Models data calibrated using modeltime_calibrate(), .test .calibration_data columns removed. re-calibrate, simply run modeltime_calibrate() newly combined Modeltime Table.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/combine_modeltime_tables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combine multiple Modeltime Tables into a single Modeltime Table — combine_modeltime_tables","text":"","code":"library(modeltime) library(tidymodels) library(tidyverse) library(timetk) library(lubridate)  # Setup m750 <- m4_monthly %>% filter(id == \"M750\")  splits <- time_series_split(m750, assess = \"3 years\", cumulative = TRUE) #> Using date_var: date  model_fit_arima <- arima_reg() %>%     set_engine(\"auto_arima\") %>%     fit(value ~ date, training(splits)) #> frequency = 12 observations per 1 year  model_fit_prophet <- prophet_reg() %>%     set_engine(\"prophet\") %>%     fit(value ~ date, training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.  # Multiple Modeltime Tables model_tbl_1 <- modeltime_table(model_fit_arima) model_tbl_2 <- modeltime_table(model_fit_prophet)  # Combine combine_modeltime_tables(model_tbl_1, model_tbl_2) #> # Modeltime Table #> # A tibble: 2 × 3 #>   .model_id .model   .model_desc             #>       <int> <list>   <chr>                   #> 1         1 <fit[+]> ARIMA(0,1,1)(0,1,1)[12] #> 2         2 <fit[+]> PROPHET"},{"path":"https://business-science.github.io/modeltime/reference/control_modeltime.html","id":null,"dir":"Reference","previous_headings":"","what":"Control aspects of the training process — control_modeltime","title":"Control aspects of the training process — control_modeltime","text":"functions matched associated training functions: control_refit(): Used modeltime_refit() control_fit_workflowset(): Used modeltime_fit_workflowset() control_nested_fit(): Used modeltime_nested_fit() control_nested_refit(): Used modeltime_nested_refit() control_nested_forecast(): Used modeltime_nested_forecast()","code":""},{"path":"https://business-science.github.io/modeltime/reference/control_modeltime.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Control aspects of the training process — control_modeltime","text":"","code":"control_refit(verbose = FALSE, allow_par = FALSE, cores = -1, packages = NULL)  control_fit_workflowset(   verbose = FALSE,   allow_par = FALSE,   cores = -1,   packages = NULL )  control_nested_fit(   verbose = FALSE,   allow_par = FALSE,   cores = -1,   packages = NULL )  control_nested_refit(   verbose = FALSE,   allow_par = FALSE,   cores = -1,   packages = NULL )  control_nested_forecast(   verbose = FALSE,   allow_par = FALSE,   cores = -1,   packages = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/control_modeltime.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Control aspects of the training process — control_modeltime","text":"verbose Logical control printing. allow_par Logical allow parallel computation. Default: FALSE (single threaded). cores Number cores computation. -1, uses available physical cores. Default: -1. packages optional character string additional R package names loaded parallel processing. Packages namespace loaded default Key Packages loaded default: tidymodels, parsnip, modeltime, dplyr, stats, lubridate timetk.","code":""},{"path":"https://business-science.github.io/modeltime/reference/control_modeltime.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Control aspects of the training process — control_modeltime","text":"List control settings.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/control_modeltime.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Control aspects of the training process — control_modeltime","text":"","code":"# No parallel processing by default control_refit() #> refit control object #> -------------------- #> allow_par : FALSE  #> cores     : 1  #> verbose   : FALSE   # Allow parallel processing control_refit(allow_par = TRUE) #> refit control object #> -------------------- #> allow_par : TRUE  #> cores     : 6  #> verbose   : FALSE  #> packages  : modeltime parsnip workflows dplyr stats lubridate tidymodels timetk rsample recipes yardstick dials tune forcats stringr readr tidyverse workflowsets tidyr tibble purrr modeldata infer ggplot2 scales broom graphics grDevices utils datasets methods base   # Set verbosity to show additional training information control_refit(verbose = TRUE) #> refit control object #> -------------------- #> allow_par : FALSE  #> cores     : 1  #> verbose   : TRUE   # Add additional packages used during modeling in parallel processing # - This is useful if your namespace does not load all needed packages #   to run models. # - An example is if I use `temporal_hierarchy()`, which depends on the `thief` package control_refit(allow_par = TRUE, packages = \"thief\") #> refit control object #> -------------------- #> allow_par : TRUE  #> cores     : 6  #> verbose   : FALSE  #> packages  : modeltime parsnip workflows dplyr stats lubridate tidymodels timetk rsample recipes yardstick dials tune forcats stringr readr tidyverse workflowsets tidyr tibble purrr modeldata infer ggplot2 scales broom graphics grDevices utils datasets methods base thief"},{"path":"https://business-science.github.io/modeltime/reference/create_model_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper to make parsnip model specs from a dials parameter grid — create_model_grid","title":"Helper to make parsnip model specs from a dials parameter grid — create_model_grid","text":"Helper make parsnip model specs dials parameter grid","code":""},{"path":"https://business-science.github.io/modeltime/reference/create_model_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper to make parsnip model specs from a dials parameter grid — create_model_grid","text":"","code":"create_model_grid(grid, f_model_spec, engine_name, ..., engine_params = list())"},{"path":"https://business-science.github.io/modeltime/reference/create_model_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper to make parsnip model specs from a dials parameter grid — create_model_grid","text":"grid tibble forms grid parameters adjust f_model_spec function name (quoted unquoted) specifies parsnip model specification function engine_name name engine use. Gets passed parsnip::set_engine(). ... Static parameters get passed f_model_spec engine_params list additional parameters can passed engine via parsnip::set_engine(...).","code":""},{"path":"https://business-science.github.io/modeltime/reference/create_model_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper to make parsnip model specs from a dials parameter grid — create_model_grid","text":"Tibble new colum named .models","code":""},{"path":"https://business-science.github.io/modeltime/reference/create_model_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Helper to make parsnip model specs from a dials parameter grid — create_model_grid","text":"helper function combines dials grids parsnip model specifications. intent make easier generate workflowset objects forecast evaluations modeltime_fit_workflowset(). process follows: Generate grid (hyperparemeter combination) Use create_model_grid() apply parameter combinations parsnip model spec engine. output contains \".model\" column can used list models inside workflow_set() function.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/create_model_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Helper to make parsnip model specs from a dials parameter grid — create_model_grid","text":"","code":"library(tidymodels) library(modeltime)  # Parameters that get optimized grid_tbl <- grid_regular(     learn_rate(),     levels = 3 )  # Generate model specs grid_tbl %>%     create_model_grid(         f_model_spec = boost_tree,         engine_name  = \"xgboost\",         # Static boost_tree() args         mode = \"regression\",         # Static set_engine() args         engine_params = list(             max_depth = 5         )     ) #> # A tibble: 3 × 2 #>     learn_rate .models   #>          <dbl> <list>    #> 1 0.0000000001 <spec[+]> #> 2 0.00000316   <spec[+]> #> 3 0.1          <spec[+]>"},{"path":"https://business-science.github.io/modeltime/reference/create_xreg_recipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Developer Tools for preparing XREGS (Regressors) — create_xreg_recipe","title":"Developer Tools for preparing XREGS (Regressors) — create_xreg_recipe","text":"functions designed assist developers extending modeltime package. create_xregs_recipe() makes simple automate conversion raw un-encoded features machine-learning ready features.","code":""},{"path":"https://business-science.github.io/modeltime/reference/create_xreg_recipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Developer Tools for preparing XREGS (Regressors) — create_xreg_recipe","text":"","code":"create_xreg_recipe(   data,   prepare = TRUE,   clean_names = TRUE,   dummy_encode = TRUE,   one_hot = FALSE )"},{"path":"https://business-science.github.io/modeltime/reference/create_xreg_recipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Developer Tools for preparing XREGS (Regressors) — create_xreg_recipe","text":"data data frame prepare Whether run recipes::prep() final recipe. Default prepare. User can set FALSE return un prepared recipe. clean_names Uses janitor::clean_names() process names improve robustness failure dummy (one-hot) encoding step. dummy_encode factors (categorical data) one_hot dummy_encode = TRUE, encoding return one column feature one less column feature. Default FALSE.","code":""},{"path":"https://business-science.github.io/modeltime/reference/create_xreg_recipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Developer Tools for preparing XREGS (Regressors) — create_xreg_recipe","text":"recipe either prepared un-prepared format.","code":""},{"path":"https://business-science.github.io/modeltime/reference/create_xreg_recipe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Developer Tools for preparing XREGS (Regressors) — create_xreg_recipe","text":"default recipe contains steps : Remove date features Clean column names removing spaces bad characters Convert ordered factors regular factors Convert factors dummy variables Remove variables zero variance","code":""},{"path":"https://business-science.github.io/modeltime/reference/create_xreg_recipe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Developer Tools for preparing XREGS (Regressors) — create_xreg_recipe","text":"","code":"library(dplyr) library(timetk) library(recipes) library(lubridate)  predictors <- m4_monthly %>%     filter(id == \"M750\") %>%     select(-value) %>%     mutate(month = month(date, label = TRUE)) predictors #> # A tibble: 306 × 3 #>    id    date       month #>    <fct> <date>     <ord> #>  1 M750  1990-01-01 Jan   #>  2 M750  1990-02-01 Feb   #>  3 M750  1990-03-01 Mar   #>  4 M750  1990-04-01 Apr   #>  5 M750  1990-05-01 May   #>  6 M750  1990-06-01 Jun   #>  7 M750  1990-07-01 Jul   #>  8 M750  1990-08-01 Aug   #>  9 M750  1990-09-01 Sep   #> 10 M750  1990-10-01 Oct   #> # … with 296 more rows  # Create default recipe xreg_recipe_spec <- create_xreg_recipe(predictors, prepare = TRUE)  # Extracts the preprocessed training data from the recipe (used in your fit function) juice_xreg_recipe(xreg_recipe_spec) #> # A tibble: 306 × 11 #>    month_Feb month_Mar month_Apr month_May month_Jun month_Jul month_Aug #>        <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl> #>  1         0         0         0         0         0         0         0 #>  2         1         0         0         0         0         0         0 #>  3         0         1         0         0         0         0         0 #>  4         0         0         1         0         0         0         0 #>  5         0         0         0         1         0         0         0 #>  6         0         0         0         0         1         0         0 #>  7         0         0         0         0         0         1         0 #>  8         0         0         0         0         0         0         1 #>  9         0         0         0         0         0         0         0 #> 10         0         0         0         0         0         0         0 #> # … with 296 more rows, and 4 more variables: month_Sep <dbl>, month_Oct <dbl>, #> #   month_Nov <dbl>, month_Dec <dbl>  # Applies the prepared recipe to new data (used in your predict function) bake_xreg_recipe(xreg_recipe_spec, new_data = predictors) #> # A tibble: 306 × 11 #>    month_Feb month_Mar month_Apr month_May month_Jun month_Jul month_Aug #>        <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl> #>  1         0         0         0         0         0         0         0 #>  2         1         0         0         0         0         0         0 #>  3         0         1         0         0         0         0         0 #>  4         0         0         1         0         0         0         0 #>  5         0         0         0         1         0         0         0 #>  6         0         0         0         0         1         0         0 #>  7         0         0         0         0         0         1         0 #>  8         0         0         0         0         0         0         1 #>  9         0         0         0         0         0         0         0 #> 10         0         0         0         0         0         0         0 #> # … with 296 more rows, and 4 more variables: month_Sep <dbl>, month_Oct <dbl>, #> #   month_Nov <dbl>, month_Dec <dbl>"},{"path":"https://business-science.github.io/modeltime/reference/croston_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level Exponential Smoothing function for translating modeltime to forecast — croston_fit_impl","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — croston_fit_impl","text":"Low-Level Exponential Smoothing function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/croston_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — croston_fit_impl","text":"","code":"croston_fit_impl(x, y, alpha = 0.1, ...)"},{"path":"https://business-science.github.io/modeltime/reference/croston_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — croston_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit alpha Value alpha. Default value 0.1. ... Additional arguments passed forecast::ets","code":""},{"path":"https://business-science.github.io/modeltime/reference/croston_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for CROSTON models — croston_predict_impl","title":"Bridge prediction function for CROSTON models — croston_predict_impl","text":"Bridge prediction function CROSTON models","code":""},{"path":"https://business-science.github.io/modeltime/reference/croston_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for CROSTON models — croston_predict_impl","text":"","code":"croston_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/croston_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for CROSTON models — croston_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed stats::predict()","code":""},{"path":"https://business-science.github.io/modeltime/reference/dot_prepare_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare Recursive Transformations — .prepare_transform","title":"Prepare Recursive Transformations — .prepare_transform","text":"Prepare Recursive Transformations","code":""},{"path":"https://business-science.github.io/modeltime/reference/dot_prepare_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare Recursive Transformations — .prepare_transform","text":"","code":".prepare_transform(.transform)  .prepare_panel_transform(.transform)"},{"path":"https://business-science.github.io/modeltime/reference/dot_prepare_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare Recursive Transformations — .prepare_transform","text":".transform transformation function","code":""},{"path":"https://business-science.github.io/modeltime/reference/dot_prepare_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare Recursive Transformations — .prepare_transform","text":"function applies recursive transformation","code":""},{"path":"https://business-science.github.io/modeltime/reference/ets_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level Exponential Smoothing function for translating modeltime to forecast — ets_fit_impl","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — ets_fit_impl","text":"Low-Level Exponential Smoothing function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/ets_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — ets_fit_impl","text":"","code":"ets_fit_impl(   x,   y,   period = \"auto\",   error = \"auto\",   trend = \"auto\",   season = \"auto\",   damping = \"auto\",   alpha = NULL,   beta = NULL,   gamma = NULL,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/ets_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — ets_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. error form error term: \"auto\", \"additive\", \"multiplicative\". error multiplicative, data must non-negative. trend form trend term: \"auto\", \"additive\", \"multiplicative\" \"none\". season form seasonal term: \"auto\", \"additive\", \"multiplicative\" \"none\". damping Apply damping trend: \"auto\", \"damped\", \"none\". alpha Value alpha. NULL, estimated. beta Value beta. NULL, estimated. gamma Value gamma. NULL, estimated. ... Additional arguments passed forecast::ets","code":""},{"path":"https://business-science.github.io/modeltime/reference/ets_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for Exponential Smoothing models — ets_predict_impl","title":"Bridge prediction function for Exponential Smoothing models — ets_predict_impl","text":"Bridge prediction function Exponential Smoothing models","code":""},{"path":"https://business-science.github.io/modeltime/reference/ets_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for Exponential Smoothing models — ets_predict_impl","text":"","code":"ets_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/ets_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for Exponential Smoothing models — ets_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed forecast::ets()","code":""},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for Exponential Smoothing State Space Models — exp_smoothing","title":"General Interface for Exponential Smoothing State Space Models — exp_smoothing","text":"exp_smoothing() way generate specification Exponential Smoothing model fitting allows model created using different packages. Currently package forecast. Several algorithms implemented: ETS - Automated Exponential Smoothing CROSTON - Croston's forecast special case Exponential Smoothing intermittent demand Theta - special case Exponential Smoothing Drift performed well M3 Competition","code":""},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for Exponential Smoothing State Space Models — exp_smoothing","text":"","code":"exp_smoothing(   mode = \"regression\",   seasonal_period = NULL,   error = NULL,   trend = NULL,   season = NULL,   damping = NULL,   smooth_level = NULL,   smooth_trend = NULL,   smooth_seasonal = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for Exponential Smoothing State Space Models — exp_smoothing","text":"mode single character string type model. possible value model \"regression\". seasonal_period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details . error form error term: \"auto\", \"additive\", \"multiplicative\". error multiplicative, data must non-negative. trend form trend term: \"auto\", \"additive\", \"multiplicative\" \"none\". season form seasonal term: \"auto\", \"additive\", \"multiplicative\" \"none\". damping Apply damping trend: \"auto\", \"damped\", \"none\". smooth_level often called \"alpha\" parameter used base level smoothing factor exponential smoothing models. smooth_trend often called \"beta\" parameter used trend smoothing factor exponential smoothing models. smooth_seasonal often called \"gamma\" parameter used seasonal smoothing factor exponential smoothing models.","code":""},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for Exponential Smoothing State Space Models — exp_smoothing","text":"Models can created using following engines: \"ets\" (default) - Connects forecast::ets() \"croston\" - Connects forecast::croston() \"theta\" - Connects forecast::thetaf() \"smooth_es\" - Connects smooth::es()","code":""},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for Exponential Smoothing State Space Models — exp_smoothing","text":"standardized parameter names modeltime can mapped original names engine: options can set using set_engine(). ets (default engine) engine uses forecast::ets(). Function Parameters: main arguments model damped defined using: error() = \"auto\", \"additive\", \"multiplicative\" converted  \"Z\", \"\", \"M\" trend() = \"auto\", \"additive\", \"multiplicative\", \"none\" converted \"Z\",\"\",\"M\" \"N\" season() = \"auto\", \"additive\", \"multiplicative\", \"none\" converted \"Z\",\"\",\"M\" \"N\" damping() - \"auto\", \"damped\", \"none\" converted NULL, TRUE, FALSE smooth_level(), smooth_trend(), smooth_seasonal() automatically determined provided. mapped \"alpha\", \"beta\" \"gamma\", respectively. default, arguments set \"auto\" perform automated Exponential Smoothing using -sample data following underlying forecast::ets() automation routine. options argument can set using set_engine(). Parameter Notes: xreg - model set use exogenous regressors. univariate models fit. croston engine uses forecast::croston(). Function Parameters: main arguments defined using: smooth_level(): \"alpha\" parameter Parameter Notes: xreg - model set use exogenous regressors. univariate models fit. theta engine uses forecast::thetaf() Parameter Notes: xreg - model set use exogenous regressors. univariate models fit. smooth_es engine uses smooth::es(). Function Parameters: main arguments model phi defined using: error() = \"auto\", \"additive\" \"multiplicative\" converted \"Z\", \"\" \"M\" trend() = \"auto\", \"additive\", \"multiplicative\", \"additive_damped\", \"multiplicative_damped\" \"none\" converted \"Z\", \"\", \"M\", \"Ad\", \"Md\" \"N\". season() = \"auto\", \"additive\", \"multiplicative\", \"none\" converted \"Z\", \"\",\"M\" \"N\" damping() - Value damping parameter. NULL, estimated. smooth_level(), smooth_trend(), smooth_seasonal() automatically determined provided. mapped \"persistence\"(\"alpha\", \"beta\" \"gamma\", respectively). default, arguments set \"auto\" perform automated Exponential Smoothing using -sample data following underlying smooth::es() automation routine. options argument can set using set_engine(). Parameter Notes: xreg - supplied via parsnip / modeltime fit() interface (provide manually). See Fit Details ().","code":"## function (y, model = \"ZZZ\", damped = NULL, alpha = NULL, beta = NULL, gamma = NULL,  ##     phi = NULL, additive.only = FALSE, lambda = NULL, biasadj = FALSE,  ##     lower = c(rep(1e-04, 3), 0.8), upper = c(rep(0.9999, 3), 0.98), opt.crit = c(\"lik\",  ##         \"amse\", \"mse\", \"sigma\", \"mae\"), nmse = 3, bounds = c(\"both\", \"usual\",  ##         \"admissible\"), ic = c(\"aicc\", \"aic\", \"bic\"), restrict = TRUE, allow.multiplicative.trend = FALSE,  ##     use.initial.values = FALSE, na.action = c(\"na.contiguous\", \"na.interp\",  ##         \"na.fail\"), ...) ## function (y, h = 10, alpha = 0.1, x = y) ## function (y, model = \"ZZZ\", persistence = NULL, phi = NULL, initial = c(\"optimal\",  ##     \"backcasting\"), initialSeason = NULL, ic = c(\"AICc\", \"AIC\", \"BIC\",  ##     \"BICc\"), loss = c(\"likelihood\", \"MSE\", \"MAE\", \"HAM\", \"MSEh\", \"TMSE\",  ##     \"GTMSE\", \"MSCE\"), h = 10, holdout = FALSE, cumulative = FALSE, interval = c(\"none\",  ##     \"parametric\", \"likelihood\", \"semiparametric\", \"nonparametric\"), level = 0.95,  ##     bounds = c(\"usual\", \"admissible\", \"none\"), silent = c(\"all\", \"graph\",  ##         \"legend\", \"output\", \"none\"), xreg = NULL, xregDo = c(\"use\", \"select\"),  ##     initialX = NULL, ...)"},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for Exponential Smoothing State Space Models — exp_smoothing","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) Seasonal Period Specification period can non-seasonal (seasonal_period = 1 \"none\") seasonal (e.g. seasonal_period = 12 seasonal_period = \"12 months\"). 3 ways specify: seasonal_period = \"auto\": period selected based periodicity data (e.g. 12 monthly) seasonal_period = 12: numeric frequency. example, 12 common monthly data seasonal_period = \"1 year\": time-based phrase. example, \"1 year\" convert 12 monthly data. Univariate: univariate analysis, must include date date-time feature. Simply use: Formula Interface (recommended): fit(y ~ date) ignore xreg's. XY Interface: fit_xy(x = data[,\"date\"], y = data$y) ignore xreg's. Multivariate (xregs, Exogenous Regressors) Just smooth engine. xreg parameter populated using fit() fit_xy() function: factor, ordered factor, numeric data used xregs. Date Date-time variables used xregs character data converted factor. Xreg Example: Suppose 3 features: y (target) date (time stamp), month.lbl (labeled month ordered factor). month.lbl exogenous regressor can passed arima_reg() using fit(): fit(y ~ date + month.lbl) pass month.lbl exogenous regressor. fit_xy(data[,c(\"date\", \"month.lbl\")], y = data$y) pass x, x data frame containing month.lbl date feature. month.lbl used exogenous regressor. Note date date-time class values excluded xreg.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for Exponential Smoothing State Space Models — exp_smoothing","text":"","code":"library(dplyr) library(parsnip) library(rsample) library(timetk) library(modeltime) library(smooth) #> Loading required package: greybox #> Package \"greybox\", v1.0.2 loaded. #>  #> Attaching package: ‘greybox’ #> The following object is masked from ‘package:lubridate’: #>  #>     hm #> The following object is masked from ‘package:tidyr’: #>  #>     spread #> This is package \"smooth\", v3.1.4  # Data m750 <- m4_monthly %>% filter(id == \"M750\") m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # ---- AUTO ETS ----  # Model Spec - The default parameters are all set # to \"auto\" if none are provided model_spec <- exp_smoothing() %>%     set_engine(\"ets\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) #> frequency = 12 observations per 1 year model_fit #> parsnip model object #>  #> Fit time:  661ms  #> ETS(A,A,A)  #>  #> Call: #>  forecast::ets(y = outcome, model = model_ets, damped = damping_ets,   #>  #>  Call: #>      alpha = alpha, beta = beta, gamma = gamma)  #>  #>   Smoothing parameters: #>     alpha = 0.5893  #>     beta  = 1e-04  #>     gamma = 0.1771  #>  #>   Initial states: #>     l = 8.7377  #>     b = 0.002  #>     s = 0.029 0.0259 0.0144 -0.0272 -0.1369 -0.0764 #>            0.0209 0.0358 0.036 0.035 0.0274 0.016 #>  #>   sigma:  0.0186 #>  #>       AIC      AICc       BIC  #> -584.7384 -582.0304 -525.2865    # ---- STANDARD ETS ----  # Model Spec model_spec <- exp_smoothing(         seasonal_period  = 12,         error            = \"multiplicative\",         trend            = \"additive\",         season           = \"multiplicative\"     ) %>%     set_engine(\"ets\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  165ms  #> ETS(M,Ad,M)  #>  #> Call: #>  forecast::ets(y = outcome, model = model_ets, damped = damping_ets,   #>  #>  Call: #>      alpha = alpha, beta = beta, gamma = gamma)  #>  #>   Smoothing parameters: #>     alpha = 0.5889  #>     beta  = 0.0065  #>     gamma = 0.203  #>     phi   = 0.98  #>  #>   Initial states: #>     l = 8.7353  #>     b = 0.0054  #>     s = 1.0027 1.0025 1.0012 0.9972 0.9839 0.9921 #>            1.0024 1.0041 1.0045 1.0039 1.0033 1.0022 #>  #>   sigma:  0.0021 #>  #>       AIC      AICc       BIC  #> -576.9488 -573.9088 -513.9998    # ---- CROSTON ---- # \\donttest{ # Model Spec model_spec <- exp_smoothing(         smooth_level = 0.2     ) %>%     set_engine(\"croston\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  1s  #> Croston Method #> --- # }    # ---- THETA ---- # \\donttest{ #' # Model Spec model_spec <- exp_smoothing() %>%     set_engine(\"theta\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  7ms  #> Theta Method #> --- # }     #' # ---- SMOOTH ---- # \\donttest{ #' # Model Spec model_spec <- exp_smoothing(                seasonal_period  = 12,                error            = \"multiplicative\",                trend            = \"additive_damped\",                season           = \"additive\"          ) %>%     set_engine(\"smooth_es\")  # Fit Spec model_fit <- model_spec %>%     fit(value ~ date, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  564ms  #> Time elapsed: 0.55 seconds #> Model estimated: ETS(MAdA) #> Persistence vector g: #>  alpha   beta  gamma  #> 0.5569 0.0000 0.2267  #> Damping parameter: 0.9987 #> Initial values were optimised. #>  #> Loss function type: likelihood; Loss function value: 1564.4611 #> Error standard deviation: 0.018 #> Sample size: 244 #> Number of estimated parameters: 18 #> Number of provided parameters: 1 #> Number of degrees of freedom: 226 #> Information criteria: #>      AIC     AICc      BIC     BICc  #> 3164.922 3167.962 3227.871 3236.227  # }"},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuning Parameters for Exponential Smoothing Models — exp_smoothing_params","title":"Tuning Parameters for Exponential Smoothing Models — exp_smoothing_params","text":"Tuning Parameters Exponential Smoothing Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning Parameters for Exponential Smoothing Models — exp_smoothing_params","text":"","code":"error(values = c(\"additive\", \"multiplicative\"))  trend(values = c(\"additive\", \"multiplicative\", \"none\"))  trend_smooth(   values = c(\"additive\", \"multiplicative\", \"none\", \"additive_damped\",     \"multiplicative_damped\") )  season(values = c(\"additive\", \"multiplicative\", \"none\"))  damping(values = c(\"damped\", \"none\"))  damping_smooth(range = c(0, 2), trans = NULL)  smooth_level(range = c(0, 1), trans = NULL)  smooth_trend(range = c(0, 1), trans = NULL)  smooth_seasonal(range = c(0, 1), trans = NULL)"},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuning Parameters for Exponential Smoothing Models — exp_smoothing_params","text":"values character string possible values. range two-element vector holding defaults smallest largest possible values, respectively. trans trans object scales package, scales::log10_trans() scales::reciprocal_trans(). provided, default used matches units used range. transformation, NULL.","code":""},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing_params.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tuning Parameters for Exponential Smoothing Models — exp_smoothing_params","text":"main parameters Exponential Smoothing models : error: form error term: additive\", \"multiplicative\". error multiplicative, data must non-negative. trend: form trend term: \"additive\", \"multiplicative\" \"none\". season: form seasonal term: \"additive\", \"multiplicative\" \"none\".. damping: Apply damping trend: \"damped\", \"none\". smooth_level: often called \"alpha\" parameter used base level smoothing factor exponential smoothing models. smooth_trend: often called \"beta\" parameter used trend smoothing factor exponential smoothing models. smooth_seasonal: often called \"gamma\" parameter used seasonal smoothing factor exponential smoothing models.","code":""},{"path":"https://business-science.github.io/modeltime/reference/exp_smoothing_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tuning Parameters for Exponential Smoothing Models — exp_smoothing_params","text":"","code":"error() #> Error Term  (qualitative) #> 2 possible value include: #> 'additive' and 'multiplicative'   trend() #> Trend Term  (qualitative) #> 3 possible value include: #> 'additive', 'multiplicative' and 'none'   season() #> Season Term  (qualitative) #> 3 possible value include: #> 'additive', 'multiplicative' and 'none'"},{"path":"https://business-science.github.io/modeltime/reference/get_arima_description.html","id":null,"dir":"Reference","previous_headings":"","what":"Get model descriptions for Arima objects — get_arima_description","title":"Get model descriptions for Arima objects — get_arima_description","text":"Get model descriptions Arima objects","code":""},{"path":"https://business-science.github.io/modeltime/reference/get_arima_description.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get model descriptions for Arima objects — get_arima_description","text":"","code":"get_arima_description(object, padding = FALSE)"},{"path":"https://business-science.github.io/modeltime/reference/get_arima_description.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Get model descriptions for Arima objects — get_arima_description","text":"Forecast R Package, forecast:::arima.string()","code":""},{"path":"https://business-science.github.io/modeltime/reference/get_arima_description.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get model descriptions for Arima objects — get_arima_description","text":"object Objects class Arima padding Whether include padding","code":""},{"path":"https://business-science.github.io/modeltime/reference/get_arima_description.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get model descriptions for Arima objects — get_arima_description","text":"","code":"library(forecast) #>  #> Attaching package: ‘forecast’ #> The following object is masked from ‘package:yardstick’: #>  #>     accuracy  arima_fit <- forecast::Arima(1:10)  get_arima_description(arima_fit) #> [1] \"ARIMA(0,0,0) with non-zero mean\""},{"path":"https://business-science.github.io/modeltime/reference/get_model_description.html","id":null,"dir":"Reference","previous_headings":"","what":"Get model descriptions for parsnip, workflows & modeltime objects — get_model_description","title":"Get model descriptions for parsnip, workflows & modeltime objects — get_model_description","text":"Get model descriptions parsnip, workflows & modeltime objects","code":""},{"path":"https://business-science.github.io/modeltime/reference/get_model_description.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get model descriptions for parsnip, workflows & modeltime objects — get_model_description","text":"","code":"get_model_description(object, indicate_training = FALSE, upper_case = TRUE)"},{"path":"https://business-science.github.io/modeltime/reference/get_model_description.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get model descriptions for parsnip, workflows & modeltime objects — get_model_description","text":"object Parsnip workflow objects indicate_training Whether indicate model trained upper_case Whether return upper lower case model descriptions","code":""},{"path":"https://business-science.github.io/modeltime/reference/get_model_description.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get model descriptions for parsnip, workflows & modeltime objects — get_model_description","text":"","code":"library(dplyr) library(timetk) library(parsnip) library(modeltime)  # Model Specification ----  arima_spec <- arima_reg() %>%     set_engine(\"auto_arima\")  get_model_description(arima_spec, indicate_training = TRUE) #> [1] \"AUTO_ARIMA (NOT TRAINED)\"  # Fitted Model ----  m750 <- m4_monthly %>% filter(id == \"M750\")  arima_fit <- arima_spec %>%     fit(value ~ date, data = m750) #> frequency = 12 observations per 1 year  get_model_description(arima_fit, indicate_training = TRUE) #> [1] \"ARIMA(0,1,1)(0,1,1)[12] (TRAINED)\""},{"path":"https://business-science.github.io/modeltime/reference/get_tbats_description.html","id":null,"dir":"Reference","previous_headings":"","what":"Get model descriptions for TBATS objects — get_tbats_description","title":"Get model descriptions for TBATS objects — get_tbats_description","text":"Get model descriptions TBATS objects","code":""},{"path":"https://business-science.github.io/modeltime/reference/get_tbats_description.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get model descriptions for TBATS objects — get_tbats_description","text":"","code":"get_tbats_description(object)"},{"path":"https://business-science.github.io/modeltime/reference/get_tbats_description.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Get model descriptions for TBATS objects — get_tbats_description","text":"Forecast R Package, forecast:::.character.tbats()","code":""},{"path":"https://business-science.github.io/modeltime/reference/get_tbats_description.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get model descriptions for TBATS objects — get_tbats_description","text":"object Objects class tbats","code":""},{"path":"https://business-science.github.io/modeltime/reference/is_calibrated.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if a Modeltime Table has been calibrated — is_calibrated","title":"Test if a Modeltime Table has been calibrated — is_calibrated","text":"function returns TRUE objects contains columns \".type\" \".calibration_data\"","code":""},{"path":"https://business-science.github.io/modeltime/reference/is_calibrated.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if a Modeltime Table has been calibrated — is_calibrated","text":"","code":"is_calibrated(object)"},{"path":"https://business-science.github.io/modeltime/reference/is_calibrated.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if a Modeltime Table has been calibrated — is_calibrated","text":"object object detect Calibrated Modeltime Table","code":""},{"path":"https://business-science.github.io/modeltime/reference/is_modeltime_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object contains a fitted modeltime model — is_modeltime_model","title":"Test if object contains a fitted modeltime model — is_modeltime_model","text":"function returns TRUE trained workflows parsnip objects contain modeltime models","code":""},{"path":"https://business-science.github.io/modeltime/reference/is_modeltime_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object contains a fitted modeltime model — is_modeltime_model","text":"","code":"is_modeltime_model(object)"},{"path":"https://business-science.github.io/modeltime/reference/is_modeltime_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object contains a fitted modeltime model — is_modeltime_model","text":"object object detect contains fitted modeltime model","code":""},{"path":"https://business-science.github.io/modeltime/reference/is_modeltime_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is a Modeltime Table — is_modeltime_table","title":"Test if object is a Modeltime Table — is_modeltime_table","text":"function returns TRUE objects contain class mdl_time_tbl","code":""},{"path":"https://business-science.github.io/modeltime/reference/is_modeltime_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is a Modeltime Table — is_modeltime_table","text":"","code":"is_modeltime_table(object)"},{"path":"https://business-science.github.io/modeltime/reference/is_modeltime_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is a Modeltime Table — is_modeltime_table","text":"object object detect Modeltime Table","code":""},{"path":"https://business-science.github.io/modeltime/reference/is_residuals.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if a table contains residuals. — is_residuals","title":"Test if a table contains residuals. — is_residuals","text":"function returns TRUE objects contains column name '.residuals'.","code":""},{"path":"https://business-science.github.io/modeltime/reference/is_residuals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if a table contains residuals. — is_residuals","text":"","code":"is_residuals(object)"},{"path":"https://business-science.github.io/modeltime/reference/is_residuals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if a table contains residuals. — is_residuals","text":"object object detect provides modeltime::modeltime_residuals().","code":""},{"path":"https://business-science.github.io/modeltime/reference/load_namespace.html","id":null,"dir":"Reference","previous_headings":"","what":"These are not intended for use by the general public. — load_namespace","title":"These are not intended for use by the general public. — load_namespace","text":"intended use general public.","code":""},{"path":"https://business-science.github.io/modeltime/reference/load_namespace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"These are not intended for use by the general public. — load_namespace","text":"","code":"load_namespace(x, full_load)"},{"path":"https://business-science.github.io/modeltime/reference/load_namespace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"These are not intended for use by the general public. — load_namespace","text":"x vector full_load vector","code":""},{"path":"https://business-science.github.io/modeltime/reference/load_namespace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"These are not intended for use by the general public. — load_namespace","text":"Control information","code":""},{"path":"https://business-science.github.io/modeltime/reference/log_extractors.html","id":null,"dir":"Reference","previous_headings":"","what":"Log Extractor Functions for Modeltime Nested Tables — log_extractors","title":"Log Extractor Functions for Modeltime Nested Tables — log_extractors","text":"Extract logged information calculated modeltime_nested_fit(), modeltime_nested_select_best(), modeltime_nested_refit() processes.","code":""},{"path":"https://business-science.github.io/modeltime/reference/log_extractors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log Extractor Functions for Modeltime Nested Tables — log_extractors","text":"","code":"extract_nested_test_accuracy(object)  extract_nested_test_forecast(object, .include_actual = TRUE, .id_subset = NULL)  extract_nested_error_report(object)  extract_nested_best_model_report(object)  extract_nested_future_forecast(   object,   .include_actual = TRUE,   .id_subset = NULL )  extract_nested_modeltime_table(object, .row_id = 1)  extract_nested_train_split(object, .row_id = 1)  extract_nested_test_split(object, .row_id = 1)"},{"path":"https://business-science.github.io/modeltime/reference/log_extractors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log Extractor Functions for Modeltime Nested Tables — log_extractors","text":"object nested modeltime table .include_actual Whether include actual data extracted forecast. Default: TRUE. .id_subset Can supply vector id's extract forcasts one id's, rather extracting forecasts. NULL, extracts forecasts id's. .row_id row number extract nested data.","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750.html","id":null,"dir":"Reference","previous_headings":"","what":"The 750th Monthly Time Series used in the M4 Competition — m750","title":"The 750th Monthly Time Series used in the M4 Competition — m750","text":"750th Monthly Time Series used M4 Competition","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The 750th Monthly Time Series used in the M4 Competition — m750","text":"","code":"m750"},{"path":"https://business-science.github.io/modeltime/reference/m750.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"The 750th Monthly Time Series used in the M4 Competition — m750","text":"tibble 306 rows 3 variables: id Factor. Unique series identifier date Date. Timestamp information. Monthly format. value Numeric. Value corresponding timestamp.","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"The 750th Monthly Time Series used in the M4 Competition — m750","text":"M4 Competition Website","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The 750th Monthly Time Series used in the M4 Competition — m750","text":"","code":"m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows"},{"path":"https://business-science.github.io/modeltime/reference/m750_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Three (3) Models trained on the M750 Data (Training Set) — m750_models","title":"Three (3) Models trained on the M750 Data (Training Set) — m750_models","text":"Three (3) Models trained M750 Data (Training Set)","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Three (3) Models trained on the M750 Data (Training Set) — m750_models","text":"","code":"m750_models"},{"path":"https://business-science.github.io/modeltime/reference/m750_models.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Three (3) Models trained on the M750 Data (Training Set) — m750_models","text":"time_series_cv object 6 slices Time Series Cross Validation resamples made training(m750_splits)","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750_models.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Three (3) Models trained on the M750 Data (Training Set) — m750_models","text":"","code":"library(modeltime) m750_models <- modeltime_table(     wflw_fit_arima,     wflw_fit_prophet,     wflw_fit_glmnet )"},{"path":"https://business-science.github.io/modeltime/reference/m750_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Three (3) Models trained on the M750 Data (Training Set) — m750_models","text":"","code":"library(modeltime)  m750_models #> # Modeltime Table #> # A tibble: 3 × 3 #>   .model_id .model     .model_desc             #>       <int> <list>     <chr>                   #> 1         1 <workflow> ARIMA(0,1,1)(0,1,1)[12] #> 2         2 <workflow> PROPHET                 #> 3         3 <workflow> GLMNET"},{"path":"https://business-science.github.io/modeltime/reference/m750_splits.html","id":null,"dir":"Reference","previous_headings":"","what":"The results of train/test splitting the M750 Data — m750_splits","title":"The results of train/test splitting the M750 Data — m750_splits","text":"results train/test splitting M750 Data","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750_splits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The results of train/test splitting the M750 Data — m750_splits","text":"","code":"m750_splits"},{"path":"https://business-science.github.io/modeltime/reference/m750_splits.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"The results of train/test splitting the M750 Data — m750_splits","text":"rsplit object split approximately 23.5-years training data 2-years testing data","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750_splits.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"The results of train/test splitting the M750 Data — m750_splits","text":"","code":"library(timetk) m750_splits <- time_series_split(m750, assess = \"2 years\", cumulative = TRUE)"},{"path":"https://business-science.github.io/modeltime/reference/m750_splits.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The results of train/test splitting the M750 Data — m750_splits","text":"","code":"library(rsample)  m750_splits #> <Analysis/Assess/Total> #> <282/24/306>  training(m750_splits) #> # A tibble: 282 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 272 more rows"},{"path":"https://business-science.github.io/modeltime/reference/m750_training_resamples.html","id":null,"dir":"Reference","previous_headings":"","what":"The Time Series Cross Validation Resamples the M750 Data (Training Set) — m750_training_resamples","title":"The Time Series Cross Validation Resamples the M750 Data (Training Set) — m750_training_resamples","text":"Time Series Cross Validation Resamples M750 Data (Training Set)","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750_training_resamples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Time Series Cross Validation Resamples the M750 Data (Training Set) — m750_training_resamples","text":"","code":"m750_training_resamples"},{"path":"https://business-science.github.io/modeltime/reference/m750_training_resamples.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"The Time Series Cross Validation Resamples the M750 Data (Training Set) — m750_training_resamples","text":"time_series_cv object 6 slices Time Series Cross Validation resamples made training(m750_splits)","code":""},{"path":"https://business-science.github.io/modeltime/reference/m750_training_resamples.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"The Time Series Cross Validation Resamples the M750 Data (Training Set) — m750_training_resamples","text":"","code":"library(timetk) m750_training_resamples <- time_series_cv(     data        = training(m750_splits),     assess      = \"2 years\",     skip        = \"2 years\",     cumulative  = TRUE,     slice_limit = 6 )"},{"path":"https://business-science.github.io/modeltime/reference/m750_training_resamples.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Time Series Cross Validation Resamples the M750 Data (Training Set) — m750_training_resamples","text":"","code":"library(rsample)  m750_training_resamples #> # Time Series Cross Validation Plan  #> # A tibble: 6 × 2 #>   splits           id     #>   <list>           <chr>  #> 1 <split [258/24]> Slice1 #> 2 <split [234/24]> Slice2 #> 3 <split [210/24]> Slice3 #> 4 <split [186/24]> Slice4 #> 5 <split [162/24]> Slice5 #> 6 <split [138/24]> Slice6"},{"path":"https://business-science.github.io/modeltime/reference/maape.data.frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean Arctangent Absolute Percentage Error — maape.data.frame","title":"Mean Arctangent Absolute Percentage Error — maape.data.frame","text":"basically wrapper function TSrepr::maape().","code":""},{"path":"https://business-science.github.io/modeltime/reference/maape.data.frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean Arctangent Absolute Percentage Error — maape.data.frame","text":"","code":"# S3 method for data.frame maape(data, truth, estimate, na_rm = TRUE, ...)"},{"path":"https://business-science.github.io/modeltime/reference/maape.data.frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean Arctangent Absolute Percentage Error — maape.data.frame","text":"data data.frame containing truth estimate columns. truth column identifier true results (numeric). estimate column identifier predicted results (also numeric). na_rm use...NA values managed TSrepr::maape ... currently use","code":""},{"path":"https://business-science.github.io/modeltime/reference/maape.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean Arctangent Absolute Percentage Error — maape","title":"Mean Arctangent Absolute Percentage Error — maape","text":"Useful MAPE returns Inf typically due intermittent data containing zeros. wrapper function TSrepr::maape().","code":""},{"path":"https://business-science.github.io/modeltime/reference/maape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean Arctangent Absolute Percentage Error — maape","text":"","code":"maape(data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/maape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean Arctangent Absolute Percentage Error — maape","text":"data data.frame containing truth estimate columns. ... currently use.","code":""},{"path":"https://business-science.github.io/modeltime/reference/maape_vec.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean Arctangent Absolute Percentage Error — maape_vec","title":"Mean Arctangent Absolute Percentage Error — maape_vec","text":"basically wrapper function TSrepr::maape().","code":""},{"path":"https://business-science.github.io/modeltime/reference/maape_vec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean Arctangent Absolute Percentage Error — maape_vec","text":"","code":"maape_vec(truth, estimate, na_rm = TRUE, ...)"},{"path":"https://business-science.github.io/modeltime/reference/maape_vec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean Arctangent Absolute Percentage Error — maape_vec","text":"truth column identifier true results (numeric). estimate column identifier predicted results (also numeric). na_rm use...NA values managed TSrepr::maape ... currently use","code":""},{"path":"https://business-science.github.io/modeltime/reference/make_ts_splits.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Time Series Train/Test Split Indicies — make_ts_splits","title":"Generate a Time Series Train/Test Split Indicies — make_ts_splits","text":"Makes fast train/test split indicies time series.","code":""},{"path":"https://business-science.github.io/modeltime/reference/make_ts_splits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Time Series Train/Test Split Indicies — make_ts_splits","text":"","code":"make_ts_splits(.data, .length_test, .length_train = NULL)"},{"path":"https://business-science.github.io/modeltime/reference/make_ts_splits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Time Series Train/Test Split Indicies — make_ts_splits","text":".data data frame containing ordered time seried data (ascending) .length_test number rows include test set .length_train Optional. number rows include training set. NULL, returns remaining row indicies.","code":""},{"path":"https://business-science.github.io/modeltime/reference/make_ts_splits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Time Series Train/Test Split Indicies — make_ts_splits","text":"list containing train_idx test_idx","code":""},{"path":"https://business-science.github.io/modeltime/reference/mdl_time_forecast.html","id":null,"dir":"Reference","previous_headings":"","what":"Modeltime Forecast Helpers — mdl_time_forecast","title":"Modeltime Forecast Helpers — mdl_time_forecast","text":"Used low-level forecasting modeltime, parnsip workflow models. functions intended user use.","code":""},{"path":"https://business-science.github.io/modeltime/reference/mdl_time_forecast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Modeltime Forecast Helpers — mdl_time_forecast","text":"","code":"mdl_time_forecast(   object,   calibration_data,   new_data = NULL,   h = NULL,   actual_data = NULL,   bind_actual = TRUE,   keep_data = FALSE,   arrange_index = FALSE,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/mdl_time_forecast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Modeltime Forecast Helpers — mdl_time_forecast","text":"object Modeltime Table calibration_data Data calibrated testing set new_data tibble containing future information forecast. NULL, forecasts calibration data. h forecast horizon (can used instead new_data time series exogenous regressors). Extends calibration data h periods future. actual_data Reference data combined output tibble given .key = \"actual\" bind_actual Logical. Whether skip rowwise binding `actual_data`` keep_data Whether keep new_data actual_data extra columns results. can useful important feature new_data actual_data needed forecasting. Default: FALSE. arrange_index Whether sort index rowwise chronological order (oldest newest) keep original order data. Default: FALSE. ... currently used","code":""},{"path":"https://business-science.github.io/modeltime/reference/mdl_time_forecast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Modeltime Forecast Helpers — mdl_time_forecast","text":"tibble forecast features","code":""},{"path":"https://business-science.github.io/modeltime/reference/mdl_time_refit.html","id":null,"dir":"Reference","previous_headings":"","what":"Modeltime Refit Helpers — mdl_time_refit","title":"Modeltime Refit Helpers — mdl_time_refit","text":"Used low-level refitting modeltime, parnsip workflow models functions intended user use.","code":""},{"path":"https://business-science.github.io/modeltime/reference/mdl_time_refit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Modeltime Refit Helpers — mdl_time_refit","text":"","code":"mdl_time_refit(object, data, ..., control = NULL)"},{"path":"https://business-science.github.io/modeltime/reference/mdl_time_refit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Modeltime Refit Helpers — mdl_time_refit","text":"object Modeltime Table data tibble contains data retrain model(s) using. ... Additional arguments control refitting. Ensemble Model Spec (modeltime.ensemble): making meta-learner modeltime.ensemble::ensemble_model_spec(), used pass resamples argument containing results modeltime.resample::modeltime_fit_resamples(). control Used control verbosity parallel processing. See control_refit().","code":""},{"path":"https://business-science.github.io/modeltime/reference/mdl_time_refit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Modeltime Refit Helpers — mdl_time_refit","text":"tibble forecast features","code":""},{"path":"https://business-science.github.io/modeltime/reference/metric_sets.html","id":null,"dir":"Reference","previous_headings":"","what":"Forecast Accuracy Metrics Sets — metric_sets","title":"Forecast Accuracy Metrics Sets — metric_sets","text":"wrapper metric_set() several common forecast / regression accuracy metrics included. default time series accuracy metrics used modeltime_accuracy().","code":""},{"path":"https://business-science.github.io/modeltime/reference/metric_sets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forecast Accuracy Metrics Sets — metric_sets","text":"","code":"default_forecast_accuracy_metric_set(...)  extended_forecast_accuracy_metric_set(...)"},{"path":"https://business-science.github.io/modeltime/reference/metric_sets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forecast Accuracy Metrics Sets — metric_sets","text":"... Add additional yardstick metrics","code":""},{"path":"https://business-science.github.io/modeltime/reference/metric_sets.html","id":"default-forecast-accuracy-metric-set","dir":"Reference","previous_headings":"","what":"Default Forecast Accuracy Metric Set","title":"Forecast Accuracy Metrics Sets — metric_sets","text":"primary purpose use default accuracy metrics calculate following forecast accuracy metrics using modeltime_accuracy(): MAE   - Mean absolute error, mae() MAPE  - Mean absolute percentage error, mape() MASE  - Mean absolute scaled error, mase() SMAPE - Symmetric mean absolute percentage error, smape() RMSE  - Root mean squared error, rmse() RSQ   - R-squared, rsq() Adding additional metrics possible via ....","code":""},{"path":"https://business-science.github.io/modeltime/reference/metric_sets.html","id":"extended-forecast-accuracy-metric-set","dir":"Reference","previous_headings":"","what":"Extended Forecast Accuracy Metric Set","title":"Forecast Accuracy Metrics Sets — metric_sets","text":"Extends default metric set adding: MAAPE - Mean Arctangent Absolute Percentage Error, maape(). MAAPE designed intermittent data MAPE returns Inf.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/metric_sets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forecast Accuracy Metrics Sets — metric_sets","text":"","code":"library(tibble) library(dplyr) library(timetk) library(yardstick)  fake_data <- tibble(     y    = c(1:12, 2*1:12),     yhat = c(1 + 1:12, 2*1:12 - 1) )  # ---- HOW IT WORKS ----  # Default Forecast Accuracy Metric Specification default_forecast_accuracy_metric_set() #> # A tibble: 6 × 3 #>   metric class          direction #>   <chr>  <chr>          <chr>     #> 1 mae    numeric_metric minimize  #> 2 mape   numeric_metric minimize  #> 3 mase   numeric_metric minimize  #> 4 smape  numeric_metric minimize  #> 5 rmse   numeric_metric minimize  #> 6 rsq    numeric_metric maximize   # Create a metric summarizer function from the metric set calc_default_metrics <- default_forecast_accuracy_metric_set()  # Apply the metric summarizer to new data calc_default_metrics(fake_data, y, yhat) #> # A tibble: 6 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 mae     standard       1     #> 2 mape    standard      19.4   #> 3 mase    standard       0.535 #> 4 smape   standard      18.0   #> 5 rmse    standard       1     #> 6 rsq     standard       0.979  # ---- ADD MORE PARAMETERS ----  # Can create a version of mase() with seasonality = 12 (monthly) mase12 <- metric_tweak(.name = \"mase12\", .fn = mase, m = 12)  # Add it to the default metric set my_metric_set <- default_forecast_accuracy_metric_set(mase12) my_metric_set #> # A tibble: 7 × 3 #>   metric class          direction #>   <chr>  <chr>          <chr>     #> 1 mae    numeric_metric minimize  #> 2 mape   numeric_metric minimize  #> 3 mase   numeric_metric minimize  #> 4 smape  numeric_metric minimize  #> 5 rmse   numeric_metric minimize  #> 6 rsq    numeric_metric maximize  #> 7 mase12 numeric_metric minimize   # Apply the newly created metric set my_metric_set(fake_data, y, yhat) #> # A tibble: 7 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 mae     standard       1     #> 2 mape    standard      19.4   #> 3 mase    standard       0.535 #> 4 smape   standard      18.0   #> 5 rmse    standard       1     #> 6 rsq     standard       0.979 #> 7 mase12  standard       0.154"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Accuracy Metrics — modeltime_accuracy","title":"Calculate Accuracy Metrics — modeltime_accuracy","text":"wrapper yardstick simplifies time series regression accuracy metric calculations fitted workflow (trained workflow) model_fit (trained parsnip model).","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Accuracy Metrics — modeltime_accuracy","text":"","code":"modeltime_accuracy(   object,   new_data = NULL,   metric_set = default_forecast_accuracy_metric_set(),   acc_by_id = FALSE,   quiet = TRUE,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Accuracy Metrics — modeltime_accuracy","text":"object Modeltime Table new_data tibble predict calculate residuals . provided, overrides calibration data. metric_set yardstick::metric_set() used summarize one forecast accuracy (regression) metrics. acc_by_id global local model accuracy produced? (Default: FALSE) FALSE, global model accuracy provided. TRUE, local accuracy provided group-wise time series ID. enable local accuracy, id must provided modeltime_calibrate(). quiet Hide errors (TRUE, default), display occur? ... new_data provided, parameters passed modeltime_calibrate()","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_accuracy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Accuracy Metrics — modeltime_accuracy","text":"tibble accuracy estimates.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_accuracy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Accuracy Metrics — modeltime_accuracy","text":"following accuracy metrics included default via default_forecast_accuracy_metric_set(): MAE - Mean absolute error, mae() MAPE - Mean absolute percentage error, mape() MASE  - Mean absolute scaled error, mase() SMAPE - Symmetric mean absolute percentage error, smape() RMSE  - Root mean squared error, rmse() RSQ   - R-squared, rsq()","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_accuracy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Accuracy Metrics — modeltime_accuracy","text":"","code":"library(tidymodels) library(tidyverse) library(lubridate) library(timetk) library(modeltime)   # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # --- MODELS ---  # Model 1: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  models_tbl <- modeltime_table(     model_fit_prophet )  # ---- ACCURACY ----  models_tbl %>%     modeltime_calibrate(new_data = testing(splits)) %>%     modeltime_accuracy(         metric_set = metric_set(mae, rmse, rsq)     ) #> # A tibble: 1 × 6 #>   .model_id .model_desc .type   mae  rmse   rsq #>       <int> <chr>       <chr> <dbl> <dbl> <dbl> #> 1         1 PROPHET     Test   260.  353. 0.813"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_calibrate.html","id":null,"dir":"Reference","previous_headings":"","what":"Preparation for forecasting — modeltime_calibrate","title":"Preparation for forecasting — modeltime_calibrate","text":"Calibration sets stage accuracy forecast confidence computing predictions residuals sample data.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_calibrate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preparation for forecasting — modeltime_calibrate","text":"","code":"modeltime_calibrate(object, new_data, id = NULL, quiet = TRUE, ...)"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_calibrate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preparation for forecasting — modeltime_calibrate","text":"object fitted model object either: modeltime table created using modeltime_table() workflow fit fit.workflow() parsnip model fit using fit.model_spec() new_data test data set tibble containing future information (timestamps actual values). id quoted column name containing identifier column identifying time series grouped. quiet Hide errors (TRUE, default), display occur? ... Additional arguments passed modeltime_forecast().","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_calibrate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preparation for forecasting — modeltime_calibrate","text":"Modeltime Table (mdl_time_tbl) nested .calibration_data added","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_calibrate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preparation for forecasting — modeltime_calibrate","text":"results calibration used : Forecast Confidence Interval Estimation: sample residual data used calculate confidence interval. Refer modeltime_forecast(). Accuracy Calculations: sample actual prediction values used calculate performance metrics. Refer modeltime_accuracy() calibration steps include: Modeltime Table, objects converted Modeltime Tables internally Two Columns added: .type: Indicates sample type. : \"Test\" predicted, \"Fitted\" residuals stored modeling. .calibration_data: Contains tibble Timestamps, Actual Values, Predictions Residuals calculated new_data (Test Data) id provided, contain 5th column identifier variable.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_calibrate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preparation for forecasting — modeltime_calibrate","text":"","code":"library(tidyverse) library(lubridate) library(timetk) library(parsnip) library(rsample) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # --- MODELS ---  # Model 1: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  models_tbl <- modeltime_table(     model_fit_prophet )  # ---- CALIBRATE ----  calibration_tbl <- models_tbl %>%     modeltime_calibrate(         new_data = testing(splits)     )  # ---- ACCURACY ----  calibration_tbl %>%     modeltime_accuracy() #> # A tibble: 1 × 9 #>   .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq #>       <int> <chr>       <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1         1 PROPHET     Test   260.  2.62 0.773  2.56  353. 0.813  # ---- FORECAST ----  calibration_tbl %>%     modeltime_forecast(         new_data    = testing(splits),         actual_data = m750     ) #> # A tibble: 368 × 7 #>    .model_id .model_desc .key   .index     .value .conf_lo .conf_hi #>        <int> <chr>       <fct>  <date>      <dbl>    <dbl>    <dbl> #>  1        NA ACTUAL      actual 1990-01-01   6370       NA       NA #>  2        NA ACTUAL      actual 1990-02-01   6430       NA       NA #>  3        NA ACTUAL      actual 1990-03-01   6520       NA       NA #>  4        NA ACTUAL      actual 1990-04-01   6580       NA       NA #>  5        NA ACTUAL      actual 1990-05-01   6620       NA       NA #>  6        NA ACTUAL      actual 1990-06-01   6690       NA       NA #>  7        NA ACTUAL      actual 1990-07-01   6000       NA       NA #>  8        NA ACTUAL      actual 1990-08-01   5450       NA       NA #>  9        NA ACTUAL      actual 1990-09-01   6480       NA       NA #> 10        NA ACTUAL      actual 1990-10-01   6820       NA       NA #> # … with 358 more rows"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_fit_workflowset.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a workflowset object to one or multiple time series — modeltime_fit_workflowset","title":"Fit a workflowset object to one or multiple time series — modeltime_fit_workflowset","text":"wrapper fit() takes workflowset object fits model one multiple time series either sequentially parallel.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_fit_workflowset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a workflowset object to one or multiple time series — modeltime_fit_workflowset","text":"","code":"modeltime_fit_workflowset(   object,   data,   ...,   control = control_fit_workflowset() )"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_fit_workflowset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a workflowset object to one or multiple time series — modeltime_fit_workflowset","text":"object workflow_set object, generated workflowsets::workflow_set function. data tibble contains data fit models. ... currently used. control object used modify fitting process. See control_fit_workflowset().","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_fit_workflowset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a workflowset object to one or multiple time series — modeltime_fit_workflowset","text":"Modeltime Table containing one fitted models.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/modeltime_fit_workflowset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a workflowset object to one or multiple time series — modeltime_fit_workflowset","text":"","code":"library(tidymodels) library(modeltime) library(workflowsets) library(tidyverse) library(lubridate) library(timetk)  data_set <- m4_monthly  # SETUP WORKFLOWSETS  rec1 <- recipe(value ~ date + id, data_set) %>%     step_mutate(date_num = as.numeric(date)) %>%     step_mutate(month_lbl = lubridate::month(date, label = TRUE)) %>%     step_dummy(all_nominal(), one_hot = TRUE)  mod1 <- linear_reg() %>% set_engine(\"lm\")  mod2 <- prophet_reg() %>% set_engine(\"prophet\")  wfsets <- workflowsets::workflow_set(     preproc = list(rec1 = rec1),     models  = list(         mod1 = mod1,         mod2 = mod2     ),     cross   = TRUE )  # FIT WORKFLOWSETS # - Returns a Modeltime Table with fitted workflowsets  wfsets %>% modeltime_fit_workflowset(data_set) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this. #> # Modeltime Table #> # A tibble: 2 × 3 #>   .model_id .model     .model_desc #>       <int> <list>     <chr>       #> 1         1 <workflow> REC1_MOD1   #> 2         2 <workflow> REC1_MOD2"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_forecast.html","id":null,"dir":"Reference","previous_headings":"","what":"Forecast future data — modeltime_forecast","title":"Forecast future data — modeltime_forecast","text":"goal modeltime_forecast() simplify process forecasting future data.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_forecast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forecast future data — modeltime_forecast","text":"","code":"modeltime_forecast(   object,   new_data = NULL,   h = NULL,   actual_data = NULL,   conf_interval = 0.95,   conf_by_id = FALSE,   keep_data = FALSE,   arrange_index = FALSE,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_forecast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forecast future data — modeltime_forecast","text":"object Modeltime Table new_data tibble containing future information forecast. NULL, forecasts calibration data. h forecast horizon (can used instead new_data time series exogenous regressors). Extends calibration data h periods future. actual_data Reference data combined output tibble given .key = \"actual\" conf_interval estimated confidence interval based calibration data. designed estimate future confidence --sample prediction error. conf_by_id Whether produce confidence interval estimates ID feature. FALSE, global model confidence interval provided. TRUE, local confidence interval provided group-wise time series ID. enable local confidence interval, id must provided modeltime_calibrate(). keep_data Whether keep new_data actual_data extra columns results. can useful important feature new_data actual_data needed forecasting. Default: FALSE. arrange_index Whether sort index rowwise chronological order (oldest newest) keep original order data. Default: FALSE. ... currently used","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_forecast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forecast future data — modeltime_forecast","text":"tibble predictions time-stamp data. ease plotting calculations, column names transformed : .key: Values labeled either \"prediction\" \"actual\" .index: timestamp index. .value: value forecasted. Additionally, Modeltime Table previously calibrated using modeltime_calibrate(), gain confidence intervals. .conf_lo: lower limit confidence interval. .conf_hi: upper limit confidence interval. Additional descriptive columns included: .model_id: Model ID Modeltime Table .model_desc: Model Description Modeltime Table Unnecessary columns dropped save space: .model .calibration_data","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_forecast.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forecast future data — modeltime_forecast","text":"modeltime_forecast() function prepares forecast visualization plot_modeltime_forecast(). forecast controlled new_data h, can combined existing data (controlled actual_data). Confidence intervals included incoming Modeltime Table calibrated using modeltime_calibrate(). Otherwise confidence intervals estimated. New Data forecasting can specify future data using new_data. future tibble date column columns xregs extending trained dates exogonous regressors (xregs) used. Forecasting Evaluation Data: default, new_data use .calibration_data new_data provided. equivalent using rsample::testing() getting test data sets. Forecasting Future Data: See timetk::future_frame() creating future tibbles. Xregs: Can used method H (Horizon) forecasting, can specify h. phrase like \"1 year\", extends .calibration_data (1st priority) actual_data (2nd priority) future. Forecasting Future Data: forecasts using h extended calibration data actual_data. Extending .calibration_data - Calibration data given 1st priority, desirable refitting modeltime_refit(). Internally, call made timetk::future_frame() expedite creating new data using date feature. Extending actual_data - h provided, modeltime table calibrated, \"actual_data\" extended future. useful situations want go directly modeltime_table() modeltime_forecast() without calibrating refitting. Xregs: used future data must include new xregs. xregs desired, build future data frame use new_data. Actual Data reference data contains true values time-stamp data. helps visualizing performance forecast vs actual data. h used Modeltime Table calibrated, actual data extended future periods defined h. Confidence Interval Estimation Confidence intervals (.conf_lo, .conf_hi) estimated based normal estimation testing errors (sample) modeltime_calibrate(). --sample error estimates carried applied applied future forecasts. confidence interval can adjusted conf_interval parameter. 80% confidence interval estimates normal (Gaussian distribution) assumes 80% future data fall within upper lower confidence limits. confidence interval mean-adjusted, meaning mean residuals non-zero, confidence interval adjusted widen interval capture difference means. Refitting affect confidence interval since calculated independently refitted model (data smaller sample size). New observations typically improve future accuracy, cases makes --sample confidence intervals conservative. Keep Data Include new data (actual data) extra columns results model forecasts. can helpful new data includes information useful forecasts. example forecasting Panel Data new data contains ID features related time series group forecast belongs . Arrange Index default, modeltime_forecast() keeps original order data. desired, user can sort output .key, .model_id .index.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_forecast.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forecast future data — modeltime_forecast","text":"","code":"library(tidyverse) library(lubridate) library(timetk) library(parsnip) library(rsample) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)  # --- MODELS ---  # Model 1: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  models_tbl <- modeltime_table(     model_fit_prophet )  # ---- CALIBRATE ----  calibration_tbl <- models_tbl %>%     modeltime_calibrate(new_data = testing(splits))  # ---- ACCURACY ----  calibration_tbl %>%     modeltime_accuracy() #> # A tibble: 1 × 9 #>   .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq #>       <int> <chr>       <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1         1 PROPHET     Test   172.  1.65 0.587  1.65  230. 0.882  # ---- FUTURE FORECAST ----  calibration_tbl %>%     modeltime_forecast(         new_data    = testing(splits),         actual_data = m750     ) #> # A tibble: 337 × 7 #>    .model_id .model_desc .key   .index     .value .conf_lo .conf_hi #>        <int> <chr>       <fct>  <date>      <dbl>    <dbl>    <dbl> #>  1        NA ACTUAL      actual 1990-01-01   6370       NA       NA #>  2        NA ACTUAL      actual 1990-02-01   6430       NA       NA #>  3        NA ACTUAL      actual 1990-03-01   6520       NA       NA #>  4        NA ACTUAL      actual 1990-04-01   6580       NA       NA #>  5        NA ACTUAL      actual 1990-05-01   6620       NA       NA #>  6        NA ACTUAL      actual 1990-06-01   6690       NA       NA #>  7        NA ACTUAL      actual 1990-07-01   6000       NA       NA #>  8        NA ACTUAL      actual 1990-08-01   5450       NA       NA #>  9        NA ACTUAL      actual 1990-09-01   6480       NA       NA #> 10        NA ACTUAL      actual 1990-10-01   6820       NA       NA #> # … with 327 more rows  # ---- ALTERNATIVE: FORECAST WITHOUT CONFIDENCE INTERVALS ---- # Skips Calibration Step, No Confidence Intervals  models_tbl %>%     modeltime_forecast(         new_data    = testing(splits),         actual_data = m750     ) #> # A tibble: 337 × 5 #>    .model_id .model_desc .key   .index     .value #>        <int> <chr>       <fct>  <date>      <dbl> #>  1        NA ACTUAL      actual 1990-01-01   6370 #>  2        NA ACTUAL      actual 1990-02-01   6430 #>  3        NA ACTUAL      actual 1990-03-01   6520 #>  4        NA ACTUAL      actual 1990-04-01   6580 #>  5        NA ACTUAL      actual 1990-05-01   6620 #>  6        NA ACTUAL      actual 1990-06-01   6690 #>  7        NA ACTUAL      actual 1990-07-01   6000 #>  8        NA ACTUAL      actual 1990-08-01   5450 #>  9        NA ACTUAL      actual 1990-09-01   6480 #> 10        NA ACTUAL      actual 1990-10-01   6820 #> # … with 327 more rows  # ---- KEEP NEW DATA WITH FORECAST ---- # Keeps the new data. Useful if new data has information #  like ID features that should be kept with the forecast data  calibration_tbl %>%     modeltime_forecast(         new_data      = testing(splits),         keep_data     = TRUE     ) #> # A tibble: 31 × 10 #>    .model_id .model_desc .key       .index     .value .conf_lo .conf_hi id    #>        <int> <chr>       <fct>      <date>      <dbl>    <dbl>    <dbl> <fct> #>  1         1 PROPHET     prediction 2012-12-01 10603.   10149.   11058. M750  #>  2         1 PROPHET     prediction 2013-01-01 10517.   10062.   10971. M750  #>  3         1 PROPHET     prediction 2013-02-01 10557.   10102.   11011. M750  #>  4         1 PROPHET     prediction 2013-03-01 10694.   10240.   11149. M750  #>  5         1 PROPHET     prediction 2013-04-01 10724.   10269.   11178. M750  #>  6         1 PROPHET     prediction 2013-05-01 10725.   10271.   11180. M750  #>  7         1 PROPHET     prediction 2013-06-01 10566.   10112.   11021. M750  #>  8         1 PROPHET     prediction 2013-07-01  9689.    9234.   10143. M750  #>  9         1 PROPHET     prediction 2013-08-01  9540.    9086.    9995. M750  #> 10         1 PROPHET     prediction 2013-09-01 10169.    9714.   10624. M750  #> # … with 21 more rows, and 2 more variables: date <date>, value <dbl>"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Tidymodels Workflows to Nested Time Series — modeltime_nested_fit","title":"Fit Tidymodels Workflows to Nested Time Series — modeltime_nested_fit","text":"Fits one tidymodels workflow objects nested time series data using following process: Models iteratively fit training splits. Accuracy calculated testing splits logged. Accuracy results can retrieved extract_nested_test_accuracy() model returns error logged. Error logs can retrieved extract_nested_error_report() Forecast predicted testing splits logged. Forecast results can retrieved extract_nested_test_forecast()","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Tidymodels Workflows to Nested Time Series — modeltime_nested_fit","text":"","code":"modeltime_nested_fit(   nested_data,   ...,   model_list = NULL,   metric_set = default_forecast_accuracy_metric_set(),   conf_interval = 0.95,   control = control_nested_fit() )"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Tidymodels Workflows to Nested Time Series — modeltime_nested_fit","text":"nested_data Nested time series data ... Tidymodels workflow objects fit nested time series data. model_list Optionally, list() Tidymodels workflow objects can provided metric_set yardstick::metric_set() used summarize one forecast accuracy (regression) metrics. conf_interval estimated confidence interval based calibration data. designed estimate future confidence --sample prediction error. control Used control verbosity parallel processing. See control_nested_fit().","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_fit.html","id":"preparing-data-for-nested-forecasting","dir":"Reference","previous_headings":"","what":"Preparing Data for Nested Forecasting","title":"Fit Tidymodels Workflows to Nested Time Series — modeltime_nested_fit","text":"Use extend_timeseries(), nest_timeseries(), split_nested_timeseries() preparing data Nested Forecasting. structure must nested data frame, suppplied modeltime_nested_fit(nested_data).","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_fit.html","id":"fitting-models","dir":"Reference","previous_headings":"","what":"Fitting Models","title":"Fit Tidymodels Workflows to Nested Time Series — modeltime_nested_fit","text":"Models must form tidymodels workflow objects. models can provided two ways: Using ... (dots): workflow objects can provided dots. Using model_list parameter: can supply one workflow objects wrapped list().","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_fit.html","id":"controlling-the-fitting-process","dir":"Reference","previous_headings":"","what":"Controlling the fitting process","title":"Fit Tidymodels Workflows to Nested Time Series — modeltime_nested_fit","text":"control object can provided fitting adjust verbosity parallel processing. See control_nested_fit().","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_forecast.html","id":null,"dir":"Reference","previous_headings":"","what":"Modeltime Nested Forecast — modeltime_nested_forecast","title":"Modeltime Nested Forecast — modeltime_nested_forecast","text":"Make new forecast Nested Modeltime Table.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_forecast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Modeltime Nested Forecast — modeltime_nested_forecast","text":"","code":"modeltime_nested_forecast(   object,   h = NULL,   include_actual = TRUE,   conf_interval = 0.95,   id_subset = NULL,   control = control_nested_forecast() )"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_forecast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Modeltime Nested Forecast — modeltime_nested_forecast","text":"object Nested Modeltime Table h forecast horizon. Extends \"trained \" data \"h\" periods future. include_actual Whether include \".actual_data\" part forecast. FALSE, just returns forecast predictions. conf_interval estimated confidence interval based calibration data. designed estimate future confidence --sample prediction error. id_subset sequence ID's modeltime table subset forecasting process. can speed forecasts . control Used control verbosity parallel processing. See control_nested_forecast().","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_forecast.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Modeltime Nested Forecast — modeltime_nested_forecast","text":"function designed help users want make new forecasts created logging process part Nested Modeltime Workflow.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_forecast.html","id":"logged-forecasts","dir":"Reference","previous_headings":"","what":"Logged Forecasts","title":"Modeltime Nested Forecast — modeltime_nested_forecast","text":"logged forecasts can extracted using: extract_nested_future_forecast(): Extracts future forecast created refitting modeltime_nested_refit(). extract_nested_test_forecast(): Extracts test forecast created initial fitting modeltime_nested_fit(). problem forecasts static. user need redo fitting, model selection, refitting process obtain new forecasts. modeltime_nested_forecast() exists. can create new forecast without retraining models.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_forecast.html","id":"nested-forecasts","dir":"Reference","previous_headings":"","what":"Nested Forecasts","title":"Modeltime Nested Forecast — modeltime_nested_forecast","text":"main arguments h, horizon specifies far future make new forecast. h = NULL, logged forecast returned h = 12, new forecast generated extends series 12-periods future. h = \"2 years\", new forecast generated extends series 2-years future. Use id_subset filter Nested Modeltime Table object just time series interest. Use conf_interval override logged confidence interval. Note effect h = NULL logged forecasts returned. sure provide h want update confidence interval. Use control argument apply verbosity forecasting process run forecasts parallel. Generally, parallel better many forecasts generated.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_refit.html","id":null,"dir":"Reference","previous_headings":"","what":"Refits a Nested Modeltime Table — modeltime_nested_refit","title":"Refits a Nested Modeltime Table — modeltime_nested_refit","text":"Refits Nested Modeltime Table actual data using following process: Models iteratively refit .actual_data. model returns error logged. Errors can retrieved extract_nested_error_report() Forecast predicted future_data logged. Forecast can retrieved extract_nested_future_forecast()","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_refit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Refits a Nested Modeltime Table — modeltime_nested_refit","text":"","code":"modeltime_nested_refit(object, control = control_nested_refit())"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_refit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Refits a Nested Modeltime Table — modeltime_nested_refit","text":"object Nested Modeltime Table control Used control verbosity parallel processing. See control_nested_refit().","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_select_best.html","id":null,"dir":"Reference","previous_headings":"","what":"Select the Best Models from Nested Modeltime Table — modeltime_nested_select_best","title":"Select the Best Models from Nested Modeltime Table — modeltime_nested_select_best","text":"Finds best models time series group Nested Modeltime Table using metric user specifies. Logs best results, can accessed extract_nested_best_model_report() filter_test_forecasts = TRUE, updates test forecast log, can accessed extract_nested_test_forecast()","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_select_best.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select the Best Models from Nested Modeltime Table — modeltime_nested_select_best","text":"","code":"modeltime_nested_select_best(   object,   metric = \"rmse\",   minimize = TRUE,   filter_test_forecasts = TRUE )"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_nested_select_best.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select the Best Models from Nested Modeltime Table — modeltime_nested_select_best","text":"object Nested Modeltime Table metric metric minimize maximize. default available metrics : \"rmse\" (default) \"mae\" \"mape\" \"mase\" \"smape\" \"rsq\" minimize Whether minimize maximize. Default: TRUE (minimize). filter_test_forecasts Whether update test forecast log filter best forecasts. Default: TRUE.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_refit.html","id":null,"dir":"Reference","previous_headings":"","what":"Refit one or more trained models to new data — modeltime_refit","title":"Refit one or more trained models to new data — modeltime_refit","text":"wrapper fit() takes Modeltime Table retrains model new data re-using parameters preprocessing steps used training process.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_refit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Refit one or more trained models to new data — modeltime_refit","text":"","code":"modeltime_refit(object, data, ..., control = control_refit())"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_refit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Refit one or more trained models to new data — modeltime_refit","text":"object Modeltime Table data tibble contains data retrain model(s) using. ... Additional arguments control refitting. Ensemble Model Spec (modeltime.ensemble): making meta-learner modeltime.ensemble::ensemble_model_spec(), used pass resamples argument containing results modeltime.resample::modeltime_fit_resamples(). control Used control verbosity parallel processing. See control_refit().","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_refit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Refit one or more trained models to new data — modeltime_refit","text":"Modeltime Table containing one re-trained models.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_refit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Refit one or more trained models to new data — modeltime_refit","text":"Refitting important step prior forecasting time series models. modeltime_refit() function makes easy recycle models, retraining new data. Recycling Parameters Parameters recycled retraining using following criteria: Automated models (e.g. \"auto arima\") parameters recalculated. Non-automated models (e.g. \"arima\") parameters preserved. preprocessing steps reused data Refit modeltime_refit() function used retrain models trained fit(). Refit XY XY format supported time.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/modeltime_refit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Refit one or more trained models to new data — modeltime_refit","text":"","code":"library(tidyverse) library(lubridate) library(timetk) library(parsnip) library(rsample) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)  # --- MODELS ---  model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  models_tbl <- modeltime_table(     model_fit_prophet )  # ---- CALIBRATE ---- # - Calibrate on training data set  calibration_tbl <- models_tbl %>%     modeltime_calibrate(new_data = testing(splits))   # ---- REFIT ---- # - Refit on full data set  refit_tbl <- calibration_tbl %>%     modeltime_refit(m750) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this."},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Residuals Information — modeltime_residuals","title":"Extract Residuals Information — modeltime_residuals","text":"convenience function unnest model residuals","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Residuals Information — modeltime_residuals","text":"","code":"modeltime_residuals(object, new_data = NULL, quiet = TRUE, ...)"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Residuals Information — modeltime_residuals","text":"object Modeltime Table new_data tibble predict calculate residuals . provided, overrides calibration data. quiet Hide errors (TRUE, default), display occur? ... currently used.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Residuals Information — modeltime_residuals","text":"tibble residuals.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Residuals Information — modeltime_residuals","text":"","code":"library(tidyverse) library(lubridate) library(timetk) library(parsnip) library(rsample)  # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)  # --- MODELS ---  # Model 1: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  models_tbl <- modeltime_table(     model_fit_prophet )  # ---- RESIDUALS ----  # In-Sample models_tbl %>%     modeltime_calibrate(new_data = training(splits)) %>%     modeltime_residuals() %>%     plot_modeltime_residuals(.interactive = FALSE)   # Out-of-Sample models_tbl %>%     modeltime_calibrate(new_data = testing(splits)) %>%     modeltime_residuals() %>%     plot_modeltime_residuals(.interactive = FALSE)"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Statistical Tests to Residuals — modeltime_residuals_test","title":"Apply Statistical Tests to Residuals — modeltime_residuals_test","text":"convenience function calculate statistical tests residuals models. Currently, following statistics calculated: shapiro.test check normality residuals, box-pierce ljung-box tests durbin watson test check autocorrelation residuals. cases p-values returned.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Statistical Tests to Residuals — modeltime_residuals_test","text":"","code":"modeltime_residuals_test(object, new_data = NULL, lag = 1, fitdf = 0, ...)"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Statistical Tests to Residuals — modeltime_residuals_test","text":"object tibble extracted modeltime::modeltime_residuals(). new_data tibble predict calculate residuals . provided, overrides calibration data. lag statistic based lag autocorrelation coefficients. Default: 1 (Applies Box-Pierce, Ljung-Box, Durbin-Watson Tests) fitdf Number degrees freedom subtracted. Default: 0 (Applies Box-Pierce Ljung-Box Tests) ... currently used","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Statistical Tests to Residuals — modeltime_residuals_test","text":"tibble p-values calculated statistical tests.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals_test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Statistical Tests to Residuals — modeltime_residuals_test","text":"Shapiro-Wilk Test Shapiro-Wilk tests Normality residuals. Null Hypothesis residuals normally distributed. low P-Value given significance level indicates values Normally Distributed. p-value > 0.05 (good), implies distribution data significantly different normal distribution. words, can assume normality. Box-Pierce Ljung-Box Tests Tests Ljung-Box Box-Pierce tests methods test absense autocorrelation residuals. low p-value given significance level indicates values autocorrelated. p-value > 0.05 (good), implies residuals data independent. words, can assume residuals autocorrelated. information parameters associated Box Pierce Ljung Box tests check ?Box.Test Durbin-Watson Test Durbin-Watson test method tests absense autocorrelation residuals. Durbin Watson test reports test statistic, value 0 4, : 2 autocorrelation (good) 0 <2 positive autocorrelation (common time series data) >2 4 negative autocorrelation (less common time series data)","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/modeltime_residuals_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Statistical Tests to Residuals — modeltime_residuals_test","text":"","code":"library(tidyverse) library(lubridate) library(timetk) library(parsnip) library(rsample)  # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)  # --- MODELS ---  # Model 1: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  models_tbl <- modeltime_table(     model_fit_prophet )  # ---- RESIDUALS ----  # In-Sample models_tbl %>%     modeltime_calibrate(new_data = training(splits)) %>%     modeltime_residuals() %>%     modeltime_residuals_test() #> # A tibble: 1 × 6 #>   .model_id .model_desc shapiro_wilk box_pierce ljung_box durbin_watson #>       <int> <chr>              <dbl>      <dbl>     <dbl>         <dbl> #> 1         1 PROPHET        0.0000450          0         0         0.924  # Out-of-Sample models_tbl %>%     modeltime_calibrate(new_data = testing(splits)) %>%     modeltime_residuals() %>%     modeltime_residuals_test() #> # A tibble: 1 × 6 #>   .model_id .model_desc shapiro_wilk box_pierce ljung_box durbin_watson #>       <int> <chr>              <dbl>      <dbl>     <dbl>         <dbl> #> 1         1 PROPHET          0.00169      0.197     0.176          1.37"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Scale forecast analysis with a Modeltime Table — modeltime_table","title":"Scale forecast analysis with a Modeltime Table — modeltime_table","text":"Designed perform forecasts scale using models created modeltime, parsnip, workflows, regression modeling extensions tidymodels ecosystem.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scale forecast analysis with a Modeltime Table — modeltime_table","text":"","code":"modeltime_table(...)  as_modeltime_table(.l)"},{"path":"https://business-science.github.io/modeltime/reference/modeltime_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scale forecast analysis with a Modeltime Table — modeltime_table","text":"... Fitted parsnip model workflow objects .l list containing fitted parsnip model workflow objects","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_table.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Scale forecast analysis with a Modeltime Table — modeltime_table","text":"modeltime_table(): Creates table models Validates objects models (parsnip workflows objects) models fitted (trained) Provides ID Description models as_modeltime_table(): Converts list models modeltime table. Useful programatically creating Modeltime Tables models stored list.","code":""},{"path":"https://business-science.github.io/modeltime/reference/modeltime_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scale forecast analysis with a Modeltime Table — modeltime_table","text":"","code":"library(tidyverse) library(lubridate) library(timetk) library(parsnip) library(rsample) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)  # --- MODELS ---  # Model 1: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  # Make a Modeltime Table models_tbl <- modeltime_table(     model_fit_prophet )  # Can also convert a list of models list(model_fit_prophet) %>%     as_modeltime_table() #> # Modeltime Table #> # A tibble: 1 × 3 #>   .model_id .model   .model_desc #>       <int> <list>   <chr>       #> 1         1 <fit[+]> PROPHET      # ---- CALIBRATE ----  calibration_tbl <- models_tbl %>%     modeltime_calibrate(new_data = testing(splits))  # ---- ACCURACY ----  calibration_tbl %>%     modeltime_accuracy() #> # A tibble: 1 × 9 #>   .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq #>       <int> <chr>       <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1         1 PROPHET     Test   172.  1.65 0.587  1.65  230. 0.882  # ---- FORECAST ----  calibration_tbl %>%     modeltime_forecast(         new_data    = testing(splits),         actual_data = m750     ) #> # A tibble: 337 × 7 #>    .model_id .model_desc .key   .index     .value .conf_lo .conf_hi #>        <int> <chr>       <fct>  <date>      <dbl>    <dbl>    <dbl> #>  1        NA ACTUAL      actual 1990-01-01   6370       NA       NA #>  2        NA ACTUAL      actual 1990-02-01   6430       NA       NA #>  3        NA ACTUAL      actual 1990-03-01   6520       NA       NA #>  4        NA ACTUAL      actual 1990-04-01   6580       NA       NA #>  5        NA ACTUAL      actual 1990-05-01   6620       NA       NA #>  6        NA ACTUAL      actual 1990-06-01   6690       NA       NA #>  7        NA ACTUAL      actual 1990-07-01   6000       NA       NA #>  8        NA ACTUAL      actual 1990-08-01   5450       NA       NA #>  9        NA ACTUAL      actual 1990-09-01   6480       NA       NA #> 10        NA ACTUAL      actual 1990-10-01   6820       NA       NA #> # … with 327 more rows"},{"path":"https://business-science.github.io/modeltime/reference/naive_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level NAIVE Forecast — naive_fit_impl","title":"Low-Level NAIVE Forecast — naive_fit_impl","text":"Low-Level NAIVE Forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/naive_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level NAIVE Forecast — naive_fit_impl","text":"","code":"naive_fit_impl(x, y, id = NULL, seasonal_period = \"auto\", ...)"},{"path":"https://business-science.github.io/modeltime/reference/naive_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level NAIVE Forecast — naive_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit id optional ID feature identify different time series. quoted name. seasonal_period used NAIVE forecast consistency SNAIVE ... currently used","code":""},{"path":"https://business-science.github.io/modeltime/reference/naive_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for NAIVE Models — naive_predict_impl","title":"Bridge prediction function for NAIVE Models — naive_predict_impl","text":"Bridge prediction function NAIVE Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/naive_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for NAIVE Models — naive_predict_impl","text":"","code":"naive_predict_impl(object, new_data)"},{"path":"https://business-science.github.io/modeltime/reference/naive_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for NAIVE Models — naive_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame.","code":""},{"path":"https://business-science.github.io/modeltime/reference/naive_reg.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for NAIVE Forecast Models — naive_reg","title":"General Interface for NAIVE Forecast Models — naive_reg","text":"naive_reg() way generate specification NAIVE SNAIVE model fitting allows model created using different packages.","code":""},{"path":"https://business-science.github.io/modeltime/reference/naive_reg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for NAIVE Forecast Models — naive_reg","text":"","code":"naive_reg(mode = \"regression\", id = NULL, seasonal_period = NULL)"},{"path":"https://business-science.github.io/modeltime/reference/naive_reg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for NAIVE Forecast Models — naive_reg","text":"mode single character string type model. possible value model \"regression\". id optional quoted column name (e.g. \"id\") identifying multiple time series (.e. panel data). seasonal_period SNAIVE . seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details .","code":""},{"path":"https://business-science.github.io/modeltime/reference/naive_reg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for NAIVE Forecast Models — naive_reg","text":"data given function saved used determine mode model. naive_reg(), mode always \"regression\". model can created using fit() function using following engines: \"naive\" (default) - Performs NAIVE forecast \"snaive\" - Performs Seasonal NAIVE forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/naive_reg.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for NAIVE Forecast Models — naive_reg","text":"naive (default engine) engine uses naive_fit_impl() NAIVE implementation uses last observation forecasts value forward. id can used distinguish multiple time series contained data seasonal_period used provided consistency SNAIVE implementation snaive (default engine) engine uses snaive_fit_impl() SNAIVE implementation uses last seasonal series data forecasts sequence observations forward id can used distinguish multiple time series contained data seasonal_period used determine far back define repeated series. can numeric value (e.g. 28) period (e.g. \"1 month\")","code":""},{"path":"https://business-science.github.io/modeltime/reference/naive_reg.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for NAIVE Forecast Models — naive_reg","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) ID features (Multiple Time Series, Panel Data) id parameter populated using fit() fit_xy() function: ID Example: Suppose 3 features: y (target) date (time stamp), series_id (unique identifer identifies time series data). series_id can passed naive_reg() using fit(): naive_reg(id = \"series_id\") specifes series_id column used identify time series. fit(y ~ date + series_id) pass series_id underlying naive snaive functions. Seasonal Period Specification (snaive) period can non-seasonal (seasonal_period = 1 \"none\") yearly seasonal (e.g. monthly time stamps, seasonal_period = 12, seasonal_period = \"12 months\", seasonal_period = \"yearly\"). 3 ways specify: seasonal_period = \"auto\": seasonal period selected based periodicity data (e.g. 12 monthly) seasonal_period = 12: numeric frequency. example, 12 common monthly data seasonal_period = \"1 year\": time-based phrase. example, \"1 year\" convert 12 monthly data. External Regressors (Xregs) models univariate. xregs used modeling process.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/naive_reg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for NAIVE Forecast Models — naive_reg","text":"","code":"library(dplyr) library(parsnip) library(rsample) library(timetk) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\") m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # ---- NAIVE ----  # Model Spec model_spec <- naive_reg() %>%     set_engine(\"naive\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  10ms  #> NAIVE #> -------- #> Model:  #> # A tibble: 1 × 2 #>   date       value #>   <date>     <dbl> #> 1 2010-04-01  9.29   # ---- SEASONAL NAIVE ----  # Model Spec model_spec <- naive_reg(         id = \"id\",         seasonal_period = 12     ) %>%     set_engine(\"snaive\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date + id, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  15ms  #> SNAIVE [12] #> -------- #> Model:  #> # A tibble: 12 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  2009-05-01  9.27 #>  2 M750  2009-06-01  9.27 #>  3 M750  2009-07-01  9.15 #>  4 M750  2009-08-01  9.19 #>  5 M750  2009-09-01  9.18 #>  6 M750  2009-10-01  9.25 #>  7 M750  2009-11-01  9.26 #>  8 M750  2009-12-01  9.27 #>  9 M750  2010-01-01  9.26 #> 10 M750  2010-02-01  9.26 #> 11 M750  2010-03-01  9.29 #> 12 M750  2010-04-01  9.29"},{"path":"https://business-science.github.io/modeltime/reference/new_modeltime_bridge.html","id":null,"dir":"Reference","previous_headings":"","what":"Constructor for creating modeltime models — new_modeltime_bridge","title":"Constructor for creating modeltime models — new_modeltime_bridge","text":"functions used construct new modeltime bridge functions connect tidymodels infrastructure time-series models containing date date-time features.","code":""},{"path":"https://business-science.github.io/modeltime/reference/new_modeltime_bridge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constructor for creating modeltime models — new_modeltime_bridge","text":"","code":"new_modeltime_bridge(class, models, data, extras = NULL, desc = NULL)"},{"path":"https://business-science.github.io/modeltime/reference/new_modeltime_bridge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constructor for creating modeltime models — new_modeltime_bridge","text":"class class name used creating custom printing messages models list containing one models data data frame (tibble) containing 4 columns: (date column name matches input data), .actual, .fitted, .residuals. extras optional list typically used transferring preprocessing recipes predict method. desc optional model description appear printing modeltime objects","code":""},{"path":"https://business-science.github.io/modeltime/reference/new_modeltime_bridge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Constructor for creating modeltime models — new_modeltime_bridge","text":"","code":"library(stats) library(tidyverse) library(lubridate) library(timetk)  lm_model <- lm(value ~ as.numeric(date) + hour(date) + wday(date, label = TRUE),                data = taylor_30_min)  data = tibble(     date        = taylor_30_min$date, # Important - The column name must match the modeled data     # These are standardized names: .actual, .fitted, .residuals     .actual     = taylor_30_min$value,     .fitted     = lm_model$fitted.values %>% as.numeric(),     .residuals  = lm_model$residuals %>% as.numeric() )  new_modeltime_bridge(     class  = \"lm_time_series_impl\",     models = list(model_1 = lm_model),     data   = data,     extras = NULL ) #> $model_1 #>  #> Call: #> lm(formula = value ~ as.numeric(date) + hour(date) + wday(date,  #>     label = TRUE), data = taylor_30_min) #>  #> Coefficients: #>                (Intercept)            as.numeric(date)   #>                  1.489e+05                  -1.284e-04   #>                 hour(date)  wday(date, label = TRUE).L   #>                  3.919e+02                   6.665e+02   #> wday(date, label = TRUE).Q  wday(date, label = TRUE).C   #>                 -5.952e+03                   5.479e+02   #> wday(date, label = TRUE)^4  wday(date, label = TRUE)^5   #>                 -1.955e+03                   2.482e+02   #> wday(date, label = TRUE)^6   #>                 -8.755e+01   #>  #>"},{"path":"https://business-science.github.io/modeltime/reference/nnetar_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level NNETAR function for translating modeltime to forecast — nnetar_fit_impl","title":"Low-Level NNETAR function for translating modeltime to forecast — nnetar_fit_impl","text":"Low-Level NNETAR function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/nnetar_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level NNETAR function for translating modeltime to forecast — nnetar_fit_impl","text":"","code":"nnetar_fit_impl(   x,   y,   period = \"auto\",   p = 1,   P = 1,   size = 10,   repeats = 20,   decay = 0,   maxit = 100,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/nnetar_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level NNETAR function for translating modeltime to forecast — nnetar_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. p Embedding dimension non-seasonal time series. Number non-seasonal lags used inputs. non-seasonal time series, default optimal number lags (according AIC) linear AR(p) model. seasonal time series, method used applied seasonally adjusted data (stl decomposition). P Number seasonal lags used inputs. size Number nodes hidden layer. Default half number input nodes (including external regressors, given) plus 1. repeats Number networks fit different random starting weights. averaged producing forecasts. decay Parameter weight decay. Default 0. maxit Maximum number iterations. Default 100. ... Additional arguments passed forecast::nnetar","code":""},{"path":"https://business-science.github.io/modeltime/reference/nnetar_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuning Parameters for NNETAR Models — nnetar_params","title":"Tuning Parameters for NNETAR Models — nnetar_params","text":"Tuning Parameters NNETAR Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/nnetar_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning Parameters for NNETAR Models — nnetar_params","text":"","code":"num_networks(range = c(1L, 100L), trans = NULL)"},{"path":"https://business-science.github.io/modeltime/reference/nnetar_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuning Parameters for NNETAR Models — nnetar_params","text":"range two-element vector holding defaults smallest largest possible values, respectively. trans trans object scales package, scales::log10_trans() scales::reciprocal_trans(). provided, default used matches units used range. transformation, NULL.","code":""},{"path":"https://business-science.github.io/modeltime/reference/nnetar_params.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tuning Parameters for NNETAR Models — nnetar_params","text":"main parameters NNETAR models : non_seasonal_ar: Number non-seasonal auto-regressive (AR) lags. Often denoted \"p\" pdq-notation. seasonal_ar: Number seasonal auto-regressive (SAR) lags. Often denoted \"P\" PDQ-notation. hidden_units: integer number units hidden model. num_networks: Number networks fit different random starting weights. averaged producing forecasts. penalty: non-negative numeric value amount weight decay. epochs: integer number training iterations.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/nnetar_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tuning Parameters for NNETAR Models — nnetar_params","text":"","code":"num_networks() #> Number of Neural Networks to Average (quantitative) #> Range: [1, 100]"},{"path":"https://business-science.github.io/modeltime/reference/nnetar_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for ARIMA models — nnetar_predict_impl","title":"Bridge prediction function for ARIMA models — nnetar_predict_impl","text":"Bridge prediction function ARIMA models","code":""},{"path":"https://business-science.github.io/modeltime/reference/nnetar_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for ARIMA models — nnetar_predict_impl","text":"","code":"nnetar_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/nnetar_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for ARIMA models — nnetar_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed forecast::forecast()","code":""},{"path":"https://business-science.github.io/modeltime/reference/nnetar_reg.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for NNETAR Regression Models — nnetar_reg","title":"General Interface for NNETAR Regression Models — nnetar_reg","text":"nnetar_reg() way generate specification NNETAR model fitting allows model created using different packages. Currently package forecast.","code":""},{"path":"https://business-science.github.io/modeltime/reference/nnetar_reg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for NNETAR Regression Models — nnetar_reg","text":"","code":"nnetar_reg(   mode = \"regression\",   seasonal_period = NULL,   non_seasonal_ar = NULL,   seasonal_ar = NULL,   hidden_units = NULL,   num_networks = NULL,   penalty = NULL,   epochs = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/nnetar_reg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for NNETAR Regression Models — nnetar_reg","text":"mode single character string type model. possible value model \"regression\". seasonal_period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details . non_seasonal_ar order non-seasonal auto-regressive (AR) terms. Often denoted \"p\" pdq-notation. seasonal_ar order seasonal auto-regressive (SAR) terms. Often denoted \"P\" PDQ-notation. hidden_units integer number units hidden model. num_networks Number networks fit different random starting weights. averaged producing forecasts. penalty non-negative numeric value amount weight decay. epochs integer number training iterations.","code":""},{"path":"https://business-science.github.io/modeltime/reference/nnetar_reg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for NNETAR Regression Models — nnetar_reg","text":"data given function saved used determine mode model. nnetar_reg(), mode always \"regression\". model can created using fit() function using following engines: \"nnetar\" (default) - Connects forecast::nnetar() Main Arguments main arguments (tuning parameters) model parameters nnetar_reg() function. arguments converted specific names time model fit. options argument can set using set_engine() (See Engine Details ). parameters need modified, update() can used lieu recreating object scratch.","code":""},{"path":"https://business-science.github.io/modeltime/reference/nnetar_reg.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for NNETAR Regression Models — nnetar_reg","text":"standardized parameter names modeltime can mapped original names engine: options can set using set_engine(). nnetar engine uses forecast::nnetar(). Function Parameters: Parameter Notes: xreg - supplied via parsnip / modeltime fit() interface (provide manually). See Fit Details (). size - set 10 default. differs forecast implementation p P - set 1 default. maxit decay nnet::nnet parameters exposed nnetar_reg() interface. key tuning parameters.","code":"## function (y, p, P = 1, size, repeats = 20, xreg = NULL, lambda = NULL,  ##     model = NULL, subset = NULL, scale.inputs = TRUE, x = y, ...)"},{"path":"https://business-science.github.io/modeltime/reference/nnetar_reg.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for NNETAR Regression Models — nnetar_reg","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) Seasonal Period Specification period can non-seasonal (seasonal_period = 1 \"none\") yearly seasonal (e.g. monthly time stamps, seasonal_period = 12, seasonal_period = \"12 months\", seasonal_period = \"yearly\"). 3 ways specify: seasonal_period = \"auto\": seasonal period selected based periodicity data (e.g. 12 monthly) seasonal_period = 12: numeric frequency. example, 12 common monthly data seasonal_period = \"1 year\": time-based phrase. example, \"1 year\" convert 12 monthly data. Univariate (xregs, Exogenous Regressors): univariate analysis, must include date date-time feature. Simply use: Formula Interface (recommended): fit(y ~ date) ignore xreg's. XY Interface: fit_xy(x = data[,\"date\"], y = data$y) ignore xreg's. Multivariate (xregs, Exogenous Regressors) xreg parameter populated using fit() fit_xy() function: factor, ordered factor, numeric data used xregs. Date Date-time variables used xregs character data converted factor. Xreg Example: Suppose 3 features: y (target) date (time stamp), month.lbl (labeled month ordered factor). month.lbl exogenous regressor can passed nnetar_reg() using fit(): fit(y ~ date + month.lbl) pass month.lbl exogenous regressor. fit_xy(data[,c(\"date\", \"month.lbl\")], y = data$y) pass x, x data frame containing month.lbl date feature. month.lbl used exogenous regressor. Note date date-time class values excluded xreg.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/nnetar_reg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for NNETAR Regression Models — nnetar_reg","text":"","code":"library(dplyr) library(parsnip) library(rsample) library(timetk) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\") m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # ---- NNETAR ----  # Model Spec model_spec <- nnetar_reg() %>%     set_engine(\"nnetar\")  # Fit Spec set.seed(123) model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) #> frequency = 12 observations per 1 year model_fit #> parsnip model object #>  #> Fit time:  260ms  #> Series: outcome  #> Model:  NNAR(1,1,10)[12]  #> Call:   forecast::nnetar(y = outcome, p = p, P = P, size = size, repeats = repeats,  #>     xreg = xreg_matrix, decay = decay, maxit = maxit) #>  #> Average of 20 networks, each of which is #> a 2-10-1 network with 41 weights #> options were - linear output units  #>  #> sigma^2 estimated as 0.0005868"},{"path":"https://business-science.github.io/modeltime/reference/panel_tail.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter the last N rows (Tail) for multiple time series — panel_tail","title":"Filter the last N rows (Tail) for multiple time series — panel_tail","text":"Filter last N rows (Tail) multiple time series","code":""},{"path":"https://business-science.github.io/modeltime/reference/panel_tail.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter the last N rows (Tail) for multiple time series — panel_tail","text":"","code":"panel_tail(data, id, n)"},{"path":"https://business-science.github.io/modeltime/reference/panel_tail.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter the last N rows (Tail) for multiple time series — panel_tail","text":"data data frame id \"id\" feature indicating column differentiates time series panels n number rows filter","code":""},{"path":"https://business-science.github.io/modeltime/reference/panel_tail.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter the last N rows (Tail) for multiple time series — panel_tail","text":"data frame","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/panel_tail.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter the last N rows (Tail) for multiple time series — panel_tail","text":"","code":"library(timetk)  # Get the last 6 observations from each group m4_monthly %>%     panel_tail(id = id, n = 6) #> # A tibble: 24 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M1    2015-01-01  6040 #>  2 M1    2015-02-01  5130 #>  3 M1    2015-03-01  5090 #>  4 M1    2015-04-01  5210 #>  5 M1    2015-05-01  4910 #>  6 M1    2015-06-01  6890 #>  7 M2    2015-01-01  2210 #>  8 M2    2015-02-01  1930 #>  9 M2    2015-03-01  1960 #> 10 M2    2015-04-01  1590 #> # … with 14 more rows"},{"path":"https://business-science.github.io/modeltime/reference/parallel_start.html","id":null,"dir":"Reference","previous_headings":"","what":"Start parallel clusters using parallel package — parallel_start","title":"Start parallel clusters using parallel package — parallel_start","text":"Start parallel clusters using parallel package","code":""},{"path":"https://business-science.github.io/modeltime/reference/parallel_start.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Start parallel clusters using parallel package — parallel_start","text":"","code":"parallel_start(..., .method = c(\"parallel\", \"spark\"))  parallel_stop()"},{"path":"https://business-science.github.io/modeltime/reference/parallel_start.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Start parallel clusters using parallel package — parallel_start","text":"... Parameters passed underlying functions (See Details Section) .method method create parallel backend. Supports: \"parallel\" - Uses parallel doParallel packages \"spark\" - Uses sparklyr package","code":""},{"path":"https://business-science.github.io/modeltime/reference/parallel_start.html","id":"parallel-method-parallel-","dir":"Reference","previous_headings":"","what":"Parallel (.method = \"parallel\")","title":"Start parallel clusters using parallel package — parallel_start","text":"Performs 3 Steps: Makes clusters using parallel::makeCluster(...). parallel_start(...) passed parallel::makeCluster(...). Registers clusters using doParallel::registerDoParallel(). Adds .libPaths() using parallel::clusterCall().","code":""},{"path":"https://business-science.github.io/modeltime/reference/parallel_start.html","id":"spark-method-spark-","dir":"Reference","previous_headings":"","what":"Spark (.method = \"spark\")","title":"Start parallel clusters using parallel package — parallel_start","text":"Important, make sure create spark connection using sparklyr::spark_connect(). Pass connection object first argument. example, parallel_start(sc, .method = \"spark\"). parallel_start(...) passed sparklyr::registerDoSpark(...).","code":""},{"path":"https://business-science.github.io/modeltime/reference/parallel_start.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Start parallel clusters using parallel package — parallel_start","text":"","code":"# Starts 2 clusters parallel_start(2)  # Returns to sequential processing parallel_stop()"},{"path":"https://business-science.github.io/modeltime/reference/parse_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Developer Tools for parsing date and date-time information — parse_index","title":"Developer Tools for parsing date and date-time information — parse_index","text":"functions designed assist developers extending modeltime package.","code":""},{"path":"https://business-science.github.io/modeltime/reference/parse_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Developer Tools for parsing date and date-time information — parse_index","text":"","code":"parse_index_from_data(data)  parse_period_from_index(data, period)"},{"path":"https://business-science.github.io/modeltime/reference/parse_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Developer Tools for parsing date and date-time information — parse_index","text":"data data frame period period calculate time index. Numeric values returned -. \"auto\" guesses numeric value index. time-based phrase (e.g. \"7 days\") calculates number timestamps typically occur within time-based phrase.","code":""},{"path":"https://business-science.github.io/modeltime/reference/parse_index.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Developer Tools for parsing date and date-time information — parse_index","text":"parse_index_from_data(): Returns tibble containing date date-time column. parse_period_from_index(): Returns numeric period tibble containing index.","code":""},{"path":"https://business-science.github.io/modeltime/reference/parse_index.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Developer Tools for parsing date and date-time information — parse_index","text":"","code":"library(dplyr) library(timetk)  predictors <- m4_monthly %>%     filter(id == \"M750\") %>%     select(-value)  index_tbl <- parse_index_from_data(predictors) index_tbl #> # A tibble: 306 × 1 #>    date       #>    <date>     #>  1 1990-01-01 #>  2 1990-02-01 #>  3 1990-03-01 #>  4 1990-04-01 #>  5 1990-05-01 #>  6 1990-06-01 #>  7 1990-07-01 #>  8 1990-08-01 #>  9 1990-09-01 #> 10 1990-10-01 #> # … with 296 more rows  period <- parse_period_from_index(index_tbl, period = \"1 year\") #> frequency = 12 observations per 1 year period #> [1] 12"},{"path":"https://business-science.github.io/modeltime/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://business-science.github.io/modeltime/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_forecast.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive Forecast Visualization — plot_modeltime_forecast","title":"Interactive Forecast Visualization — plot_modeltime_forecast","text":"wrapper plot_time_series() generates interactive (plotly) static (ggplot2) plot forecasted data.","code":""},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_forecast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive Forecast Visualization — plot_modeltime_forecast","text":"","code":"plot_modeltime_forecast(   .data,   .conf_interval_show = TRUE,   .conf_interval_fill = \"grey20\",   .conf_interval_alpha = 0.2,   .smooth = FALSE,   .legend_show = TRUE,   .legend_max_width = 40,   .title = \"Forecast Plot\",   .x_lab = \"\",   .y_lab = \"\",   .color_lab = \"Legend\",   .interactive = TRUE,   .plotly_slider = FALSE,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_forecast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interactive Forecast Visualization — plot_modeltime_forecast","text":".data tibble output modeltime_forecast() .conf_interval_show Logical. Whether include confidence interval ribbon. .conf_interval_fill Fill color confidence interval .conf_interval_alpha Fill opacity confidence interval. Range (0, 1). .smooth Logical - Whether include trendline smoother. Uses See smooth_vec() apply LOESS smoother. .legend_show Logical. Whether show legend. Can save space long model descriptions. .legend_max_width Numeric. width truncation apply legend text. .title Title plot .x_lab X-axis label plot .y_lab Y-axis label plot .color_lab Legend label color_var used. .interactive Returns either static (ggplot2) visualization interactive (plotly) visualization .plotly_slider TRUE, returns plotly date range slider. ... Additional arguments passed timetk::plot_time_series().","code":""},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_forecast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interactive Forecast Visualization — plot_modeltime_forecast","text":"static ggplot2 plot interactive plotly plot containing forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_forecast.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interactive Forecast Visualization — plot_modeltime_forecast","text":"","code":"library(tidyverse) library(lubridate) library(timetk) library(parsnip) library(rsample) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)  # --- MODELS ---  # Model 1: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  models_tbl <- modeltime_table(     model_fit_prophet )  # ---- FORECAST ----  models_tbl %>%     modeltime_calibrate(new_data = testing(splits)) %>%     modeltime_forecast(         new_data    = testing(splits),         actual_data = m750     ) %>%     plot_modeltime_forecast(.interactive = FALSE) #> Warning: no non-missing arguments to max; returning -Inf"},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_residuals.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive Residuals Visualization — plot_modeltime_residuals","title":"Interactive Residuals Visualization — plot_modeltime_residuals","text":"wrapper examining residuals using: Time Plot: plot_time_series() ACF Plot: plot_acf_diagnostics() Seasonality Plot: plot_seasonal_diagnostics()","code":""},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_residuals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive Residuals Visualization — plot_modeltime_residuals","text":"","code":"plot_modeltime_residuals(   .data,   .type = c(\"timeplot\", \"acf\", \"seasonality\"),   .smooth = FALSE,   .legend_show = TRUE,   .legend_max_width = 40,   .title = \"Residuals Plot\",   .x_lab = \"\",   .y_lab = \"\",   .color_lab = \"Legend\",   .interactive = TRUE,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_residuals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interactive Residuals Visualization — plot_modeltime_residuals","text":".data tibble output modeltime_residuals() .type One \"timeplot\", \"acf\", \"seasonality\". default \"timeplot\". .smooth Logical - Whether include trendline smoother. Uses See smooth_vec() apply LOESS smoother. .legend_show Logical. Whether show legend. Can save space long model descriptions. .legend_max_width Numeric. width truncation apply legend text. .title Title plot .x_lab X-axis label plot .y_lab Y-axis label plot .color_lab Legend label color_var used. .interactive Returns either static (ggplot2) visualization interactive (plotly) visualization ... Additional arguments passed : Time Plot: plot_time_series() ACF Plot: plot_acf_diagnostics() Seasonality Plot: plot_seasonal_diagnostics()","code":""},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_residuals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interactive Residuals Visualization — plot_modeltime_residuals","text":"static ggplot2 plot interactive plotly plot containing residuals vs time","code":""},{"path":"https://business-science.github.io/modeltime/reference/plot_modeltime_residuals.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interactive Residuals Visualization — plot_modeltime_residuals","text":"","code":"library(tidyverse) library(lubridate) library(timetk) library(parsnip) library(rsample) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)  # --- MODELS ---  # Model 1: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  models_tbl <- modeltime_table(     model_fit_prophet )  # ---- RESIDUALS ----  residuals_tbl <- models_tbl %>%     modeltime_calibrate(new_data = testing(splits)) %>%     modeltime_residuals()  residuals_tbl %>%     plot_modeltime_residuals(         .type = \"timeplot\",         .interactive = FALSE     )"},{"path":"https://business-science.github.io/modeltime/reference/pluck_modeltime_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract model by model id in a Modeltime Table — pluck_modeltime_model","title":"Extract model by model id in a Modeltime Table — pluck_modeltime_model","text":"pull_modeltime_model() pluck_modeltime_model() functions synonymns.","code":""},{"path":"https://business-science.github.io/modeltime/reference/pluck_modeltime_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract model by model id in a Modeltime Table — pluck_modeltime_model","text":"","code":"pluck_modeltime_model(object, .model_id)  # S3 method for mdl_time_tbl pluck_modeltime_model(object, .model_id)  pull_modeltime_model(object, .model_id)"},{"path":"https://business-science.github.io/modeltime/reference/pluck_modeltime_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract model by model id in a Modeltime Table — pluck_modeltime_model","text":"object Modeltime Table .model_id numeric value matching .model_id want update","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/pluck_modeltime_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract model by model id in a Modeltime Table — pluck_modeltime_model","text":"","code":"m750_models %>%     pluck_modeltime_model(2) #> ══ Workflow [trained] ══════════════════════════════════════════════════════════ #> Preprocessor: Recipe #> Model: prophet_reg() #>  #> ── Preprocessor ──────────────────────────────────────────────────────────────── #> 6 Recipe Steps #>  #> • step_timeseries_signature() #> • step_rm() #> • step_normalize() #> • step_dummy() #> • step_fourier() #> • step_rm() #>  #> ── Model ─────────────────────────────────────────────────────────────────────── #> PROPHET Model #> - growth: 'linear' #> - n.changepoints: 25 #> - changepoint.range: 0.8 #> - yearly.seasonality: 'auto' #> - weekly.seasonality: 'auto' #> - daily.seasonality: 'auto' #> - seasonality.mode: 'additive' #> - changepoint.prior.scale: 0.05 #> - seasonality.prior.scale: 10 #> - holidays.prior.scale: 10 #> - logistic_cap: NULL #> - logistic_floor: NULL #> - extra_regressors: 0"},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursive Model Predictions — predict.recursive","title":"Recursive Model Predictions — predict.recursive","text":"Make predictions recursive model.","code":""},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursive Model Predictions — predict.recursive","text":"","code":"# S3 method for recursive predict(object, new_data, type = NULL, opts = list(), ...)"},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recursive Model Predictions — predict.recursive","text":"object object class model_fit new_data rectangular data object, data frame. type single character value NULL. Possible values \"numeric\", \"class\", \"prob\", \"conf_int\", \"pred_int\", \"quantile\", \"time\", \"hazard\", \"survival\", \"raw\". NULL, predict() choose appropriate value based model's mode. opts list optional arguments underlying predict function used type = \"raw\". list include options model object new data predicted. ... Arguments underlying model's prediction function passed (see opts). parsnip related options can passed, depending value type. Possible arguments : level: types \"conf_int\" \"pred_int\" parameter tail area intervals (e.g. confidence level confidence intervals). Default value 0.95. std_error: add standard error fit prediction (scale linear predictors) types \"conf_int\" \"pred_int\". Default value FALSE. quantile: quantile(s) quantile regression (implemented yet) time: time(s) hazard survival probability estimates.","code":""},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recursive Model Predictions — predict.recursive","text":"Numeric values recursive panel prediction","code":""},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recursive Model Predictions — predict.recursive","text":"Refer recursive() details examples.","code":""},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive_panel.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursive Model Predictions — predict.recursive_panel","title":"Recursive Model Predictions — predict.recursive_panel","text":"Make predictions recursive model.","code":""},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive_panel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursive Model Predictions — predict.recursive_panel","text":"","code":"# S3 method for recursive_panel predict(object, new_data, type = NULL, opts = list(), ...)"},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive_panel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recursive Model Predictions — predict.recursive_panel","text":"object object class model_fit new_data rectangular data object, data frame. type single character value NULL. Possible values \"numeric\", \"class\", \"prob\", \"conf_int\", \"pred_int\", \"quantile\", \"time\", \"hazard\", \"survival\", \"raw\". NULL, predict() choose appropriate value based model's mode. opts list optional arguments underlying predict function used type = \"raw\". list include options model object new data predicted. ... Arguments underlying model's prediction function passed (see opts). parsnip related options can passed, depending value type. Possible arguments : level: types \"conf_int\" \"pred_int\" parameter tail area intervals (e.g. confidence level confidence intervals). Default value 0.95. std_error: add standard error fit prediction (scale linear predictors) types \"conf_int\" \"pred_int\". Default value FALSE. quantile: quantile(s) quantile regression (implemented yet) time: time(s) hazard survival probability estimates.","code":""},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive_panel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recursive Model Predictions — predict.recursive_panel","text":"Numeric values recursive panel prediction","code":""},{"path":"https://business-science.github.io/modeltime/reference/predict.recursive_panel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Recursive Model Predictions — predict.recursive_panel","text":"Refer recursive() details examples.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prep_nested.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepared Nested Modeltime Data — prep_nested","title":"Prepared Nested Modeltime Data — prep_nested","text":"set functions simplify preparation nested data iterative (nested) forecasting Nested Modeltime Tables.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prep_nested.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepared Nested Modeltime Data — prep_nested","text":"","code":"extend_timeseries(.data, .id_var, .date_var, .length_future, ...)  nest_timeseries(.data, .id_var, .length_future, .length_actual = NULL)  split_nested_timeseries(.data, .length_test, .length_train = NULL, ...)"},{"path":"https://business-science.github.io/modeltime/reference/prep_nested.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepared Nested Modeltime Data — prep_nested","text":".data data frame tibble containing time series data. data : identifier (.id_var): Identifying one time series groups date variable (.date_var): date date time column target variable (.value): column containing numeric values forecasted .id_var id column .date_var date datetime column .length_future Varies based function: extend_timeseries(): Defines far future extend time series time series group. nest_timeseries(): Defines observations split .future_data. ... Additional arguments passed helper function. See details. .length_actual Can used slice .actual_data recent number observations. .length_test Defines length test split evaluation. .length_train Defines length training split evaluation.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prep_nested.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepared Nested Modeltime Data — prep_nested","text":"Preparation nested time series follows 3-Step Process:","code":""},{"path":"https://business-science.github.io/modeltime/reference/prep_nested.html","id":"step-extend-the-time-series","dir":"Reference","previous_headings":"","what":"Step 1: Extend the Time Series","title":"Prepared Nested Modeltime Data — prep_nested","text":"extend_timeseries(): wrapper timetk::future_frame() extends time series group-wise future. group column specified .id_var. date column specified .date_var. length future specified .length_future. ... additional parameters can passed timetk::future_frame()","code":""},{"path":"https://business-science.github.io/modeltime/reference/prep_nested.html","id":"step-nest-the-time-series","dir":"Reference","previous_headings":"","what":"Step 2: Nest the Time Series","title":"Prepared Nested Modeltime Data — prep_nested","text":"nest_timeseries(): helper nesting data .actual_data .future_data. group column specified .id_var .length_future defines length .future_data. remaining data converted .actual_data. .length_actual can used slice .actual_data recent number observations. result \"nested data frame\".","code":""},{"path":"https://business-science.github.io/modeltime/reference/prep_nested.html","id":"step-split-the-actual-data-into-train-test-splits","dir":"Reference","previous_headings":"","what":"Step 3: Split the Actual Data into Train/Test Splits","title":"Prepared Nested Modeltime Data — prep_nested","text":"split_nested_timeseries(): wrapper timetk::time_series_split() generates training/testing splits .actual_data column. .length_test primary argument identifies size testing sample. typically size .future_data. .length_train optional size training data. ... (dots) additional arguments can passed timetk::time_series_split().","code":""},{"path":"https://business-science.github.io/modeltime/reference/prep_nested.html","id":"helpers","dir":"Reference","previous_headings":"","what":"Helpers","title":"Prepared Nested Modeltime Data — prep_nested","text":"extract_nested_train_split() extract_nested_test_split() used simplify extracting training testing data actual data. can helpful making preprocessing recipes using recipes package.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prep_nested.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepared Nested Modeltime Data — prep_nested","text":"","code":"library(tidyverse) library(timetk) library(modeltime)   nested_data_tbl <- walmart_sales_weekly %>%     select(id, Date, Weekly_Sales) %>%     set_names(c(\"id\", \"date\", \"value\")) %>%      # Step 1: Extends the time series by id     extend_timeseries(         .id_var     = id,         .date_var   = date,         .length_future = 52     ) %>%      # Step 2: Nests the time series into .actual_data and .future_data     nest_timeseries(         .id_var     = id,         .length_future = 52     ) %>%      # Step 3: Adds a column .splits that contains training/testing indicies     split_nested_timeseries(         .length_test = 52     )  nested_data_tbl #> # A tibble: 7 × 4 #>   id    .actual_data       .future_data      .splits         #>   <fct> <list>             <list>            <list>          #> 1 1_1   <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 2 1_3   <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 3 1_8   <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 4 1_13  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 5 1_38  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 6 1_93  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]> #> 7 1_95  <tibble [143 × 2]> <tibble [52 × 2]> <split [91|52]>  # Helpers: Getting the Train/Test Sets extract_nested_train_split(nested_data_tbl, .row_id = 1) #> # A tibble: 91 × 2 #>    date        value #>    <date>      <dbl> #>  1 2010-02-05 24924. #>  2 2010-02-12 46039. #>  3 2010-02-19 41596. #>  4 2010-02-26 19404. #>  5 2010-03-05 21828. #>  6 2010-03-12 21043. #>  7 2010-03-19 22137. #>  8 2010-03-26 26229. #>  9 2010-04-02 57258. #> 10 2010-04-09 42961. #> # … with 81 more rows"},{"path":"https://business-science.github.io/modeltime/reference/prophet_boost.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for Boosted PROPHET Time Series Models — prophet_boost","title":"General Interface for Boosted PROPHET Time Series Models — prophet_boost","text":"prophet_boost() way generate specification Boosted PROPHET model fitting allows model created using different packages. Currently package prophet.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_boost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for Boosted PROPHET Time Series Models — prophet_boost","text":"","code":"prophet_boost(   mode = \"regression\",   growth = NULL,   changepoint_num = NULL,   changepoint_range = NULL,   seasonality_yearly = NULL,   seasonality_weekly = NULL,   seasonality_daily = NULL,   season = NULL,   prior_scale_changepoints = NULL,   prior_scale_seasonality = NULL,   prior_scale_holidays = NULL,   logistic_cap = NULL,   logistic_floor = NULL,   mtry = NULL,   trees = NULL,   min_n = NULL,   tree_depth = NULL,   learn_rate = NULL,   loss_reduction = NULL,   sample_size = NULL,   stop_iter = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/prophet_boost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for Boosted PROPHET Time Series Models — prophet_boost","text":"mode single character string type model. possible value model \"regression\". growth String 'linear' 'logistic' specify linear logistic trend. changepoint_num Number potential changepoints include modeling trend. changepoint_range Adjusts flexibility trend component limiting percentage data end time series. 0.80 means changepoint exist first 80% data. seasonality_yearly One \"auto\", TRUE FALSE. Toggles /seasonal component models year--year seasonality. seasonality_weekly One \"auto\", TRUE FALSE. Toggles /seasonal component models week--week seasonality. seasonality_daily One \"auto\", TRUE FALSE. Toggles /seasonal componet models day--day seasonality. season 'additive' (default) 'multiplicative'. prior_scale_changepoints Parameter modulating flexibility automatic changepoint selection. Large values allow many changepoints, small values allow changepoints. prior_scale_seasonality Parameter modulating strength seasonality model. Larger values allow model fit larger seasonal fluctuations, smaller values dampen seasonality. prior_scale_holidays Parameter modulating strength holiday components model, unless overridden holidays input. logistic_cap growth logistic, upper-bound \"saturation\". logistic_floor growth logistic, lower-bound \"saturation\". mtry number number (proportion) predictors randomly sampled split creating tree models (specific engines ) trees integer number trees contained ensemble. min_n integer minimum number data points node required node split . tree_depth integer maximum depth tree (.e. number splits) (specific engines ). learn_rate number rate boosting algorithm adapts iteration--iteration (specific engines ). loss_reduction number reduction loss function required split (specific engines ). sample_size number number (proportion) data exposed fitting routine. stop_iter number iterations without improvement stopping  (xgboost ).","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_boost.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for Boosted PROPHET Time Series Models — prophet_boost","text":"data given function saved used determine mode model. prophet_boost(), mode always \"regression\". model can created using fit() function using following engines: \"prophet_xgboost\" (default) - Connects prophet::prophet() xgboost::xgb.train() Main Arguments main arguments (tuning parameters) PROPHET model : growth: String 'linear' 'logistic' specify linear logistic trend. changepoint_num: Number potential changepoints include modeling trend. changepoint_range: Range changepoints adjusts close end last changepoint can located. season: 'additive' (default) 'multiplicative'. prior_scale_changepoints: Parameter modulating flexibility automatic changepoint selection. Large values allow many changepoints, small values allow changepoints. prior_scale_seasonality: Parameter modulating strength seasonality model. Larger values allow model fit larger seasonal fluctuations, smaller values dampen seasonality. prior_scale_holidays: Parameter modulating strength holiday components model, unless overridden holidays input. logistic_cap: growth logistic, upper-bound \"saturation\". logistic_floor: growth logistic, lower-bound \"saturation\". main arguments (tuning parameters) model XGBoost model : mtry: number predictors randomly sampled split creating tree models. trees: number trees contained ensemble. min_n: minimum number data points node required node split . tree_depth: maximum depth tree (.e. number splits). learn_rate: rate boosting algorithm adapts iteration--iteration. loss_reduction: reduction loss function required split . sample_size: amount data exposed fitting routine. stop_iter: number iterations without improvement stopping. arguments converted specific names time model fit. options argument can set using set_engine() (See Engine Details ). parameters need modified, update() can used lieu recreating object scratch.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_boost.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for Boosted PROPHET Time Series Models — prophet_boost","text":"standardized parameter names modeltime can mapped original names engine: Model 1: PROPHET: Model 2: XGBoost: options can set using set_engine(). prophet_xgboost Model 1: PROPHET (prophet::prophet): Parameter Notes: df: supplied via parsnip / modeltime fit() interface (provide manually). See Fit Details (). holidays: data.frame holidays can supplied via set_engine() uncertainty.samples: default set 0 prophet uncertainty intervals used part Modeltime Workflow. can override setting plan use prophet's uncertainty tools. Logistic Growth Saturation Levels: growth = \"logistic\", simply add numeric values logistic_cap / logistic_floor. need add additional columns \"cap\" \"floor\" data frame. Limitations: prophet::add_seasonality() currently implemented. used specify non-standard seasonalities using fourier series. alternative use step_fourier() supply custom seasonalities Extra Regressors. Model 2: XGBoost (xgboost::xgb.train): Parameter Notes: XGBoost uses params = list() capture. Parsnip / Modeltime automatically sends args provided ... inside set_engine() params = list(...).","code":"## function (df = NULL, growth = \"linear\", changepoints = NULL, n.changepoints = 25,  ##     changepoint.range = 0.8, yearly.seasonality = \"auto\", weekly.seasonality = \"auto\",  ##     daily.seasonality = \"auto\", holidays = NULL, seasonality.mode = \"additive\",  ##     seasonality.prior.scale = 10, holidays.prior.scale = 10, changepoint.prior.scale = 0.05,  ##     mcmc.samples = 0, interval.width = 0.8, uncertainty.samples = 1000,  ##     fit = TRUE, ...) ## function (params = list(), data, nrounds, watchlist = list(), obj = NULL,  ##     feval = NULL, verbose = 1, print_every_n = 1L, early_stopping_rounds = NULL,  ##     maximize = NULL, save_period = NULL, save_name = \"xgboost.model\", xgb_model = NULL,  ##     callbacks = list(), ...)"},{"path":"https://business-science.github.io/modeltime/reference/prophet_boost.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for Boosted PROPHET Time Series Models — prophet_boost","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) Univariate (Extra Regressors): univariate analysis, must include date date-time feature. Simply use: Formula Interface (recommended): fit(y ~ date) ignore xreg's. XY Interface: fit_xy(x = data[,\"date\"], y = data$y) ignore xreg's. Multivariate (Extra Regressors) Extra Regressors parameter populated using fit() fit_xy() function: factor, ordered factor, numeric data used xregs. Date Date-time variables used xregs character data converted factor. Xreg Example: Suppose 3 features: y (target) date (time stamp), month.lbl (labeled month ordered factor). month.lbl exogenous regressor can passed arima_reg() using fit(): fit(y ~ date + month.lbl) pass month.lbl exogenous regressor. fit_xy(data[,c(\"date\", \"month.lbl\")], y = data$y) pass x, x data frame containing month.lbl date feature. month.lbl used exogenous regressor. Note date date-time class values excluded xreg.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/prophet_boost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for Boosted PROPHET Time Series Models — prophet_boost","text":"","code":"library(dplyr) library(lubridate) library(parsnip) library(rsample) library(timetk) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\") m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # ---- PROPHET ----  # Model Spec model_spec <- prophet_boost(     learn_rate = 0.1 ) %>%     set_engine(\"prophet_xgboost\")  # Fit Spec if (FALSE) { model_fit <- model_spec %>%     fit(log(value) ~ date + as.numeric(date) + month(date, label = TRUE),         data = training(splits)) model_fit }"},{"path":"https://business-science.github.io/modeltime/reference/prophet_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level PROPHET function for translating modeltime to PROPHET — prophet_fit_impl","title":"Low-Level PROPHET function for translating modeltime to PROPHET — prophet_fit_impl","text":"Low-Level PROPHET function translating modeltime PROPHET","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level PROPHET function for translating modeltime to PROPHET — prophet_fit_impl","text":"","code":"prophet_fit_impl(   x,   y,   growth = \"linear\",   n.changepoints = 25,   changepoint.range = 0.8,   yearly.seasonality = \"auto\",   weekly.seasonality = \"auto\",   daily.seasonality = \"auto\",   seasonality.mode = \"additive\",   changepoint.prior.scale = 0.05,   seasonality.prior.scale = 10,   holidays.prior.scale = 10,   regressors.prior.scale = 10000,   regressors.standardize = \"auto\",   regressors.mode = NULL,   logistic_cap = NULL,   logistic_floor = NULL,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/prophet_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level PROPHET function for translating modeltime to PROPHET — prophet_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit growth String 'linear', 'logistic', 'flat' specify linear, logistic flat trend. n.changepoints Number potential changepoints include. used input `changepoints` supplied. `changepoints` supplied, n.changepoints potential changepoints selected uniformly first `changepoint.range` proportion df$ds. changepoint.range Proportion history trend changepoints estimated. Defaults 0.8 first 80 `changepoints` specified. yearly.seasonality Fit yearly seasonality. Can 'auto', TRUE, FALSE, number Fourier terms generate. weekly.seasonality Fit weekly seasonality. Can 'auto', TRUE, FALSE, number Fourier terms generate. daily.seasonality Fit daily seasonality. Can 'auto', TRUE, FALSE, number Fourier terms generate. seasonality.mode 'additive' (default) 'multiplicative'. changepoint.prior.scale Parameter modulating flexibility automatic changepoint selection. Large values allow many changepoints, small values allow changepoints. seasonality.prior.scale Parameter modulating strength seasonality model. Larger values allow model fit larger seasonal fluctuations, smaller values dampen seasonality. Can specified individual seasonalities using add_seasonality. holidays.prior.scale Parameter modulating strength holiday components model, unless overridden holidays input. regressors.prior.scale Float scale normal prior. Default 10,000. Gets passed prophet::add_regressor(prior.scale) regressors.standardize Bool, specify whether regressor standardized prior fitting. Can 'auto' (standardize binary), True, False. Gets passed prophet::add_regressor(standardize). regressors.mode Optional, 'additive' 'multiplicative'. Defaults seasonality.mode. logistic_cap growth logistic, upper-bound \"saturation\". logistic_floor growth logistic, lower-bound \"saturation\". ... Additional arguments passed prophet::prophet","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuning Parameters for Prophet Models — prophet_params","title":"Tuning Parameters for Prophet Models — prophet_params","text":"Tuning Parameters Prophet Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning Parameters for Prophet Models — prophet_params","text":"","code":"growth(values = c(\"linear\", \"logistic\"))  changepoint_num(range = c(0L, 50L), trans = NULL)  changepoint_range(range = c(0.6, 0.9), trans = NULL)  seasonality_yearly(values = c(TRUE, FALSE))  seasonality_weekly(values = c(TRUE, FALSE))  seasonality_daily(values = c(TRUE, FALSE))  prior_scale_changepoints(range = c(-3, 2), trans = log10_trans())  prior_scale_seasonality(range = c(-3, 2), trans = log10_trans())  prior_scale_holidays(range = c(-3, 2), trans = log10_trans())"},{"path":"https://business-science.github.io/modeltime/reference/prophet_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuning Parameters for Prophet Models — prophet_params","text":"values character string possible values. range two-element vector holding defaults smallest largest possible values, respectively. trans trans object scales package, scales::log10_trans() scales::reciprocal_trans(). provided, default used matches units used range. transformation, NULL.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_params.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tuning Parameters for Prophet Models — prophet_params","text":"main parameters Prophet models : growth: form trend: \"linear\", \"logistic\". changepoint_num: maximum number trend changepoints allowed modeling trend changepoint_range: range affects close changepoints can go end time series. larger value, flexible trend. Yearly, Weekly, Daily Seasonality: Yearly: seasonality_yearly - Useful seasonal patterns appear year--year Weekly: seasonality_weekly - Useful seasonal patterns appear week--week (e.g. daily data) Daily: seasonality_daily - Useful seasonal patterns appear day--day (e.g. hourly data) season: form seasonal term: \"additive\" \"multiplicative\". See season(). \"Prior Scale\": Controls flexibility Changepoints: prior_scale_changepoints Seasonality: prior_scale_seasonality Holidays: prior_scale_holidays log10_trans() converts priors scale 0.001 100, effectively weights lower values heavily larger values.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tuning Parameters for Prophet Models — prophet_params","text":"","code":"growth() #> Growth Trend  (qualitative) #> 2 possible value include: #> 'linear' and 'logistic'   changepoint_num() #> Number of Possible Trend Changepoints (quantitative) #> Range: [0, 50]  season() #> Season Term  (qualitative) #> 3 possible value include: #> 'additive', 'multiplicative' and 'none'   prior_scale_changepoints() #> Prior Scale Changepoints (quantitative) #> Transformer:  log-10  #> Range (transformed scale): [-3, 2]"},{"path":"https://business-science.github.io/modeltime/reference/prophet_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for PROPHET models — prophet_predict_impl","title":"Bridge prediction function for PROPHET models — prophet_predict_impl","text":"Bridge prediction function PROPHET models","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for PROPHET models — prophet_predict_impl","text":"","code":"prophet_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/prophet_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for PROPHET models — prophet_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed prophet::predict()","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_reg.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for PROPHET Time Series Models — prophet_reg","title":"General Interface for PROPHET Time Series Models — prophet_reg","text":"prophet_reg() way generate specification PROPHET model fitting allows model created using different packages. Currently package prophet.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_reg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for PROPHET Time Series Models — prophet_reg","text":"","code":"prophet_reg(   mode = \"regression\",   growth = NULL,   changepoint_num = NULL,   changepoint_range = NULL,   seasonality_yearly = NULL,   seasonality_weekly = NULL,   seasonality_daily = NULL,   season = NULL,   prior_scale_changepoints = NULL,   prior_scale_seasonality = NULL,   prior_scale_holidays = NULL,   logistic_cap = NULL,   logistic_floor = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/prophet_reg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for PROPHET Time Series Models — prophet_reg","text":"mode single character string type model. possible value model \"regression\". growth String 'linear' 'logistic' specify linear logistic trend. changepoint_num Number potential changepoints include modeling trend. changepoint_range Adjusts flexibility trend component limiting percentage data end time series. 0.80 means changepoint exist first 80% data. seasonality_yearly One \"auto\", TRUE FALSE. Toggles /seasonal component models year--year seasonality. seasonality_weekly One \"auto\", TRUE FALSE. Toggles /seasonal component models week--week seasonality. seasonality_daily One \"auto\", TRUE FALSE. Toggles /seasonal componet models day--day seasonality. season 'additive' (default) 'multiplicative'. prior_scale_changepoints Parameter modulating flexibility automatic changepoint selection. Large values allow many changepoints, small values allow changepoints. prior_scale_seasonality Parameter modulating strength seasonality model. Larger values allow model fit larger seasonal fluctuations, smaller values dampen seasonality. prior_scale_holidays Parameter modulating strength holiday components model, unless overridden holidays input. logistic_cap growth logistic, upper-bound \"saturation\". logistic_floor growth logistic, lower-bound \"saturation\".","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_reg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for PROPHET Time Series Models — prophet_reg","text":"data given function saved used determine mode model. prophet_reg(), mode always \"regression\". model can created using fit() function using following engines: \"prophet\" (default) - Connects prophet::prophet() Main Arguments main arguments (tuning parameters) model : growth: String 'linear' 'logistic' specify linear logistic trend. changepoint_num: Number potential changepoints include modeling trend. changepoint_range: Range changepoints adjusts close end last changepoint can located. season: 'additive' (default) 'multiplicative'. prior_scale_changepoints: Parameter modulating flexibility automatic changepoint selection. Large values allow many changepoints, small values allow changepoints. prior_scale_seasonality: Parameter modulating strength seasonality model. Larger values allow model fit larger seasonal fluctuations, smaller values dampen seasonality. prior_scale_holidays: Parameter modulating strength holiday components model, unless overridden holidays input. logistic_cap: growth logistic, upper-bound \"saturation\". logistic_floor: growth logistic, lower-bound \"saturation\". arguments converted specific names time model fit. options argument can set using set_engine() (See Engine Details ). parameters need modified, update() can used lieu recreating object scratch.","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_reg.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for PROPHET Time Series Models — prophet_reg","text":"standardized parameter names modeltime can mapped original names engine: options can set using set_engine(). prophet engine uses prophet::prophet(). Function Parameters: Parameter Notes: df: supplied via parsnip / modeltime fit() interface (provide manually). See Fit Details (). holidays: data.frame holidays can supplied via set_engine() uncertainty.samples: default set 0 prophet uncertainty intervals used part Modeltime Workflow. can override setting plan use prophet's uncertainty tools. Regressors: Regressors provided via fit() recipes interface, passes regressors prophet::add_regressor() Parameters can controlled set_engine() via: regressors.prior.scale, regressors.standardize, regressors.mode regressor prior scale implementation default regressors.prior.scale = 1e4, deviates prophet implementation (defaults holidays.prior.scale) Logistic Growth Saturation Levels: growth = \"logistic\", simply add numeric values logistic_cap / logistic_floor. need add additional columns \"cap\" \"floor\" data frame. Limitations: prophet::add_seasonality() currently implemented. used specify non-standard seasonalities using fourier series. alternative use step_fourier() supply custom seasonalities Extra Regressors.","code":"## function (df = NULL, growth = \"linear\", changepoints = NULL, n.changepoints = 25,  ##     changepoint.range = 0.8, yearly.seasonality = \"auto\", weekly.seasonality = \"auto\",  ##     daily.seasonality = \"auto\", holidays = NULL, seasonality.mode = \"additive\",  ##     seasonality.prior.scale = 10, holidays.prior.scale = 10, changepoint.prior.scale = 0.05,  ##     mcmc.samples = 0, interval.width = 0.8, uncertainty.samples = 1000,  ##     fit = TRUE, ...)"},{"path":"https://business-science.github.io/modeltime/reference/prophet_reg.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for PROPHET Time Series Models — prophet_reg","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) Univariate (Extra Regressors): univariate analysis, must include date date-time feature. Simply use: Formula Interface (recommended): fit(y ~ date) ignore xreg's. XY Interface: fit_xy(x = data[,\"date\"], y = data$y) ignore xreg's. Multivariate (Extra Regressors) Extra Regressors parameter populated using fit() fit_xy() function: factor, ordered factor, numeric data used xregs. Date Date-time variables used xregs character data converted factor. Xreg Example: Suppose 3 features: y (target) date (time stamp), month.lbl (labeled month ordered factor). month.lbl exogenous regressor can passed arima_reg() using fit(): fit(y ~ date + month.lbl) pass month.lbl exogenous regressor. fit_xy(data[,c(\"date\", \"month.lbl\")], y = data$y) pass x, x data frame containing month.lbl date feature. month.lbl used exogenous regressor. Note date date-time class values excluded xreg.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/prophet_reg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for PROPHET Time Series Models — prophet_reg","text":"","code":"library(dplyr) library(parsnip) library(rsample) library(timetk) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\") m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # ---- PROPHET ----  # Model Spec model_spec <- prophet_reg() %>%     set_engine(\"prophet\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this. model_fit #> parsnip model object #>  #> Fit time:  285ms  #> PROPHET Model #> - growth: 'linear' #> - n.changepoints: 25 #> - changepoint.range: 0.8 #> - yearly.seasonality: 'auto' #> - weekly.seasonality: 'auto' #> - daily.seasonality: 'auto' #> - seasonality.mode: 'additive' #> - changepoint.prior.scale: 0.05 #> - seasonality.prior.scale: 10 #> - holidays.prior.scale: 10 #> - logistic_cap: NULL #> - logistic_floor: NULL #> - extra_regressors: 0"},{"path":"https://business-science.github.io/modeltime/reference/prophet_xgboost_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level PROPHET function for translating modeltime to Boosted PROPHET — prophet_xgboost_fit_impl","title":"Low-Level PROPHET function for translating modeltime to Boosted PROPHET — prophet_xgboost_fit_impl","text":"Low-Level PROPHET function translating modeltime Boosted PROPHET","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_xgboost_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level PROPHET function for translating modeltime to Boosted PROPHET — prophet_xgboost_fit_impl","text":"","code":"prophet_xgboost_fit_impl(   x,   y,   df = NULL,   growth = \"linear\",   changepoints = NULL,   n.changepoints = 25,   changepoint.range = 0.8,   yearly.seasonality = \"auto\",   weekly.seasonality = \"auto\",   daily.seasonality = \"auto\",   holidays = NULL,   seasonality.mode = \"additive\",   seasonality.prior.scale = 10,   holidays.prior.scale = 10,   changepoint.prior.scale = 0.05,   logistic_cap = NULL,   logistic_floor = NULL,   mcmc.samples = 0,   interval.width = 0.8,   uncertainty.samples = 1000,   fit = TRUE,   max_depth = 6,   nrounds = 15,   eta = 0.3,   colsample_bytree = NULL,   colsample_bynode = NULL,   min_child_weight = 1,   gamma = 0,   subsample = 1,   validation = 0,   early_stop = NULL,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/prophet_xgboost_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level PROPHET function for translating modeltime to Boosted PROPHET — prophet_xgboost_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit df (optional) Dataframe containing history. Must columns ds (date type) y, time series. growth logistic, df must also column cap specifies capacity ds. provided, model object instantiated fit; use fit.prophet(m, df) fit model. growth String 'linear', 'logistic', 'flat' specify linear, logistic flat trend. changepoints Vector dates include potential changepoints. specified, potential changepoints selected automatically. n.changepoints Number potential changepoints include. used input `changepoints` supplied. `changepoints` supplied, n.changepoints potential changepoints selected uniformly first `changepoint.range` proportion df$ds. changepoint.range Proportion history trend changepoints estimated. Defaults 0.8 first 80 `changepoints` specified. yearly.seasonality Fit yearly seasonality. Can 'auto', TRUE, FALSE, number Fourier terms generate. weekly.seasonality Fit weekly seasonality. Can 'auto', TRUE, FALSE, number Fourier terms generate. daily.seasonality Fit daily seasonality. Can 'auto', TRUE, FALSE, number Fourier terms generate. holidays data frame columns holiday (character) ds (date type)optionally columns lower_window upper_window specify range days around date included holidays. lower_window=-2 include 2 days prior date holidays. Also optionally can column prior_scale specifying prior scale holiday. seasonality.mode 'additive' (default) 'multiplicative'. seasonality.prior.scale Parameter modulating strength seasonality model. Larger values allow model fit larger seasonal fluctuations, smaller values dampen seasonality. Can specified individual seasonalities using add_seasonality. holidays.prior.scale Parameter modulating strength holiday components model, unless overridden holidays input. changepoint.prior.scale Parameter modulating flexibility automatic changepoint selection. Large values allow many changepoints, small values allow changepoints. logistic_cap growth logistic, upper-bound \"saturation\". logistic_floor growth logistic, lower-bound \"saturation\". mcmc.samples Integer, greater 0, full Bayesian inference specified number MCMC samples. 0, MAP estimation. interval.width Numeric, width uncertainty intervals provided forecast. mcmc.samples=0, uncertainty trend using MAP estimate extrapolated generative model. mcmc.samples>0, integrated model parameters, include uncertainty seasonality. uncertainty.samples Number simulated draws used estimate uncertainty intervals. Settings value 0 False disable uncertainty estimation speed calculation. fit Boolean, FALSE model initialized fit. max_depth integer maximum depth tree. nrounds integer number boosting iterations. eta numeric value zero one control learning rate. colsample_bytree Subsampling proportion columns. colsample_bynode Subsampling proportion columns node within tree. See counts argument . default uses columns. min_child_weight numeric value minimum sum instance weights needed child continue split. gamma number minimum loss reduction required make partition leaf node tree subsample Subsampling proportion rows. validation positive number. [0, 1) value, validation random proportion data x y used performance assessment potential early stopping. 1 greater, number training set samples use purposes. early_stop integer NULL. NULL, number training iterations without improvement stopping. validation used, performance base validation set; otherwise training set used. ... Additional arguments passed xgboost::xgb.train","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_xgboost_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for Boosted PROPHET models — prophet_xgboost_predict_impl","title":"Bridge prediction function for Boosted PROPHET models — prophet_xgboost_predict_impl","text":"Bridge prediction function Boosted PROPHET models","code":""},{"path":"https://business-science.github.io/modeltime/reference/prophet_xgboost_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for Boosted PROPHET models — prophet_xgboost_predict_impl","text":"","code":"prophet_xgboost_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/prophet_xgboost_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for Boosted PROPHET models — prophet_xgboost_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed prophet::predict()","code":""},{"path":"https://business-science.github.io/modeltime/reference/pull_modeltime_residuals.html","id":null,"dir":"Reference","previous_headings":"","what":"Extracts modeltime residuals data from a Modeltime Model — pull_modeltime_residuals","title":"Extracts modeltime residuals data from a Modeltime Model — pull_modeltime_residuals","text":"modeltime model contains data residuals information, function extract data frame.","code":""},{"path":"https://business-science.github.io/modeltime/reference/pull_modeltime_residuals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extracts modeltime residuals data from a Modeltime Model — pull_modeltime_residuals","text":"","code":"pull_modeltime_residuals(object)"},{"path":"https://business-science.github.io/modeltime/reference/pull_modeltime_residuals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extracts modeltime residuals data from a Modeltime Model — pull_modeltime_residuals","text":"object fitted parsnip / modeltime model workflow","code":""},{"path":"https://business-science.github.io/modeltime/reference/pull_modeltime_residuals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extracts modeltime residuals data from a Modeltime Model — pull_modeltime_residuals","text":"tibble containing model timestamp, actual, fitted, residuals data","code":""},{"path":"https://business-science.github.io/modeltime/reference/pull_parsnip_preprocessor.html","id":null,"dir":"Reference","previous_headings":"","what":"Pulls the Formula from a Fitted Parsnip Model Object — pull_parsnip_preprocessor","title":"Pulls the Formula from a Fitted Parsnip Model Object — pull_parsnip_preprocessor","text":"Pulls Formula Fitted Parsnip Model Object","code":""},{"path":"https://business-science.github.io/modeltime/reference/pull_parsnip_preprocessor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pulls the Formula from a Fitted Parsnip Model Object — pull_parsnip_preprocessor","text":"","code":"pull_parsnip_preprocessor(object)"},{"path":"https://business-science.github.io/modeltime/reference/pull_parsnip_preprocessor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pulls the Formula from a Fitted Parsnip Model Object — pull_parsnip_preprocessor","text":"object fitted parsnip model model_fit object","code":""},{"path":"https://business-science.github.io/modeltime/reference/pull_parsnip_preprocessor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pulls the Formula from a Fitted Parsnip Model Object — pull_parsnip_preprocessor","text":"formula using stats::formula()","code":""},{"path":"https://business-science.github.io/modeltime/reference/recipe_helpers.html","id":null,"dir":"Reference","previous_headings":"","what":"Developer Tools for processing XREGS (Regressors) — recipe_helpers","title":"Developer Tools for processing XREGS (Regressors) — recipe_helpers","text":"Wrappers using recipes::bake recipes::juice process data returning data either data frame matrix format (Common formats needed machine learning algorithms).","code":""},{"path":"https://business-science.github.io/modeltime/reference/recipe_helpers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Developer Tools for processing XREGS (Regressors) — recipe_helpers","text":"","code":"juice_xreg_recipe(recipe, format = c(\"tbl\", \"matrix\"))  bake_xreg_recipe(recipe, new_data, format = c(\"tbl\", \"matrix\"))"},{"path":"https://business-science.github.io/modeltime/reference/recipe_helpers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Developer Tools for processing XREGS (Regressors) — recipe_helpers","text":"recipe prepared recipe format One : tbl: Returns tibble (data.frame) matrix: Returns matrix new_data Data processed recipe","code":""},{"path":"https://business-science.github.io/modeltime/reference/recipe_helpers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Developer Tools for processing XREGS (Regressors) — recipe_helpers","text":"Data either tbl (data.frame) matrix formats","code":""},{"path":"https://business-science.github.io/modeltime/reference/recipe_helpers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Developer Tools for processing XREGS (Regressors) — recipe_helpers","text":"","code":"library(dplyr) library(timetk) library(recipes) library(lubridate)  predictors <- m4_monthly %>%     filter(id == \"M750\") %>%     select(-value) %>%     mutate(month = month(date, label = TRUE)) predictors #> # A tibble: 306 × 3 #>    id    date       month #>    <fct> <date>     <ord> #>  1 M750  1990-01-01 Jan   #>  2 M750  1990-02-01 Feb   #>  3 M750  1990-03-01 Mar   #>  4 M750  1990-04-01 Apr   #>  5 M750  1990-05-01 May   #>  6 M750  1990-06-01 Jun   #>  7 M750  1990-07-01 Jul   #>  8 M750  1990-08-01 Aug   #>  9 M750  1990-09-01 Sep   #> 10 M750  1990-10-01 Oct   #> # … with 296 more rows  # Create default recipe xreg_recipe_spec <- create_xreg_recipe(predictors, prepare = TRUE)  # Extracts the preprocessed training data from the recipe (used in your fit function) juice_xreg_recipe(xreg_recipe_spec) #> # A tibble: 306 × 11 #>    month_Feb month_Mar month_Apr month_May month_Jun month_Jul month_Aug #>        <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl> #>  1         0         0         0         0         0         0         0 #>  2         1         0         0         0         0         0         0 #>  3         0         1         0         0         0         0         0 #>  4         0         0         1         0         0         0         0 #>  5         0         0         0         1         0         0         0 #>  6         0         0         0         0         1         0         0 #>  7         0         0         0         0         0         1         0 #>  8         0         0         0         0         0         0         1 #>  9         0         0         0         0         0         0         0 #> 10         0         0         0         0         0         0         0 #> # … with 296 more rows, and 4 more variables: month_Sep <dbl>, month_Oct <dbl>, #> #   month_Nov <dbl>, month_Dec <dbl>  # Applies the prepared recipe to new data (used in your predict function) bake_xreg_recipe(xreg_recipe_spec, new_data = predictors) #> # A tibble: 306 × 11 #>    month_Feb month_Mar month_Apr month_May month_Jun month_Jul month_Aug #>        <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl> #>  1         0         0         0         0         0         0         0 #>  2         1         0         0         0         0         0         0 #>  3         0         1         0         0         0         0         0 #>  4         0         0         1         0         0         0         0 #>  5         0         0         0         1         0         0         0 #>  6         0         0         0         0         1         0         0 #>  7         0         0         0         0         0         1         0 #>  8         0         0         0         0         0         0         1 #>  9         0         0         0         0         0         0         0 #> 10         0         0         0         0         0         0         0 #> # … with 296 more rows, and 4 more variables: month_Sep <dbl>, month_Oct <dbl>, #> #   month_Nov <dbl>, month_Dec <dbl>"},{"path":"https://business-science.github.io/modeltime/reference/recursive.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Recursive Time Series Model from a Parsnip or Workflow Regression Model — recursive","title":"Create a Recursive Time Series Model from a Parsnip or Workflow Regression Model — recursive","text":"Create Recursive Time Series Model Parsnip Workflow Regression Model","code":""},{"path":"https://business-science.github.io/modeltime/reference/recursive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Recursive Time Series Model from a Parsnip or Workflow Regression Model — recursive","text":"","code":"recursive(object, transform, train_tail, id = NULL, ...)"},{"path":"https://business-science.github.io/modeltime/reference/recursive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Recursive Time Series Model from a Parsnip or Workflow Regression Model — recursive","text":"object object model_fit fitted workflow class transform transformation performed new_data step recursive algorithm. Transformation Function: Must one argument data (see examples) train_tail tibble tail training data set. cases 'll required create variables based dependent variable. id (Optional) identifier can provided perform panel forecast. single quoted column name (e.g. id = \"id\"). ... currently used.","code":""},{"path":"https://business-science.github.io/modeltime/reference/recursive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Recursive Time Series Model from a Parsnip or Workflow Regression Model — recursive","text":"object added recursive class","code":""},{"path":"https://business-science.github.io/modeltime/reference/recursive.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Recursive Time Series Model from a Parsnip or Workflow Regression Model — recursive","text":"Recursive Model? recursive model uses predictions generate new values independent features. features typically lags used autoregressive models. important understand recursive model needed Lag Size < Forecast Horizon. Recursive needed Autoregressive Models Lag Size < Forecast Horizon? lag length less forecast horizon, problem exists missing values (NA) generated future data. solution recursive() implements iteratively fill missing values values generated predictions. Recursive Process producing forecast, following steps performed: Computing forecast first row new data. first row contain NA required column. Filling -th place dependent variable column already computed forecast. Computing missing features next step, based already calculated prediction. features computed tibble object made binded train_tail (.e. tail training data set) new_data (argument predict function). Jumping point 2., repeating rest steps till -loop ended. Recursion Panel Data Panel data time series data multiple groups identified ID column. recursive() function can used Panel Data following modifications: Supply id column quoted column name Replace tail() panel_tail() use tails time series group.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/recursive.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Recursive Time Series Model from a Parsnip or Workflow Regression Model — recursive","text":"","code":"# \\donttest{ # Libraries & Setup ---- library(modeltime) library(tidymodels) library(tidyverse) library(lubridate) library(timetk) library(slider)  # ---- SINGLE TIME SERIES (NON-PANEL) -----  m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows  FORECAST_HORIZON <- 24  m750_extended <- m750 %>%     group_by(id) %>%     future_frame(         .length_out = FORECAST_HORIZON,         .bind_data  = TRUE     ) %>%     ungroup() #> .date_var is missing. Using: date  # TRANSFORM FUNCTION ---- # - Function runs recursively that updates the forecasted dataset lag_roll_transformer <- function(data){     data %>%         # Lags         tk_augment_lags(value, .lags = 1:12) %>%         # Rolling Features         mutate(rolling_mean_12 = lag(slide_dbl(             value, .f = mean, .before = 12, .complete = FALSE         ), 1)) }  # Data Preparation m750_rolling <- m750_extended %>%     lag_roll_transformer() %>%     select(-id)  train_data <- m750_rolling %>%     drop_na()  future_data <- m750_rolling %>%     filter(is.na(value))  # Modeling  # Straight-Line Forecast model_fit_lm <- linear_reg() %>%     set_engine(\"lm\") %>%     # Use only date feature as regressor     fit(value ~ date, data = train_data)  # Autoregressive Forecast model_fit_lm_recursive <- linear_reg() %>%     set_engine(\"lm\") %>%     # Use date plus all lagged features     fit(value ~ ., data = train_data) %>%     # Add recursive() w/ transformer and train_tail     recursive(         transform  = lag_roll_transformer,         train_tail = tail(train_data, FORECAST_HORIZON)     )  model_fit_lm_recursive #> Recursive [parsnip model] #>  #> parsnip model object #>  #> Fit time:  2ms  #>  #> Call: #> stats::lm(formula = value ~ ., data = data) #>  #> Coefficients: #>     (Intercept)             date       value_lag1       value_lag2   #>       147.32008          0.01273          1.59298          0.76666   #>      value_lag3       value_lag4       value_lag5       value_lag6   #>         0.73081          0.76950          0.76871          0.74755   #>      value_lag7       value_lag8       value_lag9      value_lag10   #>         0.77872          0.72985          0.75257          0.76582   #>     value_lag11      value_lag12  rolling_mean_12   #>         0.79979          1.62469         -9.85822   #>   # Forecasting modeltime_table(     model_fit_lm,     model_fit_lm_recursive ) %>%     update_model_description(2, \"LM - Lag Roll\") %>%     modeltime_forecast(         new_data    = future_data,         actual_data = m750     ) %>%     plot_modeltime_forecast(         .interactive        = FALSE,         .conf_interval_show = FALSE     )   # MULTIPLE TIME SERIES (PANEL DATA) -----  m4_monthly #> # A tibble: 1,574 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M1    1976-06-01  8000 #>  2 M1    1976-07-01  8350 #>  3 M1    1976-08-01  8570 #>  4 M1    1976-09-01  7700 #>  5 M1    1976-10-01  7080 #>  6 M1    1976-11-01  6520 #>  7 M1    1976-12-01  6070 #>  8 M1    1977-01-01  6650 #>  9 M1    1977-02-01  6830 #> 10 M1    1977-03-01  5710 #> # … with 1,564 more rows  FORECAST_HORIZON <- 24  m4_extended <- m4_monthly %>%     group_by(id) %>%     future_frame(         .length_out = FORECAST_HORIZON,         .bind_data  = TRUE     ) %>%     ungroup() #> .date_var is missing. Using: date  # TRANSFORM FUNCTION ---- # - NOTE - We create lags by group lag_transformer_grouped <- function(data){     data %>%         group_by(id) %>%         tk_augment_lags(value, .lags = 1:FORECAST_HORIZON) %>%         ungroup() }  m4_lags <- m4_extended %>%     lag_transformer_grouped()  train_data <- m4_lags %>%     drop_na()  future_data <- m4_lags %>%     filter(is.na(value))  # Modeling Autoregressive Panel Data model_fit_lm_recursive <- linear_reg() %>%     set_engine(\"lm\") %>%     fit(value ~ ., data = train_data) %>%     recursive(         id         = \"id\", # We add an id = \"id\" to specify the groups         transform  = lag_transformer_grouped,         # We use panel_tail() to grab tail by groups         train_tail = panel_tail(train_data, id, FORECAST_HORIZON)     )  modeltime_table(     model_fit_lm_recursive ) %>%     modeltime_forecast(         new_data    = future_data,         actual_data = m4_monthly,         keep_data   = TRUE     ) %>%     group_by(id) %>%     plot_modeltime_forecast(         .interactive = FALSE,         .conf_interval_show = FALSE     )   # }"},{"path":"https://business-science.github.io/modeltime/reference/seasonal_reg.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for Multiple Seasonality Regression Models (TBATS, STLM) — seasonal_reg","title":"General Interface for Multiple Seasonality Regression Models (TBATS, STLM) — seasonal_reg","text":"seasonal_reg() way generate specification Seasonal Decomposition model fitting allows model created using different packages. Currently package forecast.","code":""},{"path":"https://business-science.github.io/modeltime/reference/seasonal_reg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for Multiple Seasonality Regression Models (TBATS, STLM) — seasonal_reg","text":"","code":"seasonal_reg(   mode = \"regression\",   seasonal_period_1 = NULL,   seasonal_period_2 = NULL,   seasonal_period_3 = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/seasonal_reg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for Multiple Seasonality Regression Models (TBATS, STLM) — seasonal_reg","text":"mode single character string type model. possible value model \"regression\". seasonal_period_1 (required) primary seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details . seasonal_period_2 (optional) second seasonal frequency. NULL default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details . seasonal_period_3 (optional) third seasonal frequency. NULL default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details .","code":""},{"path":"https://business-science.github.io/modeltime/reference/seasonal_reg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for Multiple Seasonality Regression Models (TBATS, STLM) — seasonal_reg","text":"data given function saved used determine mode model. seasonal_reg(), mode always \"regression\". model can created using fit() function using following engines: \"tbats\" - Connects forecast::tbats() \"stlm_ets\" - Connects forecast::stlm(), method = \"ets\" \"stlm_arima\" - Connects forecast::stlm(), method = \"arima\"","code":""},{"path":"https://business-science.github.io/modeltime/reference/seasonal_reg.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for Multiple Seasonality Regression Models (TBATS, STLM) — seasonal_reg","text":"standardized parameter names modeltime can mapped original names engine: options can set using set_engine(). engines use forecast::stlm(). Function Parameters: tbats Method: Uses method = \"tbats\", default auto-TBATS. Xregs: Univariate. accept Exogenous Regressors (xregs). Xregs ignored. stlm_ets Method: Uses method = \"stlm_ets\", default auto-ETS. Xregs: Univariate. accept Exogenous Regressors (xregs). Xregs ignored. stlm_arima Method: Uses method = \"stlm_arima\", default auto-ARIMA. Xregs: Multivariate. Can accept Exogenous Regressors (xregs).","code":"## function (y, s.window = 7 + 4 * seq(6), robust = FALSE, method = c(\"ets\",  ##     \"arima\"), modelfunction = NULL, model = NULL, etsmodel = \"ZZN\", lambda = NULL,  ##     biasadj = FALSE, xreg = NULL, allow.multiplicative.trend = FALSE, x = y,  ##     ...)"},{"path":"https://business-science.github.io/modeltime/reference/seasonal_reg.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for Multiple Seasonality Regression Models (TBATS, STLM) — seasonal_reg","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) Seasonal Period Specification period can non-seasonal (seasonal_period = 1 \"none\") yearly seasonal (e.g. monthly time stamps, seasonal_period = 12, seasonal_period = \"12 months\", seasonal_period = \"yearly\"). 3 ways specify: seasonal_period = \"auto\": seasonal period selected based periodicity data (e.g. 12 monthly) seasonal_period = 12: numeric frequency. example, 12 common monthly data seasonal_period = \"1 year\": time-based phrase. example, \"1 year\" convert 12 monthly data. Univariate (xregs, Exogenous Regressors): univariate analysis, must include date date-time feature. Simply use: Formula Interface (recommended): fit(y ~ date) ignore xreg's. XY Interface: fit_xy(x = data[,\"date\"], y = data$y) ignore xreg's. Multivariate (xregs, Exogenous Regressors) tbats engine accept Xregs. stlm_ets engine accept Xregs. stlm_arima engine can accept Xregs xreg parameter populated using fit() fit_xy() function: factor, ordered factor, numeric data used xregs. Date Date-time variables used xregs character data converted factor. Xreg Example: Suppose 3 features: y (target) date (time stamp), month.lbl (labeled month ordered factor). month.lbl exogenous regressor can passed seasonal_reg() using fit(): fit(y ~ date + month.lbl) pass month.lbl exogenous regressor. fit_xy(data[,c(\"date\", \"month.lbl\")], y = data$y) pass x, x data frame containing month.lbl date feature. month.lbl used exogenous regressor. Note date date-time class values excluded xreg.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/seasonal_reg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for Multiple Seasonality Regression Models (TBATS, STLM) — seasonal_reg","text":"","code":"library(dplyr) library(parsnip) library(rsample) library(timetk) library(modeltime)  # Data taylor_30_min #> # A tibble: 4,032 × 2 #>    date                value #>    <dttm>              <dbl> #>  1 2000-06-05 00:00:00 22262 #>  2 2000-06-05 00:30:00 21756 #>  3 2000-06-05 01:00:00 22247 #>  4 2000-06-05 01:30:00 22759 #>  5 2000-06-05 02:00:00 22549 #>  6 2000-06-05 02:30:00 22313 #>  7 2000-06-05 03:00:00 22128 #>  8 2000-06-05 03:30:00 21860 #>  9 2000-06-05 04:00:00 21751 #> 10 2000-06-05 04:30:00 21336 #> # … with 4,022 more rows  # Split Data 80/20 splits <- initial_time_split(taylor_30_min, prop = 0.8)  # ---- STLM ETS ----  # Model Spec model_spec <- seasonal_reg() %>%     set_engine(\"stlm_ets\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) #> frequency = 48 observations per 1 day model_fit #> parsnip model object #>  #> Fit time:  395ms  #> SEASONAL DECOMP: ETS(A,Ad,N) #>  #> # A tibble: 1 × 5 #>      aic    bic   aicc loglik       mse #>    <dbl>  <dbl>  <dbl>  <dbl>     <dbl> #> 1 -6473. -6437. -6473.  3243. 0.0000415   # ---- STLM ARIMA ----  # Model Spec model_spec <- seasonal_reg() %>%     set_engine(\"stlm_arima\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) #> frequency = 48 observations per 1 day model_fit #> parsnip model object #>  #> Fit time:  616ms  #> SEASONAL DECOMP: ARIMA(3,1,2) #>  #> Series: x  #> ARIMA(3,1,2)  #>  #> Coefficients: #>          ar1     ar2      ar3      ma1      ma2 #>       1.0031  0.0782  -0.3096  -0.3203  -0.1378 #> s.e.  0.0838  0.1286   0.0575   0.0875   0.0817 #>  #> sigma^2 estimated as 3.918e-05:  log likelihood=11786.32 #> AIC=-23560.65   AICc=-23560.62   BIC=-23524.18"},{"path":"https://business-science.github.io/modeltime/reference/smooth_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level Exponential Smoothing function for translating modeltime to forecast — smooth_fit_impl","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — smooth_fit_impl","text":"Low-Level Exponential Smoothing function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/smooth_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — smooth_fit_impl","text":"","code":"smooth_fit_impl(   x,   y,   period = \"auto\",   error = \"auto\",   trend = \"auto\",   season = \"auto\",   damping = NULL,   alpha = NULL,   beta = NULL,   gamma = NULL,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/smooth_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — smooth_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. error form error term: \"auto\", \"additive\", \"multiplicative\". error multiplicative, data must non-negative. trend form trend term: \"auto\", \"additive\", \"multiplicative\" \"none\". season form seasonal term: \"auto\", \"additive\", \"multiplicative\" \"none\". damping Apply damping trend: \"auto\", \"damped\", \"none\". alpha Value alpha. NULL, estimated. beta Value beta. NULL, estimated. gamma Value gamma. NULL, estimated. ... Additional arguments passed smooth::es","code":""},{"path":"https://business-science.github.io/modeltime/reference/smooth_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for Exponential Smoothing models — smooth_predict_impl","title":"Bridge prediction function for Exponential Smoothing models — smooth_predict_impl","text":"Bridge prediction function Exponential Smoothing models","code":""},{"path":"https://business-science.github.io/modeltime/reference/smooth_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for Exponential Smoothing models — smooth_predict_impl","text":"","code":"smooth_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/smooth_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for Exponential Smoothing models — smooth_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed smooth::es()","code":""},{"path":"https://business-science.github.io/modeltime/reference/snaive_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level SNAIVE Forecast — snaive_fit_impl","title":"Low-Level SNAIVE Forecast — snaive_fit_impl","text":"Low-Level SNAIVE Forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/snaive_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level SNAIVE Forecast — snaive_fit_impl","text":"","code":"snaive_fit_impl(x, y, id = NULL, seasonal_period = \"auto\", ...)"},{"path":"https://business-science.github.io/modeltime/reference/snaive_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level SNAIVE Forecast — snaive_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit id optional ID feature identify different time series. quoted name. seasonal_period seasonal period forecast future ... currently used","code":""},{"path":"https://business-science.github.io/modeltime/reference/snaive_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for SNAIVE Models — snaive_predict_impl","title":"Bridge prediction function for SNAIVE Models — snaive_predict_impl","text":"Bridge prediction function SNAIVE Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/snaive_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for SNAIVE Models — snaive_predict_impl","text":"","code":"snaive_predict_impl(object, new_data)"},{"path":"https://business-science.github.io/modeltime/reference/snaive_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for SNAIVE Models — snaive_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame.","code":""},{"path":"https://business-science.github.io/modeltime/reference/stlm_arima_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level stlm function for translating modeltime to forecast — stlm_arima_fit_impl","title":"Low-Level stlm function for translating modeltime to forecast — stlm_arima_fit_impl","text":"Low-Level stlm function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/stlm_arima_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level stlm function for translating modeltime to forecast — stlm_arima_fit_impl","text":"","code":"stlm_arima_fit_impl(   x,   y,   period_1 = \"auto\",   period_2 = NULL,   period_3 = NULL,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/stlm_arima_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level stlm function for translating modeltime to forecast — stlm_arima_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period_1 (required) First seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. period_2 (optional) First seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. period_3 (optional) First seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. ... Additional arguments passed forecast::stlm()","code":""},{"path":"https://business-science.github.io/modeltime/reference/stlm_arima_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for ARIMA models — stlm_arima_predict_impl","title":"Bridge prediction function for ARIMA models — stlm_arima_predict_impl","text":"Bridge prediction function ARIMA models","code":""},{"path":"https://business-science.github.io/modeltime/reference/stlm_arima_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for ARIMA models — stlm_arima_predict_impl","text":"","code":"stlm_arima_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/stlm_arima_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for ARIMA models — stlm_arima_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed forecast::forecast()","code":""},{"path":"https://business-science.github.io/modeltime/reference/stlm_ets_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level stlm function for translating modeltime to forecast — stlm_ets_fit_impl","title":"Low-Level stlm function for translating modeltime to forecast — stlm_ets_fit_impl","text":"Low-Level stlm function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/stlm_ets_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level stlm function for translating modeltime to forecast — stlm_ets_fit_impl","text":"","code":"stlm_ets_fit_impl(   x,   y,   period_1 = \"auto\",   period_2 = NULL,   period_3 = NULL,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/stlm_ets_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level stlm function for translating modeltime to forecast — stlm_ets_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period_1 (required) First seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. period_2 (optional) First seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. period_3 (optional) First seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. ... Additional arguments passed forecast::stlm()","code":""},{"path":"https://business-science.github.io/modeltime/reference/stlm_ets_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for ARIMA models — stlm_ets_predict_impl","title":"Bridge prediction function for ARIMA models — stlm_ets_predict_impl","text":"Bridge prediction function ARIMA models","code":""},{"path":"https://business-science.github.io/modeltime/reference/stlm_ets_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for ARIMA models — stlm_ets_predict_impl","text":"","code":"stlm_ets_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/stlm_ets_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for ARIMA models — stlm_ets_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed forecast::forecast()","code":""},{"path":"https://business-science.github.io/modeltime/reference/summarize_accuracy_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize Accuracy Metrics — summarize_accuracy_metrics","title":"Summarize Accuracy Metrics — summarize_accuracy_metrics","text":"internal function used modeltime_accuracy().","code":""},{"path":"https://business-science.github.io/modeltime/reference/summarize_accuracy_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize Accuracy Metrics — summarize_accuracy_metrics","text":"","code":"summarize_accuracy_metrics(data, truth, estimate, metric_set)"},{"path":"https://business-science.github.io/modeltime/reference/summarize_accuracy_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize Accuracy Metrics — summarize_accuracy_metrics","text":"data data.frame containing truth estimate columns. truth column identifier true results (numeric). estimate column identifier predicted results (also numeric). metric_set yardstick::metric_set() used summarize one forecast accuracy (regression) metrics.","code":""},{"path":"https://business-science.github.io/modeltime/reference/summarize_accuracy_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize Accuracy Metrics — summarize_accuracy_metrics","text":"","code":"library(tibble) library(dplyr)  predictions_tbl <- tibble(     group = c(\"model 1\", \"model 1\", \"model 1\",               \"model 2\", \"model 2\", \"model 2\"),     truth = c(1, 2, 3,               1, 2, 3),     estimate = c(1.2, 2.0, 2.5,                  0.9, 1.9, 3.3) )  predictions_tbl %>%     group_by(group) %>%     summarize_accuracy_metrics(         truth, estimate,         metric_set = default_forecast_accuracy_metric_set()     ) #> # A tibble: 2 × 7 #>   group     mae  mape  mase smape  rmse   rsq #>   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1 model 1 0.233 12.2  0.233 12.1  0.311 0.983 #> 2 model 2 0.167  8.33 0.167  8.39 0.191 0.991"},{"path":"https://business-science.github.io/modeltime/reference/table_modeltime_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive Accuracy Tables — table_modeltime_accuracy","title":"Interactive Accuracy Tables — table_modeltime_accuracy","text":"Converts results modeltime_accuracy() either interactive (reactable) static (gt) tables.","code":""},{"path":"https://business-science.github.io/modeltime/reference/table_modeltime_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive Accuracy Tables — table_modeltime_accuracy","text":"","code":"table_modeltime_accuracy(   .data,   .round_digits = 2,   .sortable = TRUE,   .show_sortable = TRUE,   .searchable = TRUE,   .filterable = FALSE,   .expand_groups = TRUE,   .title = \"Accuracy Table\",   .interactive = TRUE,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/table_modeltime_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interactive Accuracy Tables — table_modeltime_accuracy","text":".data tibble output modeltime_accuracy() .round_digits Rounds accuracy metrics specified number digits. NULL, rounding performed. .sortable Allows sorting columns. applied reactable tables. Passed reactable(sortable). .show_sortable Shows sorting. applied reactable tables. Passed reactable(showSortable). .searchable Adds search input. applied reactable tables. Passed reactable(searchable). .filterable Adds filters table columns. applied reactable tables. Passed reactable(filterable). .expand_groups Expands groups dropdowns. applied reactable tables. Passed reactable(defaultExpanded). .title title static (gt) tables. .interactive Return interactive static tables. TRUE, returns reactable table. FALSE, returns static gt table. ... Additional arguments passed reactable::reactable() gt::gt() (depending .interactive selection).","code":""},{"path":"https://business-science.github.io/modeltime/reference/table_modeltime_accuracy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interactive Accuracy Tables — table_modeltime_accuracy","text":"static gt table interactive reactable table containing accuracy information.","code":""},{"path":"https://business-science.github.io/modeltime/reference/table_modeltime_accuracy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Interactive Accuracy Tables — table_modeltime_accuracy","text":"Groups function respects dplyr::group_by() groups thus scales multiple groups. Reactable Output reactable() table interactive format enables live searching sorting. .interactive = TRUE, call made  reactable::reactable(). table_modeltime_accuracy() includes several common options like toggles sorting searching. Additional arguments can passed reactable::reactable() via .... GT Output gt table HTML-based table \"static\" (e.g. non-searchable, non-sortable). commonly used PDF Word documents support interactive content. .interactive = FALSE, call made gt::gt(). Arguments can passed via .... Table customization implemented using piping workflow (%>%). information, refer GT Documentation.","code":""},{"path":"https://business-science.github.io/modeltime/reference/table_modeltime_accuracy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interactive Accuracy Tables — table_modeltime_accuracy","text":"","code":"library(tidyverse) library(lubridate) library(timetk) library(parsnip) library(rsample) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\")  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.9)  # --- MODELS ---  # Model 1: prophet ---- model_fit_prophet <- prophet_reg() %>%     set_engine(engine = \"prophet\") %>%     fit(value ~ date, data = training(splits)) #> Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. #> Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.   # ---- MODELTIME TABLE ----  models_tbl <- modeltime_table(     model_fit_prophet )  # ---- ACCURACY ----  models_tbl %>%     modeltime_calibrate(new_data = testing(splits)) %>%     modeltime_accuracy() %>%     table_modeltime_accuracy() #> Error in x$width %||% settings$fig.width * settings$dpi: non-numeric argument to binary operator"},{"path":"https://business-science.github.io/modeltime/reference/tbats_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level tbats function for translating modeltime to forecast — tbats_fit_impl","title":"Low-Level tbats function for translating modeltime to forecast — tbats_fit_impl","text":"Low-Level tbats function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/tbats_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level tbats function for translating modeltime to forecast — tbats_fit_impl","text":"","code":"tbats_fit_impl(   x,   y,   period_1 = \"auto\",   period_2 = NULL,   period_3 = NULL,   use.parallel = length(y) > 1000,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/tbats_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level tbats function for translating modeltime to forecast — tbats_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period_1 (required) First seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. period_2 (optional) First seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. period_3 (optional) First seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. use.parallel TRUE/FALSE indicates whether use parallel processing. ... Additional arguments passed forecast::tbats()","code":""},{"path":"https://business-science.github.io/modeltime/reference/tbats_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for ARIMA models — tbats_predict_impl","title":"Bridge prediction function for ARIMA models — tbats_predict_impl","text":"Bridge prediction function ARIMA models","code":""},{"path":"https://business-science.github.io/modeltime/reference/tbats_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for ARIMA models — tbats_predict_impl","text":"","code":"tbats_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/tbats_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for ARIMA models — tbats_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed forecast::forecast()","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hier_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level Temporaral Hierarchical function for translating modeltime to forecast — temporal_hier_fit_impl","title":"Low-Level Temporaral Hierarchical function for translating modeltime to forecast — temporal_hier_fit_impl","text":"Low-Level Temporaral Hierarchical function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hier_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level Temporaral Hierarchical function for translating modeltime to forecast — temporal_hier_fit_impl","text":"","code":"temporal_hier_fit_impl(   x,   y,   period = \"auto\",   comb = c(\"struc\", \"mse\", \"ols\", \"bu\", \"shr\", \"sam\"),   usemodel = c(\"ets\", \"arima\", \"theta\", \"naive\", \"snaive\"),   ... )"},{"path":"https://business-science.github.io/modeltime/reference/temporal_hier_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level Temporaral Hierarchical function for translating modeltime to forecast — temporal_hier_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. comb Combination method temporal hierarchies usemodel Model used forecasting aggregation level ... Additional arguments passed forecast::ets","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hier_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for TEMPORAL HIERARCHICAL models — temporal_hier_predict_impl","title":"Bridge prediction function for TEMPORAL HIERARCHICAL models — temporal_hier_predict_impl","text":"Bridge prediction function TEMPORAL HIERARCHICAL models","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hier_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for TEMPORAL HIERARCHICAL models — temporal_hier_predict_impl","text":"","code":"temporal_hier_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/temporal_hier_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for TEMPORAL HIERARCHICAL models — temporal_hier_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed stats::predict()","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for Temporal Hierarchical Forecasting (THIEF) Models — temporal_hierarchy","title":"General Interface for Temporal Hierarchical Forecasting (THIEF) Models — temporal_hierarchy","text":"temporal_hierarchy() way generate specification Temporal Hierarchical Forecasting model fitting allows model created using different packages. Currently package thief. Note function requires thief package installed.","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for Temporal Hierarchical Forecasting (THIEF) Models — temporal_hierarchy","text":"","code":"temporal_hierarchy(   mode = \"regression\",   seasonal_period = NULL,   combination_method = NULL,   use_model = NULL )"},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for Temporal Hierarchical Forecasting (THIEF) Models — temporal_hierarchy","text":"mode single character string type model. possible value model \"regression\". seasonal_period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details . combination_method Combination method temporal hierarchies, taking one following values: \"struc\" - Structural scaling: weights temporal hierarchy \"mse\" - Variance scaling: weights -sample MSE \"ols\" - Unscaled OLS combination weights \"bu\" - Bottom-combination – .e., aggregate forecasts ignored. \"shr\" - GLS using shrinkage (block diagonal) estimate residuals \"sam\" - GLS using sample covariance matrix residuals use_model Model used forecasting aggregation level: \"ets\" - exponential smoothing \"arima\" - arima \"theta\" - theta \"naive\" - random walk forecasts \"snaive\" - seasonal naive forecasts, based last year observed data","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for Temporal Hierarchical Forecasting (THIEF) Models — temporal_hierarchy","text":"Models can created using following engines: \"thief\" (default) - Connects thief::thief()","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for Temporal Hierarchical Forecasting (THIEF) Models — temporal_hierarchy","text":"standardized parameter names modeltime can mapped original names engine: options can set using set_engine(). thief (default engine) engine uses thief::thief(). Function Parameters: options argument can set using set_engine(). Parameter Notes: xreg - model set use exogenous regressors. univariate models fit.","code":"## function (y, m = frequency(y), h = m * 2, comb = c(\"struc\", \"mse\", \"ols\",  ##     \"bu\", \"shr\", \"sam\"), usemodel = c(\"ets\", \"arima\", \"theta\", \"naive\",  ##     \"snaive\"), forecastfunction = NULL, aggregatelist = NULL, ...)"},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for Temporal Hierarchical Forecasting (THIEF) Models — temporal_hierarchy","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) Univariate: univariate analysis, must include date date-time feature. Simply use: Formula Interface (recommended): fit(y ~ date) ignore xreg's. XY Interface: fit_xy(x = data[,\"date\"], y = data$y) ignore xreg's. Multivariate (xregs, Exogenous Regressors) model set use exogenous regressors.","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"General Interface for Temporal Hierarchical Forecasting (THIEF) Models — temporal_hierarchy","text":"forecasting temporal hierarchies see: Athanasopoulos G., Hyndman R.J., Kourentzes N., Petropoulos F. (2017) Forecasting Temporal Hierarchies. European Journal Operational research, 262(1), 60-74. combination operators see: Kourentzes N., Barrow B.K., Crone S.F. (2014) Neural network ensemble operators time series forecasting. Expert Systems Applications, 41(9), 4235-4244.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for Temporal Hierarchical Forecasting (THIEF) Models — temporal_hierarchy","text":"","code":"library(dplyr) library(parsnip) library(rsample) library(timetk) library(modeltime) library(thief)  # Data m750 <- m4_monthly %>% filter(id == \"M750\") m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # ---- HIERARCHICAL ----  # Model Spec - The default parameters are all set # to \"auto\" if none are provided model_spec <- temporal_hierarchy() %>%     set_engine(\"thief\")  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) #> frequency = 12 observations per 1 year model_fit #> parsnip model object #>  #> Fit time:  1.6s  #>         Jan      Feb      Mar      Apr      May      Jun      Jul      Aug #> 21                                     9.292876 9.278808 9.177530 9.161701 #> 22 9.292288 9.290665 9.310183 9.314852 9.318892 9.304824 9.203573 9.187635 #> 23 9.318275 9.316547 9.336091 9.340759                                     #>         Sep      Oct      Nov      Dec #> 21 9.210732 9.279095 9.292842 9.296845 #> 22 9.236714 9.305078 9.318782 9.322784 #> 23"},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuning Parameters for TEMPORAL HIERARCHICAL Models — temporal_hierarchy_params","title":"Tuning Parameters for TEMPORAL HIERARCHICAL Models — temporal_hierarchy_params","text":"Tuning Parameters TEMPORAL HIERARCHICAL Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning Parameters for TEMPORAL HIERARCHICAL Models — temporal_hierarchy_params","text":"","code":"combination_method()  use_model()"},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy_params.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tuning Parameters for TEMPORAL HIERARCHICAL Models — temporal_hierarchy_params","text":"main parameters Temporal Hierarchical models : combination_method: Combination method temporal hierarchies. use_model: Model used forecasting aggregation level.","code":""},{"path":"https://business-science.github.io/modeltime/reference/temporal_hierarchy_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tuning Parameters for TEMPORAL HIERARCHICAL Models — temporal_hierarchy_params","text":"","code":"combination_method() #> Combination method of temporal hierarchies.  (qualitative) #> 6 possible value include: #> 'struc', 'mse', 'ols', 'bu', 'shr' and 'sam'   use_model() #> Model used for forecasting each aggregation level.  (qualitative) #> 5 possible value include: #> 'ets', 'arima', 'theta', 'naive' and 'snaive'"},{"path":"https://business-science.github.io/modeltime/reference/theta_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level Exponential Smoothing function for translating modeltime to forecast — theta_fit_impl","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — theta_fit_impl","text":"Low-Level Exponential Smoothing function translating modeltime forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/theta_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — theta_fit_impl","text":"","code":"theta_fit_impl(x, y, ...)"},{"path":"https://business-science.github.io/modeltime/reference/theta_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level Exponential Smoothing function for translating modeltime to forecast — theta_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit ... Additional arguments passed forecast::ets","code":""},{"path":"https://business-science.github.io/modeltime/reference/theta_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for THETA models — theta_predict_impl","title":"Bridge prediction function for THETA models — theta_predict_impl","text":"Bridge prediction function THETA models","code":""},{"path":"https://business-science.github.io/modeltime/reference/theta_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for THETA models — theta_predict_impl","text":"","code":"theta_predict_impl(object, new_data, ...)"},{"path":"https://business-science.github.io/modeltime/reference/theta_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for THETA models — theta_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame. ... Additional arguments passed stats::predict()","code":""},{"path":"https://business-science.github.io/modeltime/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"sym() creates symbol string syms() creates list symbols character vector. enquo() enquos() delay execution one several function arguments. enquo() returns single quoted expression, like blueprint delayed computation. enquos() returns list quoted expressions. expr() quotes new expression locally. mostly useful build new expressions around arguments captured enquo() enquos(): expr(mean(!!enquo(arg), na.rm = TRUE)). as_name() transforms quoted variable name string. Supplying something else quoted variable name error. unlike as_label() also returns single string supports kind R object input, including quoted function calls vectors. purpose summarise object single label. label often suitable default name. know quoted expression contains (instance expressions captured enquo() variable name, call function, unquoted constant), use as_label(). know quoted simple variable name, like enforce , use as_name(). learn tidy eval use tools, visit https://tidyeval.tidyverse.org Metaprogramming section Advanced R.","code":""},{"path":"https://business-science.github.io/modeltime/reference/time_series_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuning Parameters for Time Series (ts-class) Models — time_series_params","title":"Tuning Parameters for Time Series (ts-class) Models — time_series_params","text":"Tuning Parameters Time Series (ts-class) Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/time_series_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning Parameters for Time Series (ts-class) Models — time_series_params","text":"","code":"seasonal_period(values = c(\"none\", \"daily\", \"weekly\", \"yearly\"))"},{"path":"https://business-science.github.io/modeltime/reference/time_series_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuning Parameters for Time Series (ts-class) Models — time_series_params","text":"values time-based phrase","code":""},{"path":"https://business-science.github.io/modeltime/reference/time_series_params.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tuning Parameters for Time Series (ts-class) Models — time_series_params","text":"Time series models (e.g. Arima() ets()) use stats::ts() forecast::msts() apply seasonality. can process using following general time series parameter: period: periodic nature seasonality. usually best practice tune parameter, rather set obvious values based seasonality data: Daily Seasonality: Often used hourly data (e.g. 24 hourly timestamps per day) Weekly Seasonality: Often used daily data (e.g. 7 daily timestamps per week) Yearly Seasonalty: Often used weekly, monthly, quarterly data (e.g. 12 monthly observations per year). However, event users want experiment period tuning, can seasonal_period().","code":""},{"path":"https://business-science.github.io/modeltime/reference/time_series_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tuning Parameters for Time Series (ts-class) Models — time_series_params","text":"","code":"seasonal_period() #> Period (Seasonal Frequency)  (qualitative) #> 4 possible value include: #> 'none', 'daily', 'weekly' and 'yearly'"},{"path":"https://business-science.github.io/modeltime/reference/type_sum.mdl_time_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Succinct summary of Modeltime Tables — type_sum.mdl_time_tbl","title":"Succinct summary of Modeltime Tables — type_sum.mdl_time_tbl","text":"type_sum controls objects shown inside tibble columns.","code":""},{"path":"https://business-science.github.io/modeltime/reference/type_sum.mdl_time_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Succinct summary of Modeltime Tables — type_sum.mdl_time_tbl","text":"","code":"# S3 method for mdl_time_tbl type_sum(x)"},{"path":"https://business-science.github.io/modeltime/reference/type_sum.mdl_time_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Succinct summary of Modeltime Tables — type_sum.mdl_time_tbl","text":"x mdl_time_tbl object summarise.","code":""},{"path":"https://business-science.github.io/modeltime/reference/type_sum.mdl_time_tbl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Succinct summary of Modeltime Tables — type_sum.mdl_time_tbl","text":"character value.","code":""},{"path":"https://business-science.github.io/modeltime/reference/update_model_description.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the model description by model id in a Modeltime Table — update_model_description","title":"Update the model description by model id in a Modeltime Table — update_model_description","text":"update_model_description() update_modeltime_description() functions synonyms.","code":""},{"path":"https://business-science.github.io/modeltime/reference/update_model_description.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the model description by model id in a Modeltime Table — update_model_description","text":"","code":"update_model_description(object, .model_id, .new_model_desc)  update_modeltime_description(object, .model_id, .new_model_desc)"},{"path":"https://business-science.github.io/modeltime/reference/update_model_description.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the model description by model id in a Modeltime Table — update_model_description","text":"object Modeltime Table .model_id numeric value matching .model_id want update .new_model_desc Text describing new model description","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/update_model_description.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update the model description by model id in a Modeltime Table — update_model_description","text":"","code":"m750_models %>%     update_modeltime_description(2, \"PROPHET - No Regressors\") #> # Modeltime Table #> # A tibble: 3 × 3 #>   .model_id .model     .model_desc             #>       <int> <list>     <chr>                   #> 1         1 <workflow> ARIMA(0,1,1)(0,1,1)[12] #> 2         2 <workflow> PROPHET - No Regressors #> 3         3 <workflow> GLMNET"},{"path":"https://business-science.github.io/modeltime/reference/update_modeltime_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the model by model id in a Modeltime Table — update_modeltime_model","title":"Update the model by model id in a Modeltime Table — update_modeltime_model","text":"Update model model id Modeltime Table","code":""},{"path":"https://business-science.github.io/modeltime/reference/update_modeltime_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the model by model id in a Modeltime Table — update_modeltime_model","text":"","code":"update_modeltime_model(object, .model_id, .new_model)"},{"path":"https://business-science.github.io/modeltime/reference/update_modeltime_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the model by model id in a Modeltime Table — update_modeltime_model","text":"object Modeltime Table .model_id numeric value matching .model_id want update .new_model fitted workflow, model_fit, mdl_time_ensmble object","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/update_modeltime_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update the model by model id in a Modeltime Table — update_modeltime_model","text":"","code":"# \\donttest{ library(tidymodels)  model_fit_ets <- exp_smoothing() %>%     set_engine(\"ets\") %>%     fit(value ~ date, training(m750_splits)) #> frequency = 12 observations per 1 year  m750_models %>%     update_modeltime_model(1, model_fit_ets) #> # Modeltime Table #> # A tibble: 3 × 3 #>   .model_id .model     .model_desc #>       <int> <list>     <chr>       #> 1         1 <fit[+]>   ETS(A,A,A)  #> 2         2 <workflow> PROPHET     #> 3         3 <workflow> GLMNET      # }"},{"path":"https://business-science.github.io/modeltime/reference/window_function_fit_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Low-Level Window Forecast — window_function_fit_impl","title":"Low-Level Window Forecast — window_function_fit_impl","text":"Low-Level Window Forecast","code":""},{"path":"https://business-science.github.io/modeltime/reference/window_function_fit_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Low-Level Window Forecast — window_function_fit_impl","text":"","code":"window_function_fit_impl(   x,   y,   id = NULL,   window_size = \"all\",   window_function = NULL,   ... )"},{"path":"https://business-science.github.io/modeltime/reference/window_function_fit_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Low-Level Window Forecast — window_function_fit_impl","text":"x dataframe xreg (exogenous regressors) y numeric vector values fit id optional ID feature identify different time series. quoted name. window_size period apply window function window_function function apply window. default mean(). ... Additional arguments window_function. example, common pass na.rm = TRUE mean forecast.","code":""},{"path":"https://business-science.github.io/modeltime/reference/window_function_predict_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Bridge prediction function for window Models — window_function_predict_impl","title":"Bridge prediction function for window Models — window_function_predict_impl","text":"Bridge prediction function window Models","code":""},{"path":"https://business-science.github.io/modeltime/reference/window_function_predict_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bridge prediction function for window Models — window_function_predict_impl","text":"","code":"window_function_predict_impl(object, new_data)"},{"path":"https://business-science.github.io/modeltime/reference/window_function_predict_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bridge prediction function for window Models — window_function_predict_impl","text":"object object class model_fit new_data rectangular data object, data frame.","code":""},{"path":"https://business-science.github.io/modeltime/reference/window_reg.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for Window Forecast Models — window_reg","title":"General Interface for Window Forecast Models — window_reg","text":"window_reg() way generate specification window model fitting allows model created using different backends.","code":""},{"path":"https://business-science.github.io/modeltime/reference/window_reg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for Window Forecast Models — window_reg","text":"","code":"window_reg(mode = \"regression\", id = NULL, window_size = NULL)"},{"path":"https://business-science.github.io/modeltime/reference/window_reg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for Window Forecast Models — window_reg","text":"mode single character string type model. possible value model \"regression\". id optional quoted column name (e.g. \"id\") identifying multiple time series (.e. panel data). window_size window apply window function. default, window uses full data set, rarely best choice.","code":""},{"path":"https://business-science.github.io/modeltime/reference/window_reg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"General Interface for Window Forecast Models — window_reg","text":"time series window regression derived using window_reg(). model can created using fit() function using following engines: \"window_function\" (default) - Performs Window Forecast applying window_function (engine parameter) window size defined window_size","code":""},{"path":"https://business-science.github.io/modeltime/reference/window_reg.html","id":"engine-details","dir":"Reference","previous_headings":"","what":"Engine Details","title":"General Interface for Window Forecast Models — window_reg","text":"function (default engine) engine uses window_function_fit_impl(). time series window function applies window_function window data (last N observations). function can return scalar (single value) multiple values repeated window Common use cases: Moving Average Forecasts: Forecast forward 20-day average Weighted Average Forecasts: Exponentially weighting recent observations Median Forecasts: Forecasting forward 20-day median Repeating Forecasts: Simulating Seasonal Naive Forecast broadcasting last 12 observations monthly dataset future key engine parameter window_function. function / formula: function, e.g. mean, function used additional arguments, ... set_engine(). formula, e.g. ~ mean(., na.rm = TRUE), converted function. syntax allows create compact anonymous functions.","code":""},{"path":"https://business-science.github.io/modeltime/reference/window_reg.html","id":"fit-details","dir":"Reference","previous_headings":"","what":"Fit Details","title":"General Interface for Window Forecast Models — window_reg","text":"Date Date-Time Variable requirement date date-time variable predictor. fit() interface accepts date date-time features handles internally. fit(y ~ date) ID features (Multiple Time Series, Panel Data) id parameter populated using fit() fit_xy() function: ID Example: Suppose 3 features: y (target) date (time stamp), series_id (unique identifer identifies time series data). series_id can passed window_reg() using fit(): window_reg(id = \"series_id\") specifes series_id column used identify time series. fit(y ~ date + series_id) pass series_id underlying functions. Window Function Specification (window_function) can specify function / formula using purrr syntax. function, e.g. mean, function used additional arguments, ... set_engine(). formula, e.g. ~ mean(., na.rm = TRUE), converted function. syntax allows create compact anonymous functions. Window Size Specification (window_size) period can non-seasonal (window_size = 1 \"none\") yearly seasonal (e.g. monthly time stamps, window_size = 12, window_size = \"12 months\", window_size = \"yearly\"). 3 ways specify: window_size = \"\": seasonal period selected based periodicity data (e.g. 12 monthly) window_size = 12: numeric frequency. example, 12 common monthly data window_size = \"1 year\": time-based phrase. example, \"1 year\" convert 12 monthly data. External Regressors (Xregs) models univariate. xregs used modeling process.","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/reference/window_reg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"General Interface for Window Forecast Models — window_reg","text":"","code":"library(dplyr) library(parsnip) library(rsample) library(timetk) library(modeltime)  # Data m750 <- m4_monthly %>% filter(id == \"M750\") m750 #> # A tibble: 306 × 3 #>    id    date       value #>    <fct> <date>     <dbl> #>  1 M750  1990-01-01  6370 #>  2 M750  1990-02-01  6430 #>  3 M750  1990-03-01  6520 #>  4 M750  1990-04-01  6580 #>  5 M750  1990-05-01  6620 #>  6 M750  1990-06-01  6690 #>  7 M750  1990-07-01  6000 #>  8 M750  1990-08-01  5450 #>  9 M750  1990-09-01  6480 #> 10 M750  1990-10-01  6820 #> # … with 296 more rows  # Split Data 80/20 splits <- initial_time_split(m750, prop = 0.8)  # ---- WINDOW FUNCTION -----  # Used to make: # - Mean/Median forecasts # - Simple repeating forecasts  # Median Forecast ----  # Model Spec model_spec <- window_reg(         window_size     = 12     ) %>%     # Extra parameters passed as: set_engine(...)     set_engine(         engine          = \"window_function\",         window_function = median,         na.rm           = TRUE     )  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  14ms  #> WINDOW FUNC [12] #> -------- #> Model:  #> # A tibble: 1 × 1 #>   value #>   <dbl> #> 1  9.26  # Predict # - The 12-month median repeats going forward predict(model_fit, testing(splits)) #> # A tibble: 62 × 1 #>    .pred #>    <dbl> #>  1  9.26 #>  2  9.26 #>  3  9.26 #>  4  9.26 #>  5  9.26 #>  6  9.26 #>  7  9.26 #>  8  9.26 #>  9  9.26 #> 10  9.26 #> # … with 52 more rows   # ---- PANEL FORECAST - WINDOW FUNCTION ----  # Weighted Average Forecast model_spec <- window_reg(         # Specify the ID column for Panel Data         id          = \"id\",         window_size = 12     ) %>%     set_engine(         engine = \"window_function\",         # Create a Weighted Average         window_function = ~ sum(tail(.x, 3) * c(0.1, 0.3, 0.6)),     )  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date + id, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  17ms  #> WINDOW FUNC [12] #> -------- #> Model:  #> # A tibble: 1 × 2 #>   id    value #>   <fct> <dbl> #> 1 M750   9.29  # Predict: The weighted average (scalar) repeats going forward predict(model_fit, testing(splits)) #> # A tibble: 62 × 1 #>    .pred #>    <dbl> #>  1  9.29 #>  2  9.29 #>  3  9.29 #>  4  9.29 #>  5  9.29 #>  6  9.29 #>  7  9.29 #>  8  9.29 #>  9  9.29 #> 10  9.29 #> # … with 52 more rows  # ---- BROADCASTING PANELS (REPEATING) ----  # Simulating a Seasonal Naive Forecast by # broadcasted model the last 12 observations into the future model_spec <- window_reg(         id          = \"id\",         window_size = Inf     ) %>%     set_engine(         engine          = \"window_function\",         window_function = ~ tail(.x, 12),     )  # Fit Spec model_fit <- model_spec %>%     fit(log(value) ~ date + id, data = training(splits)) model_fit #> parsnip model object #>  #> Fit time:  17ms  #> WINDOW FUNC [Inf] #> -------- #> Model:  #> # A tibble: 12 × 2 #>    id    value #>    <fct> <dbl> #>  1 M750   9.27 #>  2 M750   9.27 #>  3 M750   9.15 #>  4 M750   9.19 #>  5 M750   9.18 #>  6 M750   9.25 #>  7 M750   9.26 #>  8 M750   9.27 #>  9 M750   9.26 #> 10 M750   9.26 #> 11 M750   9.29 #> 12 M750   9.29  # Predict: The sequence is broadcasted (repeated) during prediction predict(model_fit, testing(splits)) #> # A tibble: 62 × 1 #>    .pred #>    <dbl> #>  1  9.27 #>  2  9.27 #>  3  9.15 #>  4  9.19 #>  5  9.18 #>  6  9.25 #>  7  9.26 #>  8  9.27 #>  9  9.26 #> 10  9.26 #> # … with 52 more rows"},{"path":"https://business-science.github.io/modeltime/reference/xgboost_impl.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for parsnip::xgb_train — xgboost_impl","title":"Wrapper for parsnip::xgb_train — xgboost_impl","text":"Wrapper parsnip::xgb_train","code":""},{"path":"https://business-science.github.io/modeltime/reference/xgboost_impl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper for parsnip::xgb_train — xgboost_impl","text":"","code":"xgboost_impl(   x,   y,   max_depth = 6,   nrounds = 15,   eta = 0.3,   colsample_bynode = NULL,   colsample_bytree = NULL,   min_child_weight = 1,   gamma = 0,   subsample = 1,   validation = 0,   early_stop = NULL,   objective = NULL,   counts = TRUE,   event_level = c(\"first\", \"second\"),   ... )"},{"path":"https://business-science.github.io/modeltime/reference/xgboost_impl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper for parsnip::xgb_train — xgboost_impl","text":"x data frame matrix predictors y vector (factor numeric) matrix (numeric) outcome data. max_depth integer maximum depth tree. nrounds integer number boosting iterations. eta numeric value zero one control learning rate. colsample_bynode Subsampling proportion columns node within tree. See counts argument . default uses columns. colsample_bytree Subsampling proportion columns tree. See counts argument . default uses columns. min_child_weight numeric value minimum sum instance weights needed child continue split. gamma number minimum loss reduction required make partition leaf node tree subsample Subsampling proportion rows. default, training data used. validation positive number. [0, 1) value, validation random proportion data x y used performance assessment potential early stopping. 1 greater, number training set samples use purposes. early_stop integer NULL. NULL, number training iterations without improvement stopping. validation used, performance base validation set; otherwise training set used. objective single string (NULL) defines loss function xgboost uses create trees. See xgboost::xgb.train() options. left NULL, appropriate loss function chosen. counts logical. FALSE, colsample_bynode colsample_bytree assumed proportions proportion columns affects (instead counts). event_level binary classification, single string either \"first\" \"second\" pass along describing level outcome considered \"event\". ... options pass xgb.train.","code":""},{"path":"https://business-science.github.io/modeltime/reference/xgboost_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for xgboost::predict — xgboost_predict","title":"Wrapper for xgboost::predict — xgboost_predict","text":"Wrapper xgboost::predict","code":""},{"path":"https://business-science.github.io/modeltime/reference/xgboost_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper for xgboost::predict — xgboost_predict","text":"","code":"xgboost_predict(object, newdata, ...)"},{"path":"https://business-science.github.io/modeltime/reference/xgboost_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper for xgboost::predict — xgboost_predict","text":"object model object prediction desired. newdata New data predicted ... additional arguments affecting predictions produced.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-111","dir":"Changelog","previous_headings":"","what":"modeltime 1.1.1","title":"modeltime 1.1.1","text":"CRAN release: 2022-01-12","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"fixes-1-1-1","dir":"Changelog","previous_headings":"","what":"Fixes","title":"modeltime 1.1.1","text":"Fixes issue incorrect order forecasts #142","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-110","dir":"Changelog","previous_headings":"","what":"modeltime 1.1.0","title":"modeltime 1.1.0","text":"CRAN release: 2021-10-18","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"spark-backend-1-1-0","dir":"Changelog","previous_headings":"","what":"Spark Backend","title":"modeltime 1.1.0","text":"Modeltime now Spark Backend NEW Vignette - Modeltime Spark Backend describing set Modeltime Spark Backend.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-algorithms-smooth-package-integration-1-1-0","dir":"Changelog","previous_headings":"","what":"New Algorithms: Smooth Package Integration","title":"modeltime 1.1.0","text":"users install smooth, following models become available: adam_reg(): Interfaces ADAM forecasting algorithm smooth. exp_smoothing(): new engine “smooth_es” connects Exponential Smoothing algorithm smooth::es(). algorithm several advantages, importantly can use x-regs (unlike “ets” engine).","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"nested-modeltime-improvements-1-1-0","dir":"Changelog","previous_headings":"","what":"Nested Modeltime Improvements","title":"modeltime 1.1.0","text":"New extractor: extract_nested_modeltime_table() - Extracts nested modeltime table row id.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"potentially-breaking-changes-1-1-0","dir":"Changelog","previous_headings":"","what":"(potentially) Breaking Changes","title":"modeltime 1.1.0","text":"extract_nested_train_split extract_nested_test_split: Changed parameter .data .object consistency “extract” functions Added new logged feature modeltime_nested_fit() track attribute “metric_set”, needed ensembles. Old nested modeltime objects need re-run get new attribute. used ensembles.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-100","dir":"Changelog","previous_headings":"","what":"modeltime 1.0.0","title":"modeltime 1.0.0","text":"CRAN release: 2021-09-14","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-feature-nested-iterative-forecasting-1-0-0","dir":"Changelog","previous_headings":"","what":"New Feature: Nested (Iterative) Forecasting","title":"modeltime 1.0.0","text":"Nested (Iterative) Forecasting aimed making easier perform forecasting traditionally done -loop models like ARIMA, Prophet, Exponential Smoothing. Functionality added :","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"format-data-in-a-nested-time-series-structure-1-0-0","dir":"Changelog","previous_headings":"New Feature: Nested (Iterative) Forecasting","what":"Format data in a Nested Time Series structure","title":"modeltime 1.0.0","text":"Data Preparation Utilities: extend_timeseries(), nest_timeseries(), split_nested_timeseris().","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"nested-model-fitting-traintest-1-0-0","dir":"Changelog","previous_headings":"New Feature: Nested (Iterative) Forecasting","what":"Nested Model Fitting (Train/Test)","title":"modeltime 1.0.0","text":"modeltime_nested_fit(): Fits many models nested time series data organizes “Nested Modeltime Table”. Logs Accuracy, Errors, Test Forecasts. control_nested_fit(): Used control fitting process including verbosity parallel processing. Logging Extractors: Functions retrieve logged information initial fitting process. extract_nested_test_accuracy(), extract_nested_error_report(), extract_nested_test_forecast().","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"nested-model-selection-1-0-0","dir":"Changelog","previous_headings":"New Feature: Nested (Iterative) Forecasting","what":"Nested Model Selection","title":"modeltime 1.0.0","text":"modeltime_nested_select_best(): Selects best model time series ID. Logging Extractors: Functions retrieve logged information model selection process. extract_nested_best_model_report()","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"nested-model-refitting-actual-data-1-0-0","dir":"Changelog","previous_headings":"New Feature: Nested (Iterative) Forecasting","what":"Nested Model Refitting (Actual Data)","title":"modeltime 1.0.0","text":"modeltime_nested_refit(): Refits .future_data. Logs Future Forecasts. control_nested_refit(): Used control re-fitting process including verbosity parallel processing. Logging Extractors: Functions retrieve logged information re-fitting process. extract_nested_future_forecast().","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-vignette-1-0-0","dir":"Changelog","previous_headings":"","what":"New Vignette","title":"modeltime 1.0.0","text":"Nested Forecasting","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"vignette-improvements-1-0-0","dir":"Changelog","previous_headings":"","what":"Vignette Improvements","title":"modeltime 1.0.0","text":"Forecasting Global Models: Added complete steps forecasting process now user can see forecast step start finish including future forecasting.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-accuracy-metric-set-and-yardstick-functions-1-0-0","dir":"Changelog","previous_headings":"","what":"New Accuracy Metric Set and Yardstick Functions","title":"modeltime 1.0.0","text":"extended_forecast_accuracy_metric_set(): Adds new MAAPE metric handling intermittent data MAPE returns Inf. maape(): New yardstick metric calculates “Mean Arctangent Absolute Percentage Error” (MAAPE). Used MAPE returns Inf typically due intermittent data.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"improvements-1-0-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"modeltime 1.0.0","text":"modeltime_fit_workflowset(): Improved handling Workflowset Descriptions, now match wflow_id.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-070","dir":"Changelog","previous_headings":"","what":"modeltime 0.7.0","title":"modeltime 0.7.0","text":"CRAN release: 2021-07-16","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"group-wise-accuracy-and-confidence-interval-by-time-series-id-0-7-0","dir":"Changelog","previous_headings":"","what":"Group-Wise Accuracy and Confidence Interval by Time Series ID","title":"modeltime 0.7.0","text":"’ve expanded Panel Data functionality produce model accuracy confidence interval estimates Time Series ID (#114). useful Global Model produces forecasts one time series. can easily obtain grouped accuracy confidence interval estimates. modeltime_calibrate(): Gains id argument quoted column name. identifies residuals tracked time series identifier feature indicates time series groups. modeltime_accuracy(): Gains acc_by_id argument TRUE/FALSE. data calibrated id, user can return local model accuracy identifier column. accuracy data frame return row combination Model ID Time Series ID. modeltime_forecast(): Gains conf_by_id argument TRUE/FALSE. data calibrated id, user can return local model confidence identifier column. forecast data frame return extra column indicating identifier column. confidence intervals adjusted based local time series ID variance instead global model variance.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-vignette-0-7-0","dir":"Changelog","previous_headings":"","what":"New Vignette","title":"modeltime 0.7.0","text":"Forecasting Panel Data","code":""},{"path":[]},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"thief-temporal-hierarchical-forecasting-0-7-0","dir":"Changelog","previous_headings":"New Algorithms","what":"THIEF: Temporal Hierarchical Forecasting","title":"modeltime 0.7.0","text":"temporal_hierarchy(): Implements thief package Rob Hyndman Nikolaos Kourentzes “Temporal HIErarchical Forecasting”. #117","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"bug-fixes-0-7-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"modeltime 0.7.0","text":"Issue #111: Fix bug modeltime_fit_workflowset() workflowset (wflw_id) order maintained.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-061","dir":"Changelog","previous_headings":"","what":"modeltime 0.6.1","title":"modeltime 0.6.1","text":"CRAN release: 2021-06-13 Parallel Processing New Vignette: Parallel Processing parallel_start() parallel_stop(): Helpers setting multicore processing. create_model_grid(): Helper generate model specifications filled-parameters parameter grid (e.g. dials::grid_regular()). control_refit() control_fit_workflowset(): Better printing. Bug Fixes Issue #110: Fix bug cores > cores_available.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-060","dir":"Changelog","previous_headings":"","what":"modeltime 0.6.0","title":"modeltime 0.6.0","text":"CRAN release: 2021-05-30","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"workflowset-integration-0-6-0","dir":"Changelog","previous_headings":"","what":"Workflowset Integration","title":"modeltime 0.6.0","text":"modeltime_fit_workflowset() (#85) makes easy convert workflow_set objects Modeltime Tables (mdl_time_tbl). Requires refitting process can now performed parallel sequence.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-algorithms-0-6-0","dir":"Changelog","previous_headings":"","what":"New Algorithms","title":"modeltime 0.6.0","text":"CROSTON (#5, #98) - new engine added exp_smoothing(). THETA (#5, #93) - new engine added exp_smoothing().","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-dials-parameters-0-6-0","dir":"Changelog","previous_headings":"","what":"New Dials Parameters","title":"modeltime 0.6.0","text":"exp_smoothing() gained 3 new tunable parameters: smooth_level(): often called “alpha” parameter used base level smoothing factor exponential smoothing models. smooth_trend(): often called “beta” parameter used trend smoothing factor exponential smoothing models. smooth_seasonal(): often called “gamma” parameter used seasonal smoothing factor exponential smoothing models.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"parallel-processing-0-6-0","dir":"Changelog","previous_headings":"","what":"Parallel Processing","title":"modeltime 0.6.0","text":"modeltime_refit(): supports parallel processing. See control_refit() modeltime_fit_workflowset(): supports parallel processing. See control_workflowset()","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"updates-for-parsnip--0-6-0","dir":"Changelog","previous_headings":"","what":"Updates for parsnip >= 0.1.6","title":"modeltime 0.6.0","text":"boost_tree(mtry): Mapping switched colsample_bytree colsample_bynode. prophet_boost() arima_boost() updated reflect change. https://github.com/tidymodels/parsnip/pull/499","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"general-improvements-0-6-0","dir":"Changelog","previous_headings":"","what":"General Improvements","title":"modeltime 0.6.0","text":"Improve Model Description Recursive Models (#96)","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"potential-breaking-changes-0-6-0","dir":"Changelog","previous_headings":"","what":"Potential Breaking Changes","title":"modeltime 0.6.0","text":"’ve added new parameters Exponential Smoothing Models. exp_smoothing() models produced prior versions may require refitting modeltime_refit() upgrade internals new parameters.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-051","dir":"Changelog","previous_headings":"","what":"modeltime 0.5.1","title":"modeltime 0.5.1","text":"CRAN release: 2021-04-03","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"recursive-ensemble-predictions-0-5-1","dir":"Changelog","previous_headings":"","what":"Recursive Ensemble Predictions","title":"modeltime 0.5.1","text":"Add support recursive() ensembles. new recursive ensemble functionality modeltime.ensemble >= 0.3.0.9000.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-050","dir":"Changelog","previous_headings":"","what":"modeltime 0.5.0","title":"modeltime 0.5.0","text":"CRAN release: 2021-03-29","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"recursive-panel-predictions-0-5-0","dir":"Changelog","previous_headings":"","what":"Recursive Panel Predictions","title":"modeltime 0.5.0","text":"recursive() (#71) - Received full upgrade work Panel Data. New Vignette: “Autoregressive Forecasting Recursive”","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"breaking-changes-0-5-0","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"modeltime 0.5.0","text":"Deprecating modeltime::metric_tweak() yardstick::metric_tweak(). yardstick::metric_tweak() required .name argument addition .fn, needed tuning.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-042","dir":"Changelog","previous_headings":"","what":"modeltime 0.4.2","title":"modeltime 0.4.2","text":"CRAN release: 2021-03-19","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-algorithms-0-4-2","dir":"Changelog","previous_headings":"","what":"New Algorithms","title":"modeltime 0.4.2","text":"Baseline algorithms (#5, #37) created comparing high-performance methods simple forecasting methods. window_reg: Window-based methods mean, median, even complex seasonal models based forecasting window. main tuning parameter window_size. naive_reg: NAIVE Seasonal NAIVE (SNAIVE) Regression Models","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"yardstick-helpers-0-4-2","dir":"Changelog","previous_headings":"","what":"Yardstick Helpers","title":"modeltime 0.4.2","text":"metric_tweak() - Can modify yardstick metrics like mase(), seasonal parameters. default_forecast_accuracy_metric_set() - Gets ... parameter allows us add metrics beyond defaults.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-residual-tests-0-4-2","dir":"Changelog","previous_headings":"","what":"Modeltime Residual Tests","title":"modeltime 0.4.2","text":"new function added modeltime_residuals_test() (#62, #68). Tests implemented: Shapiro Test - Test Normality residuals Box-Pierce, Ljung-Box, Durbin-Watson Tests - Test Autocorrelation residuals","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"fixes-0-4-2","dir":"Changelog","previous_headings":"","what":"Fixes","title":"modeltime 0.4.2","text":"plot_modeltime_forecast() - plotting single point forecast, plot_modeltime_forecast() now uses geom_point() instead geom_line(). Fixes #66.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-041","dir":"Changelog","previous_headings":"","what":"modeltime 0.4.1","title":"modeltime 0.4.1","text":"CRAN release: 2021-01-17 Fixes recursive() & modeltime_refit(): Now able refit recursive workflow recursive fitted parsnip object.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-040","dir":"Changelog","previous_headings":"","what":"modeltime 0.4.0","title":"modeltime 0.4.0","text":"CRAN release: 2020-11-23 New Functions recursive(): Turn fitted model recursive predictor. (#49, #50) update_modeltime_model(): New function update modeltime model inside Modeltime Table. Breaking Changes Removed arima_workflow_tuned dataset.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-031","dir":"Changelog","previous_headings":"","what":"modeltime 0.3.1","title":"modeltime 0.3.1","text":"CRAN release: 2020-11-09 as_modeltime_table(): New function convert one fitted models stored list Modeltime Table. Bug Fixes Update m750_models: Fixes error “R parsnip Error: Internal error: Unknown composition type.”","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-030","dir":"Changelog","previous_headings":"","what":"modeltime 0.3.0","title":"modeltime 0.3.0","text":"CRAN release: 2020-10-28 Panel Data modeltime_forecast() upgrades: keep_data: Gains new argument keep_data. useful new_data actual_data important information needed analyzing forecast. arrange_index: Gains new argument arrange_index. default, forecast keeps rows order incoming data. Prior versions arranged Model Predictions .index, impacts users ability match Panel Data likely arranged date. Prediction best-practices keep original order data, preserved default. get old behavior, simply toggle arrange_index = TRUE. modeltime_calibrate(): Can now handle panel data. modeltime_accuracy(): Can now handle panel data. plot_modeltime_forecast(): Can handle panel data provided data grouped ID column prior plotting. Error Messaging Calibration: Improve error messaging calibration. Provide warnings models fail. Provide report modeltime_calibrate(quiet = FALSE). Compatibility Compatibility parsnip >= 0.1.4. Uses set_encodings() new parameter allow_sparse_x.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-021","dir":"Changelog","previous_headings":"","what":"modeltime 0.2.1","title":"modeltime 0.2.1","text":"CRAN release: 2020-10-08 Ensembles modeltime_refit() - Changes improve fault tolerance error handling / messaging making ensembles.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-020","dir":"Changelog","previous_headings":"","what":"modeltime 0.2.0","title":"modeltime 0.2.0","text":"CRAN release: 2020-09-28 Ensembles Integrates modeltime.ensemble, new R package designed forecasting ensemble models. New Workflow Helper Functions add_modeltime_model() - helper function making easy add fitted parsnip workflow object modeltime table pluck_modeltime_model() & pull_modeltime_model() - helper function making easy extract model modeltime table Improvements Documentation - Algorithms now identify default parameter values “Engine Details” Section respective documentation. E.g. ?prophet_boost regressors.mode - Set seasonality.mode default. regressors.prior.scale - Set 10,000 default. regressors.standardize - Set “auto” default. Data Sets Modeltime now includes 4 new data sets: m750 - M750 Time Series Dataset m750_models - 3 Modeltime Models made M750 Dataset m750_splits - rsplit object containing Train/test splits M750 data m750_training_resamples - Time Series Cross Validation time_series_cv object made training(m750_splits) Bug Fix plot_modeltime_forecast() fix issue “ACTUAL” data shown bottom legend list. first item.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-010","dir":"Changelog","previous_headings":"","what":"modeltime 0.1.0","title":"modeltime 0.1.0","text":"CRAN release: 2020-09-02","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-features-0-1-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"modeltime 0.1.0","text":"Forecast without Calibration/Refitting Sometimes ’s important make fast forecasts without calculating --sample accuracy refitting (requires 2 rounds model training). can now bypass modeltime_calibrate() modeltime_refit() steps jump straight forecasting future. ’s example h = \"3 years\". Note get confidence intervals approach calibration data needed . Residual Analysis & Diagonstics common tool forecasting analyzing residuals, residuals .resid = .actual - .prediction. residuals may autocorrelation nonzero mean, can indicate model improvement opportunities. addition, users may inspect -sample --sample residuals, can display different results. modeltime_residuals() - new function used extract residual information Time Plot - Residuals time ACF Plot - Residual Autocorrelation vs Lags Seasonality - Residual Seasonality Plot","code":"# Make forecasts without calibration/refitting (No Confidence Intervals) # - This assumes the models have been trained on m750 modeltime_table(     model_fit_prophet,     model_fit_lm ) %>%     modeltime_forecast(         h = \"3 years\",         actual_data = m750     ) %>%     plot_modeltime_forecast(.conf_interval_show = F)"},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-models-0-1-0","dir":"Changelog","previous_headings":"","what":"New Models","title":"modeltime 0.1.0","text":"TBATS Model Use seasonal_reg() set engine “tbats”. NNETAR Model Use nnetar_reg() set engine “nnetar”. Prophet Model - Logistic Growth Support Now supports logistic growth. Set growth = 'logistic' one logistic_cap logistic_floor valid saturation boundaries. New arguments making easier modify changepoint_num, changepoint_range, seasonality_yearly, seasonality_weekly, seasonality_daily, logistic_cap, logistic_floor","code":"seasonal_reg(     seasonal_period_1 = \"1 day\",     seasonal_period_2 = \"1 week\" ) %>%      set_engine(\"tbats\") model_fit_nnetar <- nnetar_reg() %>%     set_engine(\"nnetar\")"},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"new-workflow-helper-functions-0-1-0","dir":"Changelog","previous_headings":"","what":"New Workflow Helper Functions","title":"modeltime 0.1.0","text":"combine_modeltime_tables() - helper function making easy combine multiple modeltime tables. update_model_description() - helper function making easier update model descriptions.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"improvements-0-1-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"modeltime 0.1.0","text":"modeltime_refit(): modeltime model parameters update (e.g. Auto ARIMA changes new model), Model Description now alerts user (e.g. “UPDATE: ARIMA(0,1,1)(1,1,1)[12]”). modeltime_calibrate(): training data supplied time window model previously trained (e.g. training(splits)), calibration calculation first inspects whether “Fitted” data exists. iexists, returns “Fitted” data. helps prevent sequence-based (e.g. ARIMA, ETS, TBATS models) displaying odd results algorithms can predict sequences directly following training window. “Fitted” data used, .type column display “Fitted” instead “Test”.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"bug-fixes-0-1-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"modeltime 0.1.0","text":"modeltime_forecast(): Implement actual_data reconciliation strategies recipe removes rows. Strategy attempts fill predictors using “downup” strategy prevent NA values removing rows. descriptive errors external regressors required. modeltime_accuracy(): Fix issue new_data recalibrating. prophet_reg() prophet_boost() - Can now perform logistic growth growth = 'logistic'. user can supply “saturation” bounds using logistic_cap /logisitc_floor.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"breaking-changes-0-1-0","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"modeltime 0.1.0","text":"seasonal_decomp() changed seasonal_reg() now supports TBATS Seasonal Decomposition Models. num_changepoints become changepoint_num","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-002","dir":"Changelog","previous_headings":"","what":"modeltime 0.0.2","title":"modeltime 0.0.2","text":"CRAN release: 2020-07-03","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"confidence-interval-estimation-0-0-2","dir":"Changelog","previous_headings":"","what":"Confidence Interval Estimation","title":"modeltime 0.0.2","text":"modeltime_forecast(): Now estimates confidence intervals using centered standard deviation. mean assumed zero residuals deviate mean = 0.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"fixes-0-0-2","dir":"Changelog","previous_headings":"","what":"Fixes","title":"modeltime 0.0.2","text":"Updates work parsnip 0.1.2. prophet_boost(): Set nthreads = 1 (default) ensure parallelization thread safe.","code":""},{"path":"https://business-science.github.io/modeltime/news/index.html","id":"modeltime-001","dir":"Changelog","previous_headings":"","what":"modeltime 0.0.1","title":"modeltime 0.0.1","text":"CRAN release: 2020-06-22 Initial Release","code":""}]
